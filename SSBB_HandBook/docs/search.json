[
  {
    "objectID": "ISI_SSBB_BA_PE_CODE.html",
    "href": "ISI_SSBB_BA_PE_CODE.html",
    "title": "Periodical Examination.",
    "section": "",
    "text": "import re\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom scipy.stats import norm, poisson, ttest_1samp,mannwhitneyu,wilcoxon\nfrom statsmodels.stats.descriptivestats import sign_test\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")\nimport statsmodels.graphics.gofplots as gof\nfrom scipy.stats import chi2_contingency, chisquare\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats import outliers_influence as sm_oi\nfrom statsmodels.stats.anova import anova_lm\n\n\ndef cols_tidy(df_cols):\n    df_cols_tidy = []\n    for item in df_cols:\n        x = re.sub(r\" +\", \" \", item)\n        x1 = re.sub(r\"\\,|\\)|\\(|\\.|\\-|\\'\", \"\", x)\n        x2 = re.sub(r\"\\/\", \"_or_\", x1)\n        x3 = re.sub(r\"\\&\", \"_and_\", x2)\n        x4 = re.sub(r\"\\s(?=\\_{1,})|(\\_(?=\\s{1,}))|\\,\", \"\", x3)\n        x5 = re.sub(r\" {2,}\", \" \", x4)\n        x6 = re.sub(r\"\\s\", \"_\", x5)\n        x7 = re.sub(r\"\\:\", \"\", x6)\n        df_cols_tidy.append(x7.lower())\n    return df_cols_tidy\n\ndef calculate_center_line(data, column_name):\n    \"\"\"Calculates the center line for the control chart.\"\"\"\n    return data[column_name].mean()\n\n\ndef calculate_control_limits(center, average_defects_per_unit, multiplier):\n    \"\"\"Calculates upper and lower control limits.\"\"\"\n    std_dev = np.sqrt(average_defects_per_unit)\n    upper_limit = center + multiplier * std_dev\n    lower_limit = center - multiplier * std_dev\n    return upper_limit, lower_limit\n\n\ndef create_run_chart(data, column_name):\n    \"\"\"Creates a run chart for the given data.\"\"\"\n    plt.figure(figsize=(25, 6))\n    plt.plot(data.index, data[column_name], marker=\"o\", linestyle=\"-\")\n    plt.xlabel(\n        \"Projects-From Jan to Dec-2023\"\n    )  # Assuming the index represents time periods\n    plt.ylabel(column_name)  # Replace with the actual column name\n    plt.title(\"Run Chart for \" + column_name)\n    plt.grid(True)\n    plt.show()\n\ndef get_hypothesis(_p_val):\n    if _p_val&lt;0.05:\n        print(f'P-Value:{round(_p_val,5)} is less than 0.05 hence we can reject the null hypothesis in favor of alternative hypothesis')\n    else:\n        print(f'P-Value:{round(_p_val,5)} is greater than 0.05 hence we failed to reject the null hypothesis.')\n\ndef custom_ols_qqplot(_resid):\n    \"\"\"Q-Q Plot of residuals\"\"\"\n    gof.qqplot(_resid, line=\"s\")\n    plt.xlabel(\"Standard Normal Quantiles\")\n    plt.ylabel(\"Standardized Residuals\")\n    plt.title(\"Normal Q-Q plot\")\n    plt.show()\n\n\ndef custom_ols_res_vs_fitted(_fitted, _resid):\n    \"\"\"Fitted Vs Residuals Plot\"\"\"\n    plt.scatter(_fitted, _resid)\n    plt.axhline(\"0\", color=\"r\")\n    plt.xlabel(\"Fitted Values\")\n    plt.ylabel(\"Residual\")\n    plt.title(\"Residual Vs Fitted\")\n\n\ndef custom_VIF(_MSPEC):\n    \"\"\"Custom function to get the VIF\"\"\"\n    var_names = _MSPEC.exog_names\n    X = _MSPEC.exog\n    _limit = X.shape[1]\n    try:\n        vif_dict = {}\n        for idx in range(_limit):\n            vif = round(sm_oi.variance_inflation_factor(X, idx), 5)\n            vif_dict[var_names[idx]] = vif\n        _DF = pd.DataFrame([vif_dict]).T\n        _DF.columns = [\"VIF\"]\n        _DF = _DF.reset_index()\n        df_sorted = _DF.iloc[1:].sort_values(by=\"VIF\", ascending=False)\n        ax = sns.barplot(x=\"index\", y=\"VIF\", data=df_sorted)\n        # Add text labels to the top of each bar\n        for bar in ax.containers[0]:\n            ax.text(\n                bar.get_x() + bar.get_width() / 2,\n                bar.get_height(),\n                int(bar.get_height()),\n                ha=\"center\",\n                va=\"bottom\",\n            )\n        ax.set_xlabel(\"FIELD\")\n        ax.set_ylabel(\"VIF\")\n        plt.xticks(rotation=45)\n        plt.title(\"VIF\")\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        pass\n\ndef custom_statsmodel_OLS(_DF, *vars):\n    \"\"\"fitting OLS on specified independent and dependent variables- DF, dependent_var and independent_var\"\"\"\n    # sm.add_constant\n    try:\n        LOS_COLS = [v for v in vars]\n        _X = LOS_COLS[1:]\n        _Y = LOS_COLS[0]\n        xvars = sm.add_constant(_DF[_X])\n        yvar = _DF[_Y]\n        _model_spec = sm.OLS(yvar, xvars)\n        return _model_spec\n    except Exception as e:\n        print(f\"There is an error while creating a model spec due to:{e}\")\n\ndef custom_model_preds(_model, _new_df):\n    \"\"\"Predictions on new data points\"\"\"\n    _feat = sm.add_constant(_new_df)\n    _pred = _model.predict(sm.add_constant(_feat))\n    _df_pred = pd.DataFrame(_pred)\n    _df_pred.columns = [\"predicted_y\"]\n    return _df_pred\n\n\ndef get_six_sigma_caluclator():\n    \"\"\"ISI Custom SixSigma Calucator\"\"\"\n    while True:\n        ### Inputs\n        print(f\"-------------------------------------------------\")\n        print(f\"############ Sigma Caluclator Inputs ############\")\n        print(f\"-------------------------------------------------\")\n        _mean = float(input(\"Enter the mean:\"))\n        _sd = float(input(\"Enter Standard Deviation:\"))\n        _LSL = float(input(\"Enter LSL:\"))\n        _USL = float(input(\"Enter USL:\"))\n        # Formulas and caluclations\n        ZLSL = (_LSL - _mean) / _sd\n        ZUSL = (_USL - _mean) / _sd\n        Area_ZLSL = norm.cdf(ZLSL)\n        Area_ZUSL = 1 - norm.cdf(ZUSL)\n        TOTAL_NC = Area_ZLSL + Area_ZUSL\n        YIELD = 1 - TOTAL_NC\n        CP_ = (_USL - _LSL) / (6 * _sd)\n        _A = (_USL - _mean) / (3 * _sd)\n        _B = (_mean - _LSL) / (3 * _sd)\n        CPK_ = min(_A, _B)\n        SIGMA_LEVEL = round(1.5 + norm.ppf(YIELD), 5)\n        DPMO = TOTAL_NC * 1000000\n        # Output\n        print(f\"-------------------------------------------------\")\n        print(f\"#### Summary Report ####\")\n        print(f\"-------------------------------------------------\")\n        print(f\"Total NonConfirmances:{round(TOTAL_NC,5)}\")\n        print(f\"Yield:{round(YIELD,5)}\")\n        print(f\"CP:{round(CP_,5)}\")\n        print(f\"CPK:{round(CPK_,5)}\")\n        print(f\"SIGMA_LEVEL:{round(SIGMA_LEVEL,5)}\")\n        print(f\"DPMO:{round(DPMO,5)}\")\n        print(f\"-------------------------------------------------\")\n        _next = input(\n            \"Would you like to continue to use sigma caluclator type 'yes' if so :\"\n        )\n        if _next.lower() == \"yes\":\n            continue\n        else:\n            print(f\"Thanks for using Sigma Caluclator..\")\n            print(f\"#### END ####\")\n            break\n\n\n# Data Importings\ndf_cars = pl.read_csv(r\"/Users/malleshamyamulla/Desktop/SSBBA/CARS.csv\")\ndf_cars.columns = cols_tidy(df_cars.columns)\ndf_cars_pd = df_cars.to_pandas()\n\n\n\n\ndf_cars_pd.head()\n\n\n\n\n\n\n\n\n\nmake\nfueltype\naspiration\nenginelocation\nnumofdoors\ndrivewheels\nfuelsystem\nbodystyle\nenginetype\nnumofcylinders\n...\nenginesize\nbore\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\n_insurance_risk_rating\ncar_price\n\n\n\n\n0\nalfa-romero\ngas\nstd\nfront\ntwo\nrwd\nmpfi\nconvertible\ndohc\nfour\n...\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\nHigh\n13495\n\n\n1\nalfa-romero\ngas\nstd\nfront\ntwo\nrwd\nmpfi\nconvertible\ndohc\nfour\n...\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\nHigh\n16500\n\n\n2\nalfa-romero\ngas\nstd\nfront\ntwo\nrwd\nmpfi\nhatchback\nohcv\nsix\n...\n152\n2.68\n3.47\n9.0\n154\n5000\n19\n26\nMedium\n16500\n\n\n3\naudi\ngas\nstd\nfront\nfour\nfwd\nmpfi\nsedan\nohc\nfour\n...\n109\n3.19\n3.40\n10.0\n102\n5500\n24\n30\nHigh\n13950\n\n\n4\naudi\ngas\nstd\nfront\nfour\n4wd\nmpfi\nsedan\nohc\nfive\n...\n136\n3.19\n3.40\n8.0\n115\n5500\n18\n22\nHigh\n17450\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\n\n\n\n# car makes group by operation\ndf_causes = (\n    df_cars.group_by(\"make\")\n    .count()\n    .with_columns(pl.sum(\"count\").alias(\"tot_\"))\n    .with_columns(pl.col(\"count\") / pl.col(\"tot_\"))\n    .drop(\"tot_\")\n    .sort(\"count\", descending=True)\n    .with_columns(pl.col(\"count\").alias(\"RPN\"))\n    .drop(\"count\")\n    .to_pandas()\n)\n\n\n# Pareto Diagram\nsorted_data = df_causes.sort_values(by=[\"RPN\"], ascending=False)\nsorted_data[\"cumulative_freq\"] = sorted_data[\"RPN\"].cumsum()\nsorted_data[\"cumulative_pct\"] = (\n    sorted_data[\"cumulative_freq\"] / sorted_data[\"RPN\"].sum() * 100\n)\n\n# Visualizations\nfig, ax1 = plt.subplots(figsize=(22, 8))\nax2 = ax1.twinx()\nax1.bar(sorted_data[\"make\"], sorted_data[\"RPN\"], color=\"skyblue\")\nax2.plot(\n    sorted_data[\"make\"],\n    sorted_data[\"cumulative_pct\"],\n    color=\"red\",\n    marker=\"o\",\n    linestyle=\"-\",\n)\nax1.set_xlabel(\"Category\")\nax1.set_ylabel(\"Frequency\", color=\"skyblue\")\nax2.set_ylabel(\"Cumulative Percentage\", color=\"red\")\nplt.title(\"Pareto Analysis- Car Makers\")\nplt.show()\n\n\n\n\n\n\n\n\nInference\nThe below car makers are found to be top 7 performares in car sales.\n\ntoyota\nnissan\nmazda\nmitsubishi\nhonda\nsubaru\nvolkswagen\n\n\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1=sns.histplot(df_cars[\"citympg\"],ax=ax1)\np2=sns.boxplot(df_cars[\"citympg\"].to_pandas(),ax=ax2)\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nCars citympg data contains 2 assignable causes i.e citympg are more than 45, they are kept a side and will carry out the stability analysis on it.\n\n\ndf_city_mpg = df_cars.select([\"citympg\"]).filter(pl.col(\"citympg\") &lt; 43)\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\nsns.boxplot(df_city_mpg.select([\"citympg\"]).to_pandas(),ax=ax1)\nsns.histplot(df_city_mpg, kde=True,ax=ax2)\nplt.show()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_city_mpg.to_pandas().describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncitympg\n202.0\n24.89604\n6.017725\n13.0\n19.0\n24.0\n30.0\n38.0\n\n\n\n\n\n\n\n\n\ncreate_run_chart(df_city_mpg.to_pandas(), \"citympg\")\n\n\n\n\n\n\n\n\nInferences\n\nAfter removing the assignable the random chances are found to be nearly normal distributed, with a mean of 23.89 and standard deviation of 6.01.\nMean and Median Values are found to approximately equal.\nThe run chart shows no trends, neither upward nor downward.\nIn a conclusion the process is found to be statistically stable one.\n\n\n\n\nBelow are the Business Specifications\n0. CTQ: Car city MPG should alteast be 20.\n1. Mean: avg city-mpg ~ 24.89\n2. SD:   Standard Deviation ~ 6.01\n3. Tolerances:\n     - LSL: ~ 20\n     - USL: ~ 38(Max MPG found from the given data)\n\n#get_six_sigma_caluclator()\n\nCaluclated Performance Metrics on citympg:\n\nTotal NonConfirmances:0.2225\nYield:0.7775\nCP:0.49917\nCPK:0.27121\nSIGMA_LEVEL:2.26377\nDPMO:222502.72038\n\nInferences:\n\nCp(0.49): This value suggests the process is not very capable of consistently producing cars that meet the CTQ of 20 MPG city mileage.\nCpk(0.27): Cpk is lower than Cp, it means that the process isn’t performing well relative to the CTQ\nSigma Level(2.2): Process Spread is lower.\nDPMO(222502): This represents the estimated number of cars per million that would not meet the city MPG criteria based on the current process performance.\nYield(0.7775): - This indicates the proportion of cars (77.75%) that meet or exceed the city MPG requirement of at least 20.\nNC(0.25): This represents the proportion of cars (22.25%) that have a city MPG below the minimum acceptable standard of 20.\n\n\n\n\n\nHypothesis\n        Ho: The average selling price of cars at Sigma Cars is less than 1200.\n        Ha: The average selling price of cars at Sigma Cars is equal to or greater than 1200.\nDescriptive Stas on CarPrice\n\nprint(f'Descriptive Statistics of car price:')\npd.DataFrame(df_cars_pd['car_price'].describe()).T\n\nDescriptive Statistics of car price:\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncar_price\n205.0\n13243.243902\n7941.72182\n5118.0\n7788.0\n10295.0\n16503.0\n45400.0\n\n\n\n\n\n\n\n\nCarPrice Distribution\n\nsns.histplot(df_cars_pd['car_price'])\nplt.axvline(13243,color='red')\nplt.axvline(10295,color='black')\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nTotal Number of observation 205.\nCar prices data exhibits a right skewed distribution.\nCar Price mean(13243) is found be greater than median value(10295) as its right skewed distribution\nCar Price is not normally distributed, however the sample size is large enough i.e 205 so it would be ok to carry out 1 Sample T-test to test the postulated hypothesis.\n\n\n_stat_3c,_p_value_3c= ttest_1samp(df_cars_pd['car_price'],popmean=12000,alternative='greater')\n_3_1_1st=ttest_1samp(df_cars_pd['car_price'],popmean=12000,alternative='greater')\n_low,_high=_3_1_1st.confidence_interval()\nget_hypothesis(_p_value_3c)\n\nP-Value:0.01304 is less than 0.05 hence we can reject the null hypothesis in favor of alternative hypothesis\n\n\nInferences\n\nI have carried out a one-sample t-test(One-tailed) on car price to see if the car price is on or above 12000.\nThe caluclated p-value(0.013) suggest that we can reject the null hypothesis in favor of alternative meaning The average selling price of cars at Sigma Cars is equal to or greater than 1200 and the sigma cars would have a potential business to do.\n\n\ndf_cars_pd.loc[df_cars_pd['car_price']&gt;=12000,'car_price_seg']='&gt;12000'\ndf_cars_pd.loc[df_cars_pd['car_price']&lt;12000,'car_price_seg']='&lt;12000'\n\n\nsns.barplot(pd.DataFrame(df_cars_pd['car_price_seg'].value_counts()).reset_index(),x='car_price_seg',y='count')\nplt.show()\n\n\n\n\n\n\n\n\nNotes\nI have categorized car prices into two groups such as &lt;12000 and &gt;=12000 and 40% of the car prices fall into the higher price segment (at or above 12000).\n\n\n\n- fueltype\n- aspiration\n- enginelocation\n- numofdoors\n\ndf_3d = df_cars_pd[['fueltype', 'aspiration','numofdoors','enginelocation','car_price']]\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\nsns.boxplot(x='fueltype',y='car_price',data=df_3d,ax=axes[0,0])\nsns.boxplot(x='aspiration',y='car_price',data=df_3d,ax=axes[0,1])\nsns.boxplot(x='numofdoors',y='car_price',data=df_3d,ax=axes[1,0])\nsns.boxplot(x='enginelocation',y='car_price',data=df_3d,ax=axes[1,1])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define the model formula\ndf_3d_model = ols(\n    \"car_price ~ fueltype+aspiration+enginelocation+numofdoors\", data=df_3d\n).fit()\n\n# Perform ANOVA\nanova_table_3d = anova_lm(df_3d_model)\n\n# Print ANOVA results\nanova_table_3d.apply(lambda x:round(x,5))\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nfueltype\n1.0\n1.492298e+08\n1.492298e+08\n2.74410\n0.09918\n\n\naspiration\n1.0\n2.987693e+08\n2.987693e+08\n5.49390\n0.02006\n\n\nenginelocation\n1.0\n1.480249e+09\n1.480249e+09\n27.21943\n0.00000\n\n\nnumofdoors\n1.0\n6.181319e+07\n6.181319e+07\n1.13665\n0.28765\n\n\nResidual\n200.0\n1.087641e+10\n5.438206e+07\nNaN\nNaN\n\n\n\n\n\n\n\n\nInferences:\n\nThe variables fueltype, aspiration, enginelocation and numofdoors are having the same number of degrees of freedom.\nFuelType: F value 2.77 and P-value 0.09.It’s not significant as it has higher p-value than 0.05\nAspiration: F Value 5.49 and P-value 0.002. It’s a statistical significant variable with lower p-value less than 0.02\nEnginelocation: F-Value 27.21 and P-Value 0.0, It’s also a statistical significant variable with lower p-value less than 0.0\nNumofdoors: F-Value 6.18 and P-Value 0.28. sIt’s not significant as it has higher p-value than 0.05\nIn a conclusion: Aspiration and EngineLocation Variable would have an advantage in car pricing. If we were to select only of these two enginee location would be the most appropriate variable in car pricing.\n\n\n\n\nHypothesis\n        Ho: There is no statistically significant difference in the average price of cars between drive wheel and fuel systems.\n        Ha: There is a statistically significant difference in the average price of cars between drive wheel and fuel systems.\n\ndf_3e = df_cars_pd[['drivewheels','fuelsystem','car_price']]\n\n\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\nsns.boxplot(x='drivewheels',y='car_price',data=df_3e,ax=ax1)\nsns.boxplot(x='fuelsystem',y='car_price',data=df_3e,ax=ax2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n# Define the model formula\ndf_3e_model = ols(\n    \"car_price ~ drivewheels+fuelsystem\", data=df_3e\n).fit()\n\n# Perform ANOVA\nanova_table_3e = anova_lm(df_3e_model)\n\n# Print ANOVA results\nanova_table_3e.apply(lambda x:round(x,5))\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\ndrivewheels\n2.0\n5.274374e+09\n2.637187e+09\n79.15850\n0.00000\n\n\nfuelsystem\n3.0\n9.623598e+08\n3.207866e+08\n9.62882\n0.00001\n\n\nResidual\n199.0\n6.629739e+09\n3.331527e+07\nNaN\nNaN\n\n\n\n\n\n\n\n\nInferences:\n\nThe variables DriveWheel and fuelsystem are having the different number of degrees of freedom. 2 and 3 respectively.\nDriveWheel: F value 79.15 and P-value 0.0 suggest to reject the null hypothesis.\nFuelType: F Value 9.62 and P-value 0.0 also suggest to reject the null hypothesis.\nDrivewheel F-Value is much higher than to FuelType, This suggests a potentially stronger effect of drive wheel on car price compared to fuel type.\nIn a conclusion: There is a statistically significant difference in the average price of cars between drive wheel and fuel systems.\n\n\n\n\n\nHypothesis-1\n        Ho: On average, the price of a car isn't affected by its body style.\n        Ha: On average, the price of a car is affected by its body style.\nHypothesis-2\n        Ho: On average, the price of a car isn't affected by its engine type.\n        Ha: On average, the price of a car is affected by its engine type.\n\ndf_3f = df_cars_pd[['bodystyle','enginetype','car_price']]\n\n\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\nsns.boxplot(x='bodystyle',y='car_price',data=df_3f,ax=ax1)\nsns.boxplot(x='enginetype',y='car_price',data=df_3f,ax=ax2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n# Define the model formula\ndf_3f_model = ols(\n    \"car_price ~ bodystyle+enginetype\", data=df_3f\n).fit()\n\n# Perform ANOVA\nanova_table_3f = anova_lm(df_3f_model)\n\n# Print ANOVA results\nanova_table_3f.apply(lambda x:round(x,5))\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nbodystyle\n4.0\n1.826663e+09\n4.566658e+08\n10.25081\n0.0\n\n\nenginetype\n4.0\n2.308158e+09\n5.770395e+08\n12.95284\n0.0\n\n\nResidual\n196.0\n8.731652e+09\n4.454924e+07\nNaN\nNaN\n\n\n\n\n\n\n\n\nInferences:\n\nThe variables Bodystyle and Engine type are having the same number of degrees of freedom.\nBodystyle: F value 10.25 and P-value 0.0 suggest to reject the null hypothesis.\nEngineType: F Value 12.95 and P-value 0.0 also suggest to reject the null hypothesis.\nEngineType F-Value is little higher than to Bodytype.\nIn a conclusion: The average car price is affected by both BodyStyle and EngineType.\n\n\n\n\n\nHypothesis\n        Ho: The average car price is the same across all cylinder levels-there is no statistically significant relationship between the number of cylinders in a car and its average selling price. \n        Ha: at least one mean car price is different across all cylinder levels-There is a statistically significant relationship between the number of cylinders in a car and its average selling price.\n\ndf_3g=df_cars_pd[['numofcylinders','car_price']]\n\n\n\n\nsns.boxplot(x='numofcylinders',y='car_price',data=df_3g)\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nThe Numofcylinders are having 5 levels.\nThe carprice data per each of these levels is distributed differently from the above box plot. The boxplot reveals that the distribution of car prices differs significantly across these cylinder levels.\n\n\n\n\n\n# Define the model formula\ndf_3g_model = ols(\n    \"car_price ~ numofcylinders\", data=df_3g\n).fit()\n\n# Perform ANOVA\nanova_table_3g = anova_lm(df_3g_model)\n\n# Print ANOVA results\nanova_table_3g.apply(lambda x:round(x,5))\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nnumofcylinders\n4.0\n7.964193e+09\n1.991048e+09\n81.22947\n0.0\n\n\nResidual\n200.0\n4.902280e+09\n2.451140e+07\nNaN\nNaN\n\n\n\n\n\n\n\n\nInferences\n\nFrom the ANOVA Table: For the numofcylinders variable The F-Value is moderately high i.e 81.22 while the caluclated P value(0.0) suggest that we can reject the null hypothesis in favor of alternative i.e the Mean car prices are different across the numberof cylinders.\n\n\n\n\n\nHypothesis:1\n    Ho: The insurance risk ratings are same for all Fuel Systems, Engine type does not have any association with insurance risk There is no association between the fuelsystem and insurance risk rating levels.\n    \n    Ha: The distribution of insurance risk ratings differs from each Fuel System,There is an association between the fuelsystem and insurance risk rating levels.\n    \nHypothesis:2\n        Ho: The insurance risk ratings are same for all engine types, Engine type does not have any association with insurance risk.\n        Ha: The distribution of insurance risk ratings differs from each engine type, engine type does have an association.\n\ndf_3h=df_cars_pd[['fuelsystem','enginetype','_insurance_risk_rating']]\ndf_3h_agg=df_3h.groupby(['fuelsystem', '_insurance_risk_rating']).count().reset_index()\ndf_3h_agg1=df_3h.groupby(['enginetype', '_insurance_risk_rating']).count().reset_index()\n\nVIS-1:FuelSystem ~ Insurance_Risk_Rating\n\nsns.barplot(\n    df_3h_agg, x=\"fuelsystem\", y=\"enginetype\", hue=\"_insurance_risk_rating\"\n)\nplt.ylabel('count')\nplt.show()\n\n\n\n\n\n\n\n\nNotes:\n\nThere are four levels of fuel system and 3 levels of insurance risk rating.\nThe graph shows the counts of each insurance risk rating per each of fuel system.\nFuel system 1bbl has only got 2 insurance ratings, and the other 3 have the three different risk ratings in different proportions.\n\nVIS-2:EngineType ~ Insurance_Risk_Rating\n\nsns.barplot(\n    df_3h_agg1, x=\"enginetype\", y=\"fuelsystem\", hue=\"_insurance_risk_rating\"\n)\nplt.ylabel('count')\nplt.show()\n\n\n\n\n\n\n\n\nNotes:\n\nThere are 5 levels of enginetype and 3 levels of insurance risk rating.\nThe graph shows the counts of each insurance risk rating per each of engine system.\nAll engine types have got the insurance risk rating, 4 of th engine type levels are found to be having very similar proportions across the three risk rating categories.\n\n\n\n\n## Chi-Square Test-1\ndf_ecom_CT = pd.crosstab(df_3h[\"fuelsystem\"], df_3h[\"_insurance_risk_rating\"]).reset_index().drop([\"fuelsystem\"], axis=1)\n\nCS_LVEL = df_ecom_CT.to_numpy()\n\n_chi2, _pvalue, _ddof, _expected = chi2_contingency(CS_LVEL)\n\nget_hypothesis(_pvalue)\n\nP-Value:1e-05 is less than 0.05 hence we can reject the null hypothesis in favor of alternative hypothesis\n\n\n\n\n\n\n## Chi-Square Test-1\ndf_ecom_CT = pd.crosstab(df_3h[\"enginetype\"], df_3h[\"_insurance_risk_rating\"]).reset_index().drop([\"enginetype\"], axis=1)\n\nCS_LVEL = df_ecom_CT.to_numpy()\n\n_chi2_, _pvalue_cs, _ddof1, _expected1 = chi2_contingency(CS_LVEL)\n\nget_hypothesis(_pvalue_cs)\n\nP-Value:0.4149 is greater than 0.05 hence we failed to reject the null hypothesis.\n\n\nConclusions\n\nThe ChiSquare Associaton test on EngineType and Insurance Risk Rating suggests that the Engine type does not have any association with insurance risk. here with higher p-value we failed to reject the null hypothesis.\n\n\n\n\n\n\nlos_con_cols = [\n    \"wheelbase\",\n    \"length\",\n    \"width\",\n    \"height\",\n    \"curbweight\",\n    \"enginesize\",\n    \"bore\",\n    \"stroke\",\n    \"compressionratio\",\n    \"horsepower\",\n    \"peakrpm\",\n    \"citympg\",\n    \"highwaympg\",\n    \"car_price\",\n]\n\n\ndf_cars_v1 = df_cars_pd[los_con_cols]\n\n\n\n\ndf_cars_v1.head()\n\n\n\n\n\n\n\n\n\nwheelbase\nlength\nwidth\nheight\ncurbweight\nenginesize\nbore\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\ncar_price\n\n\n\n\n0\n88.6\n168.8\n64.1\n48.8\n2548\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495\n\n\n1\n88.6\n168.8\n64.1\n48.8\n2548\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500\n\n\n2\n94.5\n171.2\n65.5\n52.4\n2823\n152\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500\n\n\n3\n99.8\n176.6\n66.2\n54.3\n2337\n109\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950\n\n\n4\n99.4\n176.6\n66.4\n54.3\n2824\n136\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450\n\n\n\n\n\n\n\n\n\n\n\n\nsns.heatmap(df_cars_v1.corr(), annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nX = sm.add_constant(df_cars_v1.drop([\"car_price\"], axis=1))\nY = df_cars_v1[\"car_price\"]\n_model_spec = sm.OLS(Y, X)\n_MODEL_FIT = _model_spec.fit()\n\n\n\n\n\nprint(f'Trial-1#The Regression Table:##\\n{_MODEL_FIT.summary2()}')\n\nTrial-1#The Regression Table:##\n                       Results: Ordinary least squares\n=============================================================================\nModel:                  OLS                 Adj. R-squared:        0.838     \nDependent Variable:     car_price           AIC:                   3904.1448 \nDate:                   2024-04-12 21:13    BIC:                   3950.6669 \nNo. Observations:       205                 Log-Likelihood:        -1938.1   \nDf Model:               13                  F-statistic:           82.05     \nDf Residuals:           191                 Prob (F-statistic):    8.95e-71  \nR-squared:              0.848               Scale:                 1.0230e+07\n-----------------------------------------------------------------------------\n                    Coef.     Std.Err.     t    P&gt;|t|     [0.025     0.975]  \n-----------------------------------------------------------------------------\nconst            -37199.2505 14901.3433 -2.4964 0.0134 -66591.5838 -7806.9173\nwheelbase           106.6533   100.7882  1.0582 0.2913    -92.1475   305.4541\nlength              -71.0280    55.7670 -1.2737 0.2043   -181.0263    38.9702\nwidth               381.1637   247.0076  1.5431 0.1245   -106.0493   868.3767\nheight              191.9187   135.4189  1.4172 0.1580    -75.1899   459.0272\ncurbweight            1.9427     1.7428  1.1147 0.2664     -1.4949     5.3803\nenginesize          126.3904    13.9795  9.0411 0.0000     98.8163   153.9645\nbore              -2191.7312  1105.5166 -1.9825 0.0489  -4372.3207   -11.1417\nstroke            -3357.5078   756.6845 -4.4371 0.0000  -4850.0391 -1864.9764\ncompressionratio    279.3676    82.8764  3.3709 0.0009    115.8970   442.8383\nhorsepower           24.1233    16.1420  1.4944 0.1367     -7.7163    55.9628\npeakrpm               2.1481     0.6599  3.2554 0.0013      0.8465     3.4496\ncitympg            -277.5227   177.7650 -1.5612 0.1201   -628.1574    73.1119\nhighwaympg          151.2249   160.3563  0.9431 0.3468   -165.0718   467.5216\n-----------------------------------------------------------------------------\nOmnibus:                 25.401           Durbin-Watson:              0.997  \nProb(Omnibus):           0.000            Jarque-Bera (JB):           104.409\nSkew:                    0.301            Prob(JB):                   0.000  \nKurtosis:                6.444            Condition No.:              383868 \n=============================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n[2] The condition number is large, 3.84e+05. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\nInferences\n\nThe fitted model has producted: R-SQuared IS (0.84) adn R-Squared(Adj) is 0.83, and it means the proportion of variation in Y explained by all the continous predictors. the numbers are pretty OK.\nThe p-value represents the probability of observing a slope as extreme or more extreme as the one we calculated in the sample, assuming there is truly no relationship between X and Y i.e null hypothesis and an alternative hypothesis would be there is a truly relationship between X and Y.\nConsidering the above point-2. we would nowlook at all the betas of fitted model and their P-values to determine if they are OK to predict the car price. they below mentioned 4 predictors are only statistically significant for model predictions. and we would also check other regression diagnostics to take a call on it.\n\nenginesize\nstroke\ncompressionratio\npeakrpm\n\n\n\ndf_Betas_with_alpha = pd.DataFrame(_MODEL_FIT.params).reset_index().iloc[1:, :]\ndf_Betas_with_alpha.columns = [\"betas\", \"val\"]\ndf_Betas_with_SE = (\n    pd.concat([_MODEL_FIT.params, _MODEL_FIT.bse], axis=1).reset_index().iloc[1:, :]\n)\ndf_Betas_with_SE.columns = [\"betas\", \"val\", \"se_\"]\n\n\n\n\n\nsns.barplot(\n    y=\"betas\", x=\"val\", data=df_Betas_with_SE.sort_values(\"val\", ascending=False)\n)\nplt.show()\n\n\n\n\n\n\n\n\nNotes\nThe beta coefficients for half of the predictor variables are negative. In other words, a one-unit increase in these variables is likely to lead to a decrease in the value of the response variable, but the magnitude of that decrease is not specified here.\n\n\n\n\ncustom_ols_qqplot(_MODEL_FIT.resid)\n\n\n\n\n\n\n\n\nComment: The Q-Q plot indicates that the residuals are approximately normally distributed.\n\n\n\n\ncustom_ols_res_vs_fitted(_MODEL_FIT.fittedvalues, _MODEL_FIT.resid)\n\n\n\n\n\n\n\n\nComment: The fitted vs. residual plot suggests that there is no random scatter of residuals, and there are some clusters\n\n\n\n\ncustom_VIF(_model_spec)\n\n\n\n\n\n\n\n\nComment:\n\nVIF value for the below predictors are high and This suggests that these variables may have multicollinearity with other independent variables in the model.\n\ncitympg\nhihwaympg\ncurbweight\nlength\nhorsepower\nwheelbase\n\nThe below predictors which have VIF less than or equal to 5 are considered to be the better predictors in regression modeling.\n\nstroke\nbore\npeakrpm\ncompressionratio\nheight\nwidth\nenginesize\n\n\n\n\n\n\nlos_sel_X = [\n    \"stroke\",\n    \"bore\",\n    \"peakrpm\",\n    \"compressionratio\",\n    \"height\",\n    \"width\",\n    \"enginesize\",\n]\n\n\nsns.heatmap(df_cars_v1[[ \"car_price\",\n    \"stroke\",\n    \"bore\",\n    \"peakrpm\",\n    \"compressionratio\",\n    \"height\",\n    \"width\",\n    \"enginesize\"]].corr(),annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n_model_spec_2 = custom_statsmodel_OLS(\n    df_cars_v1,\n    \"car_price\",\n    \"stroke\",\n    \"bore\",\n    \"peakrpm\",\n    \"compressionratio\",\n    \"height\",\n    \"width\",\n    \"enginesize\",\n)\n\nNotes\nI have considered these predictors for modeling further to predict the car price and in the next step I will again fit a model with these 7 variables on the outcome variable i.e car price.\n\nstroke\nbore\npeakrpm\ncompressionratio\nheight\nwidth\nenginesize\n\n\n\n\n\n_MODEL_FIT_2 = _model_spec_2.fit()\n\n\nprint(f'Trial-2#The Regression Table:##\\n{_MODEL_FIT_2.summary2()}')\n\nTrial-2#The Regression Table:##\n                       Results: Ordinary least squares\n==============================================================================\nModel:                   OLS                 Adj. R-squared:        0.828     \nDependent Variable:      car_price           AIC:                   3910.3622 \nDate:                    2024-04-12 21:13    BIC:                   3936.9463 \nNo. Observations:        205                 Log-Likelihood:        -1947.2   \nDf Model:                7                   F-statistic:           141.4     \nDf Residuals:            197                 Prob (F-statistic):    2.90e-73  \nR-squared:               0.834               Scale:                 1.0841e+07\n------------------------------------------------------------------------------\n                    Coef.     Std.Err.     t    P&gt;|t|     [0.025      0.975]  \n------------------------------------------------------------------------------\nconst            -69002.1067 11171.8990 -6.1764 0.0000 -91033.9744 -46970.2390\nstroke            -3610.8033   764.5756 -4.7226 0.0000  -5118.6069  -2102.9997\nbore              -1265.9900  1090.4152 -1.1610 0.2470  -3416.3750    884.3950\npeakrpm               3.0327     0.5840  5.1932 0.0000      1.8811      4.1844\ncompressionratio    199.4428    68.4690  2.9129 0.0040     64.4165    334.4690\nheight              177.1516   107.5631  1.6470 0.1012    -34.9714    389.2746\nwidth               778.1796   174.5819  4.4574 0.0000    433.8903   1122.4689\nenginesize          155.9542     9.4598 16.4860 0.0000    137.2987    174.6096\n------------------------------------------------------------------------------\nOmnibus:                   26.157           Durbin-Watson:              0.985 \nProb(Omnibus):             0.000            Jarque-Bera (JB):           75.943\nSkew:                      0.481            Prob(JB):                   0.000 \nKurtosis:                  5.822            Condition No.:              250464\n==============================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n[2] The condition number is large, 2.5e+05. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\nRegression Equation\n\\(\\hat{carprice} = {-69002.1067} +{-3610.80}*\\hat{stroke}+{-1265.9900}*\\hat{bore}+{3.0327}*\\hat{peakrpm}+{199.4428}*\\hat{compressionratio}+{177.1516}*\\hat{height}+{778.1796}*\\hat{width}+{155.9542}*\\hat{enginesize}+e\\)\nInferences\n\nThe fitted model has producted: R-SQuared IS (0.83) adn R-Squared(Adj) is 0.82, here the proportion of variation in Y explained by all the continous predictors is aproximately same as the first fitted model.\nThere are very few variables are not statistically significant. and we will look at the other regression diagnostics.\nThe Most of the betas have got the positive magnitudes unlike the earlier fitted model.\n\n\n\n\n\ncustom_ols_qqplot(_MODEL_FIT_2.resid)\n\n\n\n\n\n\n\n\nComment: The Q-Q plot indicates that the residuals are approximately normally distributed.\n\n\n\n\ncustom_ols_res_vs_fitted(_MODEL_FIT_2.fittedvalues, _MODEL_FIT_2.resid)\n\n\n\n\n\n\n\n\nComment: The fitted vs. residual plot suggests a random scatter of residuals, with no apparent trends. ofcourse there is a small cluster of points\n\n\n\n\ncustom_VIF(_model_spec_2)\n\n\n\n\n\n\n\n\nComment:\n\nVIF’s are Good for all the 7 predictors and the model can be used to predict the car prict with these predictors\n\n\nDF_ = pd.concat([df_cars_pd[\"make\"], df_cars_v1], axis=1)\n\n\nDF_7 = DF_[\n    DF_[\"make\"].isin(\n        [\"toyota\", \"nissan\", \"mazda\", \"mitsubishi\", \"honda\", \"subaru\", \"volkswagen\"]\n    )\n]\n\n\n\n\n\nDF_7_summary = DF_7.describe().reset_index()\n\n\nDF_TO_PREDICT = DF_7_summary.loc[:, los_sel_X]\n\n\n_df = pd.DataFrame(DF_TO_PREDICT.iloc[1]).T\n\n\n_df.head()\n\n\n\n\n\n\n\n\n\nstroke\nbore\npeakrpm\ncompressionratio\nheight\nwidth\nenginesize\n\n\n\n\n1\n3.254017\n3.249658\n5111.111111\n10.080342\n53.417949\n65.14188\n113.299145\n\n\n\n\n\n\n\n\nNote:\n\nFor these car makes “toyota”, “nissan”, “mazda”, “mitsubishi”, “honda”, “subaru”, “volkswagen” the mean values of each best predictor is caluclated and given them to regression model to predict the car price as below.\n\n\ncustom_model_preds(_MODEL_FIT_2, _df.reset_index())\n\n\n\n\n\n\n\n\n\npredicted_y\n\n\n\n\n0\n10469.921082\n\n\n\n\n\n\n\n\nNote:\n\nPredicted car price would be 10469 for the given mean values of each predictor variable."
  },
  {
    "objectID": "ISI_SSBB_BA_PE_CODE.html#statistical-analysis-on-sigma-cars-dataset.",
    "href": "ISI_SSBB_BA_PE_CODE.html#statistical-analysis-on-sigma-cars-dataset.",
    "title": "Periodical Examination.",
    "section": "",
    "text": "import re\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom scipy.stats import norm, poisson, ttest_1samp,mannwhitneyu,wilcoxon\nfrom statsmodels.stats.descriptivestats import sign_test\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")\nimport statsmodels.graphics.gofplots as gof\nfrom scipy.stats import chi2_contingency, chisquare\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats import outliers_influence as sm_oi\nfrom statsmodels.stats.anova import anova_lm\n\n\ndef cols_tidy(df_cols):\n    df_cols_tidy = []\n    for item in df_cols:\n        x = re.sub(r\" +\", \" \", item)\n        x1 = re.sub(r\"\\,|\\)|\\(|\\.|\\-|\\'\", \"\", x)\n        x2 = re.sub(r\"\\/\", \"_or_\", x1)\n        x3 = re.sub(r\"\\&\", \"_and_\", x2)\n        x4 = re.sub(r\"\\s(?=\\_{1,})|(\\_(?=\\s{1,}))|\\,\", \"\", x3)\n        x5 = re.sub(r\" {2,}\", \" \", x4)\n        x6 = re.sub(r\"\\s\", \"_\", x5)\n        x7 = re.sub(r\"\\:\", \"\", x6)\n        df_cols_tidy.append(x7.lower())\n    return df_cols_tidy\n\ndef calculate_center_line(data, column_name):\n    \"\"\"Calculates the center line for the control chart.\"\"\"\n    return data[column_name].mean()\n\n\ndef calculate_control_limits(center, average_defects_per_unit, multiplier):\n    \"\"\"Calculates upper and lower control limits.\"\"\"\n    std_dev = np.sqrt(average_defects_per_unit)\n    upper_limit = center + multiplier * std_dev\n    lower_limit = center - multiplier * std_dev\n    return upper_limit, lower_limit\n\n\ndef create_run_chart(data, column_name):\n    \"\"\"Creates a run chart for the given data.\"\"\"\n    plt.figure(figsize=(25, 6))\n    plt.plot(data.index, data[column_name], marker=\"o\", linestyle=\"-\")\n    plt.xlabel(\n        \"Projects-From Jan to Dec-2023\"\n    )  # Assuming the index represents time periods\n    plt.ylabel(column_name)  # Replace with the actual column name\n    plt.title(\"Run Chart for \" + column_name)\n    plt.grid(True)\n    plt.show()\n\ndef get_hypothesis(_p_val):\n    if _p_val&lt;0.05:\n        print(f'P-Value:{round(_p_val,5)} is less than 0.05 hence we can reject the null hypothesis in favor of alternative hypothesis')\n    else:\n        print(f'P-Value:{round(_p_val,5)} is greater than 0.05 hence we failed to reject the null hypothesis.')\n\ndef custom_ols_qqplot(_resid):\n    \"\"\"Q-Q Plot of residuals\"\"\"\n    gof.qqplot(_resid, line=\"s\")\n    plt.xlabel(\"Standard Normal Quantiles\")\n    plt.ylabel(\"Standardized Residuals\")\n    plt.title(\"Normal Q-Q plot\")\n    plt.show()\n\n\ndef custom_ols_res_vs_fitted(_fitted, _resid):\n    \"\"\"Fitted Vs Residuals Plot\"\"\"\n    plt.scatter(_fitted, _resid)\n    plt.axhline(\"0\", color=\"r\")\n    plt.xlabel(\"Fitted Values\")\n    plt.ylabel(\"Residual\")\n    plt.title(\"Residual Vs Fitted\")\n\n\ndef custom_VIF(_MSPEC):\n    \"\"\"Custom function to get the VIF\"\"\"\n    var_names = _MSPEC.exog_names\n    X = _MSPEC.exog\n    _limit = X.shape[1]\n    try:\n        vif_dict = {}\n        for idx in range(_limit):\n            vif = round(sm_oi.variance_inflation_factor(X, idx), 5)\n            vif_dict[var_names[idx]] = vif\n        _DF = pd.DataFrame([vif_dict]).T\n        _DF.columns = [\"VIF\"]\n        _DF = _DF.reset_index()\n        df_sorted = _DF.iloc[1:].sort_values(by=\"VIF\", ascending=False)\n        ax = sns.barplot(x=\"index\", y=\"VIF\", data=df_sorted)\n        # Add text labels to the top of each bar\n        for bar in ax.containers[0]:\n            ax.text(\n                bar.get_x() + bar.get_width() / 2,\n                bar.get_height(),\n                int(bar.get_height()),\n                ha=\"center\",\n                va=\"bottom\",\n            )\n        ax.set_xlabel(\"FIELD\")\n        ax.set_ylabel(\"VIF\")\n        plt.xticks(rotation=45)\n        plt.title(\"VIF\")\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        pass\n\ndef custom_statsmodel_OLS(_DF, *vars):\n    \"\"\"fitting OLS on specified independent and dependent variables- DF, dependent_var and independent_var\"\"\"\n    # sm.add_constant\n    try:\n        LOS_COLS = [v for v in vars]\n        _X = LOS_COLS[1:]\n        _Y = LOS_COLS[0]\n        xvars = sm.add_constant(_DF[_X])\n        yvar = _DF[_Y]\n        _model_spec = sm.OLS(yvar, xvars)\n        return _model_spec\n    except Exception as e:\n        print(f\"There is an error while creating a model spec due to:{e}\")\n\ndef custom_model_preds(_model, _new_df):\n    \"\"\"Predictions on new data points\"\"\"\n    _feat = sm.add_constant(_new_df)\n    _pred = _model.predict(sm.add_constant(_feat))\n    _df_pred = pd.DataFrame(_pred)\n    _df_pred.columns = [\"predicted_y\"]\n    return _df_pred\n\n\ndef get_six_sigma_caluclator():\n    \"\"\"ISI Custom SixSigma Calucator\"\"\"\n    while True:\n        ### Inputs\n        print(f\"-------------------------------------------------\")\n        print(f\"############ Sigma Caluclator Inputs ############\")\n        print(f\"-------------------------------------------------\")\n        _mean = float(input(\"Enter the mean:\"))\n        _sd = float(input(\"Enter Standard Deviation:\"))\n        _LSL = float(input(\"Enter LSL:\"))\n        _USL = float(input(\"Enter USL:\"))\n        # Formulas and caluclations\n        ZLSL = (_LSL - _mean) / _sd\n        ZUSL = (_USL - _mean) / _sd\n        Area_ZLSL = norm.cdf(ZLSL)\n        Area_ZUSL = 1 - norm.cdf(ZUSL)\n        TOTAL_NC = Area_ZLSL + Area_ZUSL\n        YIELD = 1 - TOTAL_NC\n        CP_ = (_USL - _LSL) / (6 * _sd)\n        _A = (_USL - _mean) / (3 * _sd)\n        _B = (_mean - _LSL) / (3 * _sd)\n        CPK_ = min(_A, _B)\n        SIGMA_LEVEL = round(1.5 + norm.ppf(YIELD), 5)\n        DPMO = TOTAL_NC * 1000000\n        # Output\n        print(f\"-------------------------------------------------\")\n        print(f\"#### Summary Report ####\")\n        print(f\"-------------------------------------------------\")\n        print(f\"Total NonConfirmances:{round(TOTAL_NC,5)}\")\n        print(f\"Yield:{round(YIELD,5)}\")\n        print(f\"CP:{round(CP_,5)}\")\n        print(f\"CPK:{round(CPK_,5)}\")\n        print(f\"SIGMA_LEVEL:{round(SIGMA_LEVEL,5)}\")\n        print(f\"DPMO:{round(DPMO,5)}\")\n        print(f\"-------------------------------------------------\")\n        _next = input(\n            \"Would you like to continue to use sigma caluclator type 'yes' if so :\"\n        )\n        if _next.lower() == \"yes\":\n            continue\n        else:\n            print(f\"Thanks for using Sigma Caluclator..\")\n            print(f\"#### END ####\")\n            break\n\n\n# Data Importings\ndf_cars = pl.read_csv(r\"/Users/malleshamyamulla/Desktop/SSBBA/CARS.csv\")\ndf_cars.columns = cols_tidy(df_cars.columns)\ndf_cars_pd = df_cars.to_pandas()\n\n\n\n\ndf_cars_pd.head()\n\n\n\n\n\n\n\n\n\nmake\nfueltype\naspiration\nenginelocation\nnumofdoors\ndrivewheels\nfuelsystem\nbodystyle\nenginetype\nnumofcylinders\n...\nenginesize\nbore\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\n_insurance_risk_rating\ncar_price\n\n\n\n\n0\nalfa-romero\ngas\nstd\nfront\ntwo\nrwd\nmpfi\nconvertible\ndohc\nfour\n...\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\nHigh\n13495\n\n\n1\nalfa-romero\ngas\nstd\nfront\ntwo\nrwd\nmpfi\nconvertible\ndohc\nfour\n...\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\nHigh\n16500\n\n\n2\nalfa-romero\ngas\nstd\nfront\ntwo\nrwd\nmpfi\nhatchback\nohcv\nsix\n...\n152\n2.68\n3.47\n9.0\n154\n5000\n19\n26\nMedium\n16500\n\n\n3\naudi\ngas\nstd\nfront\nfour\nfwd\nmpfi\nsedan\nohc\nfour\n...\n109\n3.19\n3.40\n10.0\n102\n5500\n24\n30\nHigh\n13950\n\n\n4\naudi\ngas\nstd\nfront\nfour\n4wd\nmpfi\nsedan\nohc\nfive\n...\n136\n3.19\n3.40\n8.0\n115\n5500\n18\n22\nHigh\n17450\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\n\n\n\n# car makes group by operation\ndf_causes = (\n    df_cars.group_by(\"make\")\n    .count()\n    .with_columns(pl.sum(\"count\").alias(\"tot_\"))\n    .with_columns(pl.col(\"count\") / pl.col(\"tot_\"))\n    .drop(\"tot_\")\n    .sort(\"count\", descending=True)\n    .with_columns(pl.col(\"count\").alias(\"RPN\"))\n    .drop(\"count\")\n    .to_pandas()\n)\n\n\n# Pareto Diagram\nsorted_data = df_causes.sort_values(by=[\"RPN\"], ascending=False)\nsorted_data[\"cumulative_freq\"] = sorted_data[\"RPN\"].cumsum()\nsorted_data[\"cumulative_pct\"] = (\n    sorted_data[\"cumulative_freq\"] / sorted_data[\"RPN\"].sum() * 100\n)\n\n# Visualizations\nfig, ax1 = plt.subplots(figsize=(22, 8))\nax2 = ax1.twinx()\nax1.bar(sorted_data[\"make\"], sorted_data[\"RPN\"], color=\"skyblue\")\nax2.plot(\n    sorted_data[\"make\"],\n    sorted_data[\"cumulative_pct\"],\n    color=\"red\",\n    marker=\"o\",\n    linestyle=\"-\",\n)\nax1.set_xlabel(\"Category\")\nax1.set_ylabel(\"Frequency\", color=\"skyblue\")\nax2.set_ylabel(\"Cumulative Percentage\", color=\"red\")\nplt.title(\"Pareto Analysis- Car Makers\")\nplt.show()\n\n\n\n\n\n\n\n\nInference\nThe below car makers are found to be top 7 performares in car sales.\n\ntoyota\nnissan\nmazda\nmitsubishi\nhonda\nsubaru\nvolkswagen\n\n\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1=sns.histplot(df_cars[\"citympg\"],ax=ax1)\np2=sns.boxplot(df_cars[\"citympg\"].to_pandas(),ax=ax2)\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nCars citympg data contains 2 assignable causes i.e citympg are more than 45, they are kept a side and will carry out the stability analysis on it.\n\n\ndf_city_mpg = df_cars.select([\"citympg\"]).filter(pl.col(\"citympg\") &lt; 43)\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\nsns.boxplot(df_city_mpg.select([\"citympg\"]).to_pandas(),ax=ax1)\nsns.histplot(df_city_mpg, kde=True,ax=ax2)\nplt.show()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_city_mpg.to_pandas().describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncitympg\n202.0\n24.89604\n6.017725\n13.0\n19.0\n24.0\n30.0\n38.0\n\n\n\n\n\n\n\n\n\ncreate_run_chart(df_city_mpg.to_pandas(), \"citympg\")\n\n\n\n\n\n\n\n\nInferences\n\nAfter removing the assignable the random chances are found to be nearly normal distributed, with a mean of 23.89 and standard deviation of 6.01.\nMean and Median Values are found to approximately equal.\nThe run chart shows no trends, neither upward nor downward.\nIn a conclusion the process is found to be statistically stable one.\n\n\n\n\nBelow are the Business Specifications\n0. CTQ: Car city MPG should alteast be 20.\n1. Mean: avg city-mpg ~ 24.89\n2. SD:   Standard Deviation ~ 6.01\n3. Tolerances:\n     - LSL: ~ 20\n     - USL: ~ 38(Max MPG found from the given data)\n\n#get_six_sigma_caluclator()\n\nCaluclated Performance Metrics on citympg:\n\nTotal NonConfirmances:0.2225\nYield:0.7775\nCP:0.49917\nCPK:0.27121\nSIGMA_LEVEL:2.26377\nDPMO:222502.72038\n\nInferences:\n\nCp(0.49): This value suggests the process is not very capable of consistently producing cars that meet the CTQ of 20 MPG city mileage.\nCpk(0.27): Cpk is lower than Cp, it means that the process isn’t performing well relative to the CTQ\nSigma Level(2.2): Process Spread is lower.\nDPMO(222502): This represents the estimated number of cars per million that would not meet the city MPG criteria based on the current process performance.\nYield(0.7775): - This indicates the proportion of cars (77.75%) that meet or exceed the city MPG requirement of at least 20.\nNC(0.25): This represents the proportion of cars (22.25%) that have a city MPG below the minimum acceptable standard of 20.\n\n\n\n\n\nHypothesis\n        Ho: The average selling price of cars at Sigma Cars is less than 1200.\n        Ha: The average selling price of cars at Sigma Cars is equal to or greater than 1200.\nDescriptive Stas on CarPrice\n\nprint(f'Descriptive Statistics of car price:')\npd.DataFrame(df_cars_pd['car_price'].describe()).T\n\nDescriptive Statistics of car price:\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncar_price\n205.0\n13243.243902\n7941.72182\n5118.0\n7788.0\n10295.0\n16503.0\n45400.0\n\n\n\n\n\n\n\n\nCarPrice Distribution\n\nsns.histplot(df_cars_pd['car_price'])\nplt.axvline(13243,color='red')\nplt.axvline(10295,color='black')\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nTotal Number of observation 205.\nCar prices data exhibits a right skewed distribution.\nCar Price mean(13243) is found be greater than median value(10295) as its right skewed distribution\nCar Price is not normally distributed, however the sample size is large enough i.e 205 so it would be ok to carry out 1 Sample T-test to test the postulated hypothesis.\n\n\n_stat_3c,_p_value_3c= ttest_1samp(df_cars_pd['car_price'],popmean=12000,alternative='greater')\n_3_1_1st=ttest_1samp(df_cars_pd['car_price'],popmean=12000,alternative='greater')\n_low,_high=_3_1_1st.confidence_interval()\nget_hypothesis(_p_value_3c)\n\nP-Value:0.01304 is less than 0.05 hence we can reject the null hypothesis in favor of alternative hypothesis\n\n\nInferences\n\nI have carried out a one-sample t-test(One-tailed) on car price to see if the car price is on or above 12000.\nThe caluclated p-value(0.013) suggest that we can reject the null hypothesis in favor of alternative meaning The average selling price of cars at Sigma Cars is equal to or greater than 1200 and the sigma cars would have a potential business to do.\n\n\ndf_cars_pd.loc[df_cars_pd['car_price']&gt;=12000,'car_price_seg']='&gt;12000'\ndf_cars_pd.loc[df_cars_pd['car_price']&lt;12000,'car_price_seg']='&lt;12000'\n\n\nsns.barplot(pd.DataFrame(df_cars_pd['car_price_seg'].value_counts()).reset_index(),x='car_price_seg',y='count')\nplt.show()\n\n\n\n\n\n\n\n\nNotes\nI have categorized car prices into two groups such as &lt;12000 and &gt;=12000 and 40% of the car prices fall into the higher price segment (at or above 12000).\n\n\n\n- fueltype\n- aspiration\n- enginelocation\n- numofdoors\n\ndf_3d = df_cars_pd[['fueltype', 'aspiration','numofdoors','enginelocation','car_price']]\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\nsns.boxplot(x='fueltype',y='car_price',data=df_3d,ax=axes[0,0])\nsns.boxplot(x='aspiration',y='car_price',data=df_3d,ax=axes[0,1])\nsns.boxplot(x='numofdoors',y='car_price',data=df_3d,ax=axes[1,0])\nsns.boxplot(x='enginelocation',y='car_price',data=df_3d,ax=axes[1,1])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define the model formula\ndf_3d_model = ols(\n    \"car_price ~ fueltype+aspiration+enginelocation+numofdoors\", data=df_3d\n).fit()\n\n# Perform ANOVA\nanova_table_3d = anova_lm(df_3d_model)\n\n# Print ANOVA results\nanova_table_3d.apply(lambda x:round(x,5))\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nfueltype\n1.0\n1.492298e+08\n1.492298e+08\n2.74410\n0.09918\n\n\naspiration\n1.0\n2.987693e+08\n2.987693e+08\n5.49390\n0.02006\n\n\nenginelocation\n1.0\n1.480249e+09\n1.480249e+09\n27.21943\n0.00000\n\n\nnumofdoors\n1.0\n6.181319e+07\n6.181319e+07\n1.13665\n0.28765\n\n\nResidual\n200.0\n1.087641e+10\n5.438206e+07\nNaN\nNaN\n\n\n\n\n\n\n\n\nInferences:\n\nThe variables fueltype, aspiration, enginelocation and numofdoors are having the same number of degrees of freedom.\nFuelType: F value 2.77 and P-value 0.09.It’s not significant as it has higher p-value than 0.05\nAspiration: F Value 5.49 and P-value 0.002. It’s a statistical significant variable with lower p-value less than 0.02\nEnginelocation: F-Value 27.21 and P-Value 0.0, It’s also a statistical significant variable with lower p-value less than 0.0\nNumofdoors: F-Value 6.18 and P-Value 0.28. sIt’s not significant as it has higher p-value than 0.05\nIn a conclusion: Aspiration and EngineLocation Variable would have an advantage in car pricing. If we were to select only of these two enginee location would be the most appropriate variable in car pricing.\n\n\n\n\nHypothesis\n        Ho: There is no statistically significant difference in the average price of cars between drive wheel and fuel systems.\n        Ha: There is a statistically significant difference in the average price of cars between drive wheel and fuel systems.\n\ndf_3e = df_cars_pd[['drivewheels','fuelsystem','car_price']]\n\n\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\nsns.boxplot(x='drivewheels',y='car_price',data=df_3e,ax=ax1)\nsns.boxplot(x='fuelsystem',y='car_price',data=df_3e,ax=ax2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n# Define the model formula\ndf_3e_model = ols(\n    \"car_price ~ drivewheels+fuelsystem\", data=df_3e\n).fit()\n\n# Perform ANOVA\nanova_table_3e = anova_lm(df_3e_model)\n\n# Print ANOVA results\nanova_table_3e.apply(lambda x:round(x,5))\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\ndrivewheels\n2.0\n5.274374e+09\n2.637187e+09\n79.15850\n0.00000\n\n\nfuelsystem\n3.0\n9.623598e+08\n3.207866e+08\n9.62882\n0.00001\n\n\nResidual\n199.0\n6.629739e+09\n3.331527e+07\nNaN\nNaN\n\n\n\n\n\n\n\n\nInferences:\n\nThe variables DriveWheel and fuelsystem are having the different number of degrees of freedom. 2 and 3 respectively.\nDriveWheel: F value 79.15 and P-value 0.0 suggest to reject the null hypothesis.\nFuelType: F Value 9.62 and P-value 0.0 also suggest to reject the null hypothesis.\nDrivewheel F-Value is much higher than to FuelType, This suggests a potentially stronger effect of drive wheel on car price compared to fuel type.\nIn a conclusion: There is a statistically significant difference in the average price of cars between drive wheel and fuel systems.\n\n\n\n\n\nHypothesis-1\n        Ho: On average, the price of a car isn't affected by its body style.\n        Ha: On average, the price of a car is affected by its body style.\nHypothesis-2\n        Ho: On average, the price of a car isn't affected by its engine type.\n        Ha: On average, the price of a car is affected by its engine type.\n\ndf_3f = df_cars_pd[['bodystyle','enginetype','car_price']]\n\n\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\nsns.boxplot(x='bodystyle',y='car_price',data=df_3f,ax=ax1)\nsns.boxplot(x='enginetype',y='car_price',data=df_3f,ax=ax2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n# Define the model formula\ndf_3f_model = ols(\n    \"car_price ~ bodystyle+enginetype\", data=df_3f\n).fit()\n\n# Perform ANOVA\nanova_table_3f = anova_lm(df_3f_model)\n\n# Print ANOVA results\nanova_table_3f.apply(lambda x:round(x,5))\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nbodystyle\n4.0\n1.826663e+09\n4.566658e+08\n10.25081\n0.0\n\n\nenginetype\n4.0\n2.308158e+09\n5.770395e+08\n12.95284\n0.0\n\n\nResidual\n196.0\n8.731652e+09\n4.454924e+07\nNaN\nNaN\n\n\n\n\n\n\n\n\nInferences:\n\nThe variables Bodystyle and Engine type are having the same number of degrees of freedom.\nBodystyle: F value 10.25 and P-value 0.0 suggest to reject the null hypothesis.\nEngineType: F Value 12.95 and P-value 0.0 also suggest to reject the null hypothesis.\nEngineType F-Value is little higher than to Bodytype.\nIn a conclusion: The average car price is affected by both BodyStyle and EngineType.\n\n\n\n\n\nHypothesis\n        Ho: The average car price is the same across all cylinder levels-there is no statistically significant relationship between the number of cylinders in a car and its average selling price. \n        Ha: at least one mean car price is different across all cylinder levels-There is a statistically significant relationship between the number of cylinders in a car and its average selling price.\n\ndf_3g=df_cars_pd[['numofcylinders','car_price']]\n\n\n\n\nsns.boxplot(x='numofcylinders',y='car_price',data=df_3g)\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nThe Numofcylinders are having 5 levels.\nThe carprice data per each of these levels is distributed differently from the above box plot. The boxplot reveals that the distribution of car prices differs significantly across these cylinder levels.\n\n\n\n\n\n# Define the model formula\ndf_3g_model = ols(\n    \"car_price ~ numofcylinders\", data=df_3g\n).fit()\n\n# Perform ANOVA\nanova_table_3g = anova_lm(df_3g_model)\n\n# Print ANOVA results\nanova_table_3g.apply(lambda x:round(x,5))\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nnumofcylinders\n4.0\n7.964193e+09\n1.991048e+09\n81.22947\n0.0\n\n\nResidual\n200.0\n4.902280e+09\n2.451140e+07\nNaN\nNaN\n\n\n\n\n\n\n\n\nInferences\n\nFrom the ANOVA Table: For the numofcylinders variable The F-Value is moderately high i.e 81.22 while the caluclated P value(0.0) suggest that we can reject the null hypothesis in favor of alternative i.e the Mean car prices are different across the numberof cylinders.\n\n\n\n\n\nHypothesis:1\n    Ho: The insurance risk ratings are same for all Fuel Systems, Engine type does not have any association with insurance risk There is no association between the fuelsystem and insurance risk rating levels.\n    \n    Ha: The distribution of insurance risk ratings differs from each Fuel System,There is an association between the fuelsystem and insurance risk rating levels.\n    \nHypothesis:2\n        Ho: The insurance risk ratings are same for all engine types, Engine type does not have any association with insurance risk.\n        Ha: The distribution of insurance risk ratings differs from each engine type, engine type does have an association.\n\ndf_3h=df_cars_pd[['fuelsystem','enginetype','_insurance_risk_rating']]\ndf_3h_agg=df_3h.groupby(['fuelsystem', '_insurance_risk_rating']).count().reset_index()\ndf_3h_agg1=df_3h.groupby(['enginetype', '_insurance_risk_rating']).count().reset_index()\n\nVIS-1:FuelSystem ~ Insurance_Risk_Rating\n\nsns.barplot(\n    df_3h_agg, x=\"fuelsystem\", y=\"enginetype\", hue=\"_insurance_risk_rating\"\n)\nplt.ylabel('count')\nplt.show()\n\n\n\n\n\n\n\n\nNotes:\n\nThere are four levels of fuel system and 3 levels of insurance risk rating.\nThe graph shows the counts of each insurance risk rating per each of fuel system.\nFuel system 1bbl has only got 2 insurance ratings, and the other 3 have the three different risk ratings in different proportions.\n\nVIS-2:EngineType ~ Insurance_Risk_Rating\n\nsns.barplot(\n    df_3h_agg1, x=\"enginetype\", y=\"fuelsystem\", hue=\"_insurance_risk_rating\"\n)\nplt.ylabel('count')\nplt.show()\n\n\n\n\n\n\n\n\nNotes:\n\nThere are 5 levels of enginetype and 3 levels of insurance risk rating.\nThe graph shows the counts of each insurance risk rating per each of engine system.\nAll engine types have got the insurance risk rating, 4 of th engine type levels are found to be having very similar proportions across the three risk rating categories.\n\n\n\n\n## Chi-Square Test-1\ndf_ecom_CT = pd.crosstab(df_3h[\"fuelsystem\"], df_3h[\"_insurance_risk_rating\"]).reset_index().drop([\"fuelsystem\"], axis=1)\n\nCS_LVEL = df_ecom_CT.to_numpy()\n\n_chi2, _pvalue, _ddof, _expected = chi2_contingency(CS_LVEL)\n\nget_hypothesis(_pvalue)\n\nP-Value:1e-05 is less than 0.05 hence we can reject the null hypothesis in favor of alternative hypothesis\n\n\n\n\n\n\n## Chi-Square Test-1\ndf_ecom_CT = pd.crosstab(df_3h[\"enginetype\"], df_3h[\"_insurance_risk_rating\"]).reset_index().drop([\"enginetype\"], axis=1)\n\nCS_LVEL = df_ecom_CT.to_numpy()\n\n_chi2_, _pvalue_cs, _ddof1, _expected1 = chi2_contingency(CS_LVEL)\n\nget_hypothesis(_pvalue_cs)\n\nP-Value:0.4149 is greater than 0.05 hence we failed to reject the null hypothesis.\n\n\nConclusions\n\nThe ChiSquare Associaton test on EngineType and Insurance Risk Rating suggests that the Engine type does not have any association with insurance risk. here with higher p-value we failed to reject the null hypothesis.\n\n\n\n\n\n\nlos_con_cols = [\n    \"wheelbase\",\n    \"length\",\n    \"width\",\n    \"height\",\n    \"curbweight\",\n    \"enginesize\",\n    \"bore\",\n    \"stroke\",\n    \"compressionratio\",\n    \"horsepower\",\n    \"peakrpm\",\n    \"citympg\",\n    \"highwaympg\",\n    \"car_price\",\n]\n\n\ndf_cars_v1 = df_cars_pd[los_con_cols]\n\n\n\n\ndf_cars_v1.head()\n\n\n\n\n\n\n\n\n\nwheelbase\nlength\nwidth\nheight\ncurbweight\nenginesize\nbore\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\ncar_price\n\n\n\n\n0\n88.6\n168.8\n64.1\n48.8\n2548\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495\n\n\n1\n88.6\n168.8\n64.1\n48.8\n2548\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500\n\n\n2\n94.5\n171.2\n65.5\n52.4\n2823\n152\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500\n\n\n3\n99.8\n176.6\n66.2\n54.3\n2337\n109\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950\n\n\n4\n99.4\n176.6\n66.4\n54.3\n2824\n136\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450\n\n\n\n\n\n\n\n\n\n\n\n\nsns.heatmap(df_cars_v1.corr(), annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nX = sm.add_constant(df_cars_v1.drop([\"car_price\"], axis=1))\nY = df_cars_v1[\"car_price\"]\n_model_spec = sm.OLS(Y, X)\n_MODEL_FIT = _model_spec.fit()\n\n\n\n\n\nprint(f'Trial-1#The Regression Table:##\\n{_MODEL_FIT.summary2()}')\n\nTrial-1#The Regression Table:##\n                       Results: Ordinary least squares\n=============================================================================\nModel:                  OLS                 Adj. R-squared:        0.838     \nDependent Variable:     car_price           AIC:                   3904.1448 \nDate:                   2024-04-12 21:13    BIC:                   3950.6669 \nNo. Observations:       205                 Log-Likelihood:        -1938.1   \nDf Model:               13                  F-statistic:           82.05     \nDf Residuals:           191                 Prob (F-statistic):    8.95e-71  \nR-squared:              0.848               Scale:                 1.0230e+07\n-----------------------------------------------------------------------------\n                    Coef.     Std.Err.     t    P&gt;|t|     [0.025     0.975]  \n-----------------------------------------------------------------------------\nconst            -37199.2505 14901.3433 -2.4964 0.0134 -66591.5838 -7806.9173\nwheelbase           106.6533   100.7882  1.0582 0.2913    -92.1475   305.4541\nlength              -71.0280    55.7670 -1.2737 0.2043   -181.0263    38.9702\nwidth               381.1637   247.0076  1.5431 0.1245   -106.0493   868.3767\nheight              191.9187   135.4189  1.4172 0.1580    -75.1899   459.0272\ncurbweight            1.9427     1.7428  1.1147 0.2664     -1.4949     5.3803\nenginesize          126.3904    13.9795  9.0411 0.0000     98.8163   153.9645\nbore              -2191.7312  1105.5166 -1.9825 0.0489  -4372.3207   -11.1417\nstroke            -3357.5078   756.6845 -4.4371 0.0000  -4850.0391 -1864.9764\ncompressionratio    279.3676    82.8764  3.3709 0.0009    115.8970   442.8383\nhorsepower           24.1233    16.1420  1.4944 0.1367     -7.7163    55.9628\npeakrpm               2.1481     0.6599  3.2554 0.0013      0.8465     3.4496\ncitympg            -277.5227   177.7650 -1.5612 0.1201   -628.1574    73.1119\nhighwaympg          151.2249   160.3563  0.9431 0.3468   -165.0718   467.5216\n-----------------------------------------------------------------------------\nOmnibus:                 25.401           Durbin-Watson:              0.997  \nProb(Omnibus):           0.000            Jarque-Bera (JB):           104.409\nSkew:                    0.301            Prob(JB):                   0.000  \nKurtosis:                6.444            Condition No.:              383868 \n=============================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n[2] The condition number is large, 3.84e+05. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\nInferences\n\nThe fitted model has producted: R-SQuared IS (0.84) adn R-Squared(Adj) is 0.83, and it means the proportion of variation in Y explained by all the continous predictors. the numbers are pretty OK.\nThe p-value represents the probability of observing a slope as extreme or more extreme as the one we calculated in the sample, assuming there is truly no relationship between X and Y i.e null hypothesis and an alternative hypothesis would be there is a truly relationship between X and Y.\nConsidering the above point-2. we would nowlook at all the betas of fitted model and their P-values to determine if they are OK to predict the car price. they below mentioned 4 predictors are only statistically significant for model predictions. and we would also check other regression diagnostics to take a call on it.\n\nenginesize\nstroke\ncompressionratio\npeakrpm\n\n\n\ndf_Betas_with_alpha = pd.DataFrame(_MODEL_FIT.params).reset_index().iloc[1:, :]\ndf_Betas_with_alpha.columns = [\"betas\", \"val\"]\ndf_Betas_with_SE = (\n    pd.concat([_MODEL_FIT.params, _MODEL_FIT.bse], axis=1).reset_index().iloc[1:, :]\n)\ndf_Betas_with_SE.columns = [\"betas\", \"val\", \"se_\"]\n\n\n\n\n\nsns.barplot(\n    y=\"betas\", x=\"val\", data=df_Betas_with_SE.sort_values(\"val\", ascending=False)\n)\nplt.show()\n\n\n\n\n\n\n\n\nNotes\nThe beta coefficients for half of the predictor variables are negative. In other words, a one-unit increase in these variables is likely to lead to a decrease in the value of the response variable, but the magnitude of that decrease is not specified here.\n\n\n\n\ncustom_ols_qqplot(_MODEL_FIT.resid)\n\n\n\n\n\n\n\n\nComment: The Q-Q plot indicates that the residuals are approximately normally distributed.\n\n\n\n\ncustom_ols_res_vs_fitted(_MODEL_FIT.fittedvalues, _MODEL_FIT.resid)\n\n\n\n\n\n\n\n\nComment: The fitted vs. residual plot suggests that there is no random scatter of residuals, and there are some clusters\n\n\n\n\ncustom_VIF(_model_spec)\n\n\n\n\n\n\n\n\nComment:\n\nVIF value for the below predictors are high and This suggests that these variables may have multicollinearity with other independent variables in the model.\n\ncitympg\nhihwaympg\ncurbweight\nlength\nhorsepower\nwheelbase\n\nThe below predictors which have VIF less than or equal to 5 are considered to be the better predictors in regression modeling.\n\nstroke\nbore\npeakrpm\ncompressionratio\nheight\nwidth\nenginesize\n\n\n\n\n\n\nlos_sel_X = [\n    \"stroke\",\n    \"bore\",\n    \"peakrpm\",\n    \"compressionratio\",\n    \"height\",\n    \"width\",\n    \"enginesize\",\n]\n\n\nsns.heatmap(df_cars_v1[[ \"car_price\",\n    \"stroke\",\n    \"bore\",\n    \"peakrpm\",\n    \"compressionratio\",\n    \"height\",\n    \"width\",\n    \"enginesize\"]].corr(),annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n_model_spec_2 = custom_statsmodel_OLS(\n    df_cars_v1,\n    \"car_price\",\n    \"stroke\",\n    \"bore\",\n    \"peakrpm\",\n    \"compressionratio\",\n    \"height\",\n    \"width\",\n    \"enginesize\",\n)\n\nNotes\nI have considered these predictors for modeling further to predict the car price and in the next step I will again fit a model with these 7 variables on the outcome variable i.e car price.\n\nstroke\nbore\npeakrpm\ncompressionratio\nheight\nwidth\nenginesize\n\n\n\n\n\n_MODEL_FIT_2 = _model_spec_2.fit()\n\n\nprint(f'Trial-2#The Regression Table:##\\n{_MODEL_FIT_2.summary2()}')\n\nTrial-2#The Regression Table:##\n                       Results: Ordinary least squares\n==============================================================================\nModel:                   OLS                 Adj. R-squared:        0.828     \nDependent Variable:      car_price           AIC:                   3910.3622 \nDate:                    2024-04-12 21:13    BIC:                   3936.9463 \nNo. Observations:        205                 Log-Likelihood:        -1947.2   \nDf Model:                7                   F-statistic:           141.4     \nDf Residuals:            197                 Prob (F-statistic):    2.90e-73  \nR-squared:               0.834               Scale:                 1.0841e+07\n------------------------------------------------------------------------------\n                    Coef.     Std.Err.     t    P&gt;|t|     [0.025      0.975]  \n------------------------------------------------------------------------------\nconst            -69002.1067 11171.8990 -6.1764 0.0000 -91033.9744 -46970.2390\nstroke            -3610.8033   764.5756 -4.7226 0.0000  -5118.6069  -2102.9997\nbore              -1265.9900  1090.4152 -1.1610 0.2470  -3416.3750    884.3950\npeakrpm               3.0327     0.5840  5.1932 0.0000      1.8811      4.1844\ncompressionratio    199.4428    68.4690  2.9129 0.0040     64.4165    334.4690\nheight              177.1516   107.5631  1.6470 0.1012    -34.9714    389.2746\nwidth               778.1796   174.5819  4.4574 0.0000    433.8903   1122.4689\nenginesize          155.9542     9.4598 16.4860 0.0000    137.2987    174.6096\n------------------------------------------------------------------------------\nOmnibus:                   26.157           Durbin-Watson:              0.985 \nProb(Omnibus):             0.000            Jarque-Bera (JB):           75.943\nSkew:                      0.481            Prob(JB):                   0.000 \nKurtosis:                  5.822            Condition No.:              250464\n==============================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is\ncorrectly specified.\n[2] The condition number is large, 2.5e+05. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\nRegression Equation\n\\(\\hat{carprice} = {-69002.1067} +{-3610.80}*\\hat{stroke}+{-1265.9900}*\\hat{bore}+{3.0327}*\\hat{peakrpm}+{199.4428}*\\hat{compressionratio}+{177.1516}*\\hat{height}+{778.1796}*\\hat{width}+{155.9542}*\\hat{enginesize}+e\\)\nInferences\n\nThe fitted model has producted: R-SQuared IS (0.83) adn R-Squared(Adj) is 0.82, here the proportion of variation in Y explained by all the continous predictors is aproximately same as the first fitted model.\nThere are very few variables are not statistically significant. and we will look at the other regression diagnostics.\nThe Most of the betas have got the positive magnitudes unlike the earlier fitted model.\n\n\n\n\n\ncustom_ols_qqplot(_MODEL_FIT_2.resid)\n\n\n\n\n\n\n\n\nComment: The Q-Q plot indicates that the residuals are approximately normally distributed.\n\n\n\n\ncustom_ols_res_vs_fitted(_MODEL_FIT_2.fittedvalues, _MODEL_FIT_2.resid)\n\n\n\n\n\n\n\n\nComment: The fitted vs. residual plot suggests a random scatter of residuals, with no apparent trends. ofcourse there is a small cluster of points\n\n\n\n\ncustom_VIF(_model_spec_2)\n\n\n\n\n\n\n\n\nComment:\n\nVIF’s are Good for all the 7 predictors and the model can be used to predict the car prict with these predictors\n\n\nDF_ = pd.concat([df_cars_pd[\"make\"], df_cars_v1], axis=1)\n\n\nDF_7 = DF_[\n    DF_[\"make\"].isin(\n        [\"toyota\", \"nissan\", \"mazda\", \"mitsubishi\", \"honda\", \"subaru\", \"volkswagen\"]\n    )\n]\n\n\n\n\n\nDF_7_summary = DF_7.describe().reset_index()\n\n\nDF_TO_PREDICT = DF_7_summary.loc[:, los_sel_X]\n\n\n_df = pd.DataFrame(DF_TO_PREDICT.iloc[1]).T\n\n\n_df.head()\n\n\n\n\n\n\n\n\n\nstroke\nbore\npeakrpm\ncompressionratio\nheight\nwidth\nenginesize\n\n\n\n\n1\n3.254017\n3.249658\n5111.111111\n10.080342\n53.417949\n65.14188\n113.299145\n\n\n\n\n\n\n\n\nNote:\n\nFor these car makes “toyota”, “nissan”, “mazda”, “mitsubishi”, “honda”, “subaru”, “volkswagen” the mean values of each best predictor is caluclated and given them to regression model to predict the car price as below.\n\n\ncustom_model_preds(_MODEL_FIT_2, _df.reset_index())\n\n\n\n\n\n\n\n\n\npredicted_y\n\n\n\n\n0\n10469.921082\n\n\n\n\n\n\n\n\nNote:\n\nPredicted car price would be 10469 for the given mean values of each predictor variable."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nI have contributed to data breach analysis projects using Six Sigma and analytics methodologies,trained under Professor Murali Rao Garimella at the Indian Statistical Institute Hyderabad.\nCurrently developing a guide on applying Six Sigma and Business Analytics for data-driven problem-solving.\nThis repository contains resources, code snippets, and case studies related to my research on Six Sigma and Business Analytics methodologies.\n\n\nCertificate\n\n\n\nSSBB-BA-Certificate\n\n\n\n\n\nSSBB-BA-Syllabus\n\n\n\n\n\nSSBB-BA-Result-Email"
  },
  {
    "objectID": "ISI_SSBB_BA_FinalExam_Code_2024.html",
    "href": "ISI_SSBB_BA_FinalExam_Code_2024.html",
    "title": "Final Examination",
    "section": "",
    "text": "# Importing Necessary libraries\nimport math\nimport re\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.graphics.gofplots as gof\nimport xgboost as xgb\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom feature_engine.imputation import MeanMedianImputer\n\n# the search algorithms\nfrom hyperopt import anneal, fmin, hp, rand, tpe\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import AllKNN, NearMiss, RandomUnderSampler, TomekLinks\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy import stats\nfrom scipy.stats import (\n    chi2_contingency,\n    chisquare,\n    mannwhitneyu,\n    norm,\n    poisson,\n    ttest_1samp,\n    ttest_ind,\n    wilcoxon,\n)\nfrom sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\nfrom sklearn.ensemble import (\n    GradientBoostingClassifier,\n    GradientBoostingRegressor,\n    RandomForestClassifier,\n    RandomForestRegressor,\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import (\n    ConfusionMatrixDisplay,\n    RocCurveDisplay,\n    accuracy_score,\n    confusion_matrix,\n    f1_score,\n    mean_absolute_error,\n    mean_squared_error,\n    precision_score,\n    r2_score,\n    recall_score,\n    roc_auc_score,\n    root_mean_squared_error,\n    silhouette_score,\n)\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_val_score,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats import outliers_influence as sm_oi\nfrom statsmodels.stats.anova import anova_lm\nfrom statsmodels.stats.descriptivestats import sign_test\nfrom statsmodels.stats.proportion import proportions_ztest\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom yellowbrick.model_selection import FeatureImportances, ValidationCurve\nfrom yellowbrick.regressor import PredictionError, ResidualsPlot\nfrom yellowbrick.target import FeatureCorrelation\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")\n\n\n## Custom Functions created by mallesham yamulla\ndef cols_tidy(df_cols):\n    df_cols_tidy = []\n    for item in df_cols:\n        x = re.sub(r\" +\", \" \", item)\n        x1 = re.sub(r\"\\,|\\)|\\(|\\.|\\-|\\'\", \"\", x)\n        x2 = re.sub(r\"\\/\", \"_or_\", x1)\n        x3 = re.sub(r\"\\&\", \"_and_\", x2)\n        x4 = re.sub(r\"\\s(?=\\_{1,})|(\\_(?=\\s{1,}))|\\,\", \"\", x3)\n        x5 = re.sub(r\" {2,}\", \" \", x4)\n        x6 = re.sub(r\"\\s\", \"_\", x5)\n        x7 = re.sub(r\"\\:\", \"\", x6)\n        df_cols_tidy.append(x7.lower())\n    return df_cols_tidy\n\n\ndef calculate_center_line(data, column_name):\n    \"\"\"Calculates the center line for the control chart.\"\"\"\n    return data[column_name].mean()\n\n\ndef calculate_control_limits(center, average_defects_per_unit, multiplier):\n    \"\"\"Calculates upper and lower control limits.\"\"\"\n    std_dev = np.sqrt(average_defects_per_unit)\n    upper_limit = center + multiplier * std_dev\n    lower_limit = center - multiplier * std_dev\n    return upper_limit, lower_limit\n\n\ndef create_run_chart(data, column_name):\n    \"\"\"Creates a run chart for the given data.\"\"\"\n    plt.figure(figsize=(25, 6))\n    plt.plot(data.index, data[column_name], marker=\"o\", linestyle=\"-\")\n    plt.xlabel(\n        \"Projects-From Jan to Dec-2023\"\n    )  # Assuming the index represents time periods\n    plt.ylabel(column_name)  # Replace with the actual column name\n    plt.title(\"Run Chart for \" + column_name)\n    plt.grid(True)\n    plt.show()\n\n\ndef get_hypothesis(_p_val):\n    if _p_val &lt; 0.05:\n        print(\n            f\"P-Value:{round(_p_val,5)} is less than 0.05 hence we can reject the null hypothesis in favor of alternative hypothesis\"\n        )\n    else:\n        print(\n            f\"P-Value:{round(_p_val,5)} is greater than 0.05 hence we failed to reject the null hypothesis.\"\n        )\n\n\ndef custom_ols_qqplot(_resid):\n    \"\"\"Q-Q Plot of residuals\"\"\"\n    gof.qqplot(_resid, line=\"s\")\n    plt.xlabel(\"Standard Normal Quantiles\")\n    plt.ylabel(\"Standardized Residuals\")\n    plt.title(\"Normal Q-Q plot\")\n    plt.show()\n\n\ndef custom_ols_res_vs_fitted(_fitted, _resid):\n    \"\"\"Fitted Vs Residuals Plot\"\"\"\n    plt.scatter(_fitted, _resid)\n    plt.axhline(\"0\", color=\"r\")\n    plt.xlabel(\"Fitted Values\")\n    plt.ylabel(\"Residual\")\n    plt.title(\"Residual Vs Fitted\")\n\n\ndef custom_VIF(_MSPEC):\n    \"\"\"Custom function to get the VIF\"\"\"\n    var_names = _MSPEC.exog_names\n    X = _MSPEC.exog\n    _limit = X.shape[1]\n    try:\n        vif_dict = {}\n        for idx in range(_limit):\n            vif = round(sm_oi.variance_inflation_factor(X, idx), 5)\n            vif_dict[var_names[idx]] = vif\n        _DF = pd.DataFrame([vif_dict]).T\n        _DF.columns = [\"VIF\"]\n        _DF = _DF.reset_index()\n        df_sorted = _DF.iloc[1:].sort_values(by=\"VIF\", ascending=False)\n        fig, ax = plt.subplots(figsize=(30, 12))\n        ax = sns.barplot(x=\"index\", y=\"VIF\", data=df_sorted)\n        # Add text labels to the top of each bar\n        for bar in ax.containers[0]:\n            ax.text(\n                bar.get_x() + bar.get_width() / 2,\n                bar.get_height(),\n                int(bar.get_height()),\n                ha=\"center\",\n                va=\"bottom\",\n            )\n        ax.set_xlabel(\"FIELD\")\n        ax.set_ylabel(\"VIF\")\n        # plt.xticks(rotation=45)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n        plt.title(\"VIF\")\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        pass\n\n\ndef custom_statsmodel_OLS(_DF, *vars):\n    \"\"\"fitting OLS on specified independent and dependent variables- DF, dependent_var and independent_var\"\"\"\n    # sm.add_constant\n    try:\n        LOS_COLS = [v for v in vars]\n        _X = LOS_COLS[1:]\n        _Y = LOS_COLS[0]\n        xvars = sm.add_constant(_DF[_X])\n        yvar = _DF[_Y]\n        _model_spec = sm.OLS(yvar, xvars)\n        return _model_spec\n    except Exception as e:\n        print(f\"There is an error while creating a model spec due to:{e}\")\n\n\ndef custom_model_preds(_model, _new_df):\n    \"\"\"Predictions on new data points\"\"\"\n    _feat = sm.add_constant(_new_df)\n    _pred = _model.predict(sm.add_constant(_feat))\n    _df_pred = pd.DataFrame(_pred)\n    _df_pred.columns = [\"predicted_y\"]\n    return _df_pred\n\n\ndef get_six_sigma_caluclator():\n    \"\"\"ISI Custom SixSigma Calucator\"\"\"\n    while True:\n        ### Inputs\n        print(f\"-------------------------------------------------\")\n        print(f\"######### Sigma Caluclator Inputs #########\")\n        print(f\"-------------------------------------------------\")\n        _mean = float(input(\"Enter the mean:\"))\n        _sd = float(input(\"Enter Standard Deviation:\"))\n        _LSL = float(input(\"Enter LSL:\"))\n        _USL = float(input(\"Enter USL:\"))\n        # Formulas and caluclations\n        ZLSL = (_LSL - _mean) / _sd\n        ZUSL = (_USL - _mean) / _sd\n        Area_ZLSL = norm.cdf(ZLSL)\n        Area_ZUSL = 1 - norm.cdf(ZUSL)\n        TOTAL_NC = Area_ZLSL + Area_ZUSL\n        YIELD = 1 - TOTAL_NC\n        CP_ = (_USL - _LSL) / (6 * _sd)\n        _A = (_USL - _mean) / (3 * _sd)\n        _B = (_mean - _LSL) / (3 * _sd)\n        CPK_ = min(_A, _B)\n        SIGMA_LEVEL = round(1.5 + norm.ppf(YIELD), 5)\n        DPMO = TOTAL_NC * 1000000\n        # Output\n        print(f\"-------------------------------------------------\")\n        print(f\"### Summary Report ###\")\n        print(f\"-------------------------------------------------\")\n        print(f\"Total NonConfirmances:{round(TOTAL_NC,5)}\")\n        print(f\"Yield:{round(YIELD,5)}\")\n        print(f\"CP:{round(CP_,5)}\")\n        print(f\"CPK:{round(CPK_,5)}\")\n        print(f\"SIGMA_LEVEL:{round(SIGMA_LEVEL,5)}\")\n        print(f\"DPMO:{round(DPMO,5)}\")\n        print(f\"-------------------------------------------------\")\n        _next = input(\n            \"Would you like to continue to use sigma caluclator type 'yes' if so :\"\n        )\n        if _next.lower() == \"yes\":\n            continue\n        else:\n            print(f\"Thanks for using Sigma Caluclator..\")\n            print(f\"### END ###\")\n            break\n\n\ndef custom_classification_metrics_report(_model_type, _actual, _predicted):\n    print(f\"Model:{_model_type} Classification Report..\")\n    print(f\"Accuary:{round(accuracy_score(_actual,_predicted),5)}\")\n    print(f\"ROC_AUC:{round(roc_auc_score(_actual,_predicted),5)}\")\n    _plt = ConfusionMatrixDisplay(confusion_matrix(_actual, _predicted))\n    _plt.plot()\n\n\ndef get_regression_plots(_MODEL_FIT):\n    df_Betas_with_alpha = pd.DataFrame(_MODEL_FIT.params).reset_index().iloc[1:, :]\n    df_Betas_with_alpha.columns = [\"betas\", \"val\"]\n    df_Betas_with_SE = (\n        pd.concat([_MODEL_FIT.params, _MODEL_FIT.bse], axis=1).reset_index().iloc[1:, :]\n    )\n    df_Betas_with_SE.columns = [\"betas\", \"val\", \"se_\"]\n    sns.barplot(\n        y=\"betas\", x=\"val\", data=df_Betas_with_SE.sort_values(\"val\", ascending=False)\n    )\n    plt.show()\n\n\ndef custom_regression_metrics(_model_type, _actual, _predicted):\n    print(f\"{_model_type}:# R-Squared:{round(r2_score(_actual,_predicted),5)}\")\n    print(f\"{_model_type}:# MSE:{round(mean_squared_error(_actual,_predicted),5)}\")\n    print(f\"{_model_type}:# MAE:{round(mean_absolute_error(_actual,_predicted),5)}\")\n    print(\n        f\"{_model_type}:# RMSE:{round(root_mean_squared_error(_actual,_predicted),5)}\"\n    )\n\n\ndef custom_regression_resid_predict_plot(_model_spec, X_train, y_train, X_test, y_test):\n    # Residual Visualizer\n    visualizer = ResidualsPlot(_model_spec)\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.show()\n    plt.show()\n    ############################\n    visualizer = PredictionError(_model_spec)\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.show()\n    plt.show()\n\n\ndef custom_sillaoute_score_plot(_model_spec, df_):\n    visualizer = SilhouetteVisualizer(_model_spec, colors=\"yellowbrick\")\n    visualizer.fit(df_)\n    visualizer.show()\n    plt.show()\n\ndef custom_regression_metrics(_model_type, _actual, _predicted):\n    print(f\"{_model_type}:# R-Squared:{round(r2_score(_actual,_predicted),5)}\")\n    print(f\"{_model_type}:# MSE:{round(mean_squared_error(_actual,_predicted),5)}\")\n    print(f\"{_model_type}:# MAE:{round(mean_absolute_error(_actual,_predicted),5)}\")\n    print(\n        f\"{_model_type}:# RMSE:{round(root_mean_squared_error(_actual,_predicted),5)}\"\n    )\n\ndef custom_classification_metrics_report(_model_type, _actual, _predicted):\n    _metric_dict = {\n        \"Accuracy\": [],\n        \"ROC-AUC\": [],\n        \"F1-Score\": [],\n        \"Precision\": [],\n        \"Recall\": [],\n    }\n    _metric_dict[\"Accuracy\"].append(round(accuracy_score(_actual, _predicted), 5))\n    _metric_dict[\"ROC-AUC\"].append(round(roc_auc_score(_actual, _predicted), 5))\n    _metric_dict[\"F1-Score\"].append(round(f1_score(_actual, _predicted), 5))\n    _metric_dict[\"Precision\"].append(round(precision_score(_actual, _predicted), 5))\n    _metric_dict[\"Recall\"].append(round(recall_score(_actual, _predicted), 5))\n    print(\"--------------------------------------\")\n    print(f\"{_model_type}:ConfusionMatrix and ROC-AUC Curve\")\n    print(\"--------------------------------------\")\n    _plt = ConfusionMatrixDisplay(confusion_matrix(_actual, _predicted))\n    _plt.plot()\n    RocCurveDisplay.from_predictions(_actual, _predicted)\n    plt.show()\n    _df = pd.DataFrame(_metric_dict)\n    _df.index = [f\"{_model_type}\"]\n    return _df"
  },
  {
    "objectID": "ISI_SSBB_BA_FinalExam_Code_2024.html#python-libraries-and-custom-function-setup",
    "href": "ISI_SSBB_BA_FinalExam_Code_2024.html#python-libraries-and-custom-function-setup",
    "title": "Final Examination",
    "section": "",
    "text": "# Importing Necessary libraries\nimport math\nimport re\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.graphics.gofplots as gof\nimport xgboost as xgb\nfrom feature_engine.encoding import OneHotEncoder, OrdinalEncoder\nfrom feature_engine.imputation import MeanMedianImputer\n\n# the search algorithms\nfrom hyperopt import anneal, fmin, hp, rand, tpe\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import AllKNN, NearMiss, RandomUnderSampler, TomekLinks\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy import stats\nfrom scipy.stats import (\n    chi2_contingency,\n    chisquare,\n    mannwhitneyu,\n    norm,\n    poisson,\n    ttest_1samp,\n    ttest_ind,\n    wilcoxon,\n)\nfrom sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\nfrom sklearn.ensemble import (\n    GradientBoostingClassifier,\n    GradientBoostingRegressor,\n    RandomForestClassifier,\n    RandomForestRegressor,\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import (\n    ConfusionMatrixDisplay,\n    RocCurveDisplay,\n    accuracy_score,\n    confusion_matrix,\n    f1_score,\n    mean_absolute_error,\n    mean_squared_error,\n    precision_score,\n    r2_score,\n    recall_score,\n    roc_auc_score,\n    root_mean_squared_error,\n    silhouette_score,\n)\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_val_score,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats import outliers_influence as sm_oi\nfrom statsmodels.stats.anova import anova_lm\nfrom statsmodels.stats.descriptivestats import sign_test\nfrom statsmodels.stats.proportion import proportions_ztest\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom yellowbrick.model_selection import FeatureImportances, ValidationCurve\nfrom yellowbrick.regressor import PredictionError, ResidualsPlot\nfrom yellowbrick.target import FeatureCorrelation\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")\n\n\n## Custom Functions created by mallesham yamulla\ndef cols_tidy(df_cols):\n    df_cols_tidy = []\n    for item in df_cols:\n        x = re.sub(r\" +\", \" \", item)\n        x1 = re.sub(r\"\\,|\\)|\\(|\\.|\\-|\\'\", \"\", x)\n        x2 = re.sub(r\"\\/\", \"_or_\", x1)\n        x3 = re.sub(r\"\\&\", \"_and_\", x2)\n        x4 = re.sub(r\"\\s(?=\\_{1,})|(\\_(?=\\s{1,}))|\\,\", \"\", x3)\n        x5 = re.sub(r\" {2,}\", \" \", x4)\n        x6 = re.sub(r\"\\s\", \"_\", x5)\n        x7 = re.sub(r\"\\:\", \"\", x6)\n        df_cols_tidy.append(x7.lower())\n    return df_cols_tidy\n\n\ndef calculate_center_line(data, column_name):\n    \"\"\"Calculates the center line for the control chart.\"\"\"\n    return data[column_name].mean()\n\n\ndef calculate_control_limits(center, average_defects_per_unit, multiplier):\n    \"\"\"Calculates upper and lower control limits.\"\"\"\n    std_dev = np.sqrt(average_defects_per_unit)\n    upper_limit = center + multiplier * std_dev\n    lower_limit = center - multiplier * std_dev\n    return upper_limit, lower_limit\n\n\ndef create_run_chart(data, column_name):\n    \"\"\"Creates a run chart for the given data.\"\"\"\n    plt.figure(figsize=(25, 6))\n    plt.plot(data.index, data[column_name], marker=\"o\", linestyle=\"-\")\n    plt.xlabel(\n        \"Projects-From Jan to Dec-2023\"\n    )  # Assuming the index represents time periods\n    plt.ylabel(column_name)  # Replace with the actual column name\n    plt.title(\"Run Chart for \" + column_name)\n    plt.grid(True)\n    plt.show()\n\n\ndef get_hypothesis(_p_val):\n    if _p_val &lt; 0.05:\n        print(\n            f\"P-Value:{round(_p_val,5)} is less than 0.05 hence we can reject the null hypothesis in favor of alternative hypothesis\"\n        )\n    else:\n        print(\n            f\"P-Value:{round(_p_val,5)} is greater than 0.05 hence we failed to reject the null hypothesis.\"\n        )\n\n\ndef custom_ols_qqplot(_resid):\n    \"\"\"Q-Q Plot of residuals\"\"\"\n    gof.qqplot(_resid, line=\"s\")\n    plt.xlabel(\"Standard Normal Quantiles\")\n    plt.ylabel(\"Standardized Residuals\")\n    plt.title(\"Normal Q-Q plot\")\n    plt.show()\n\n\ndef custom_ols_res_vs_fitted(_fitted, _resid):\n    \"\"\"Fitted Vs Residuals Plot\"\"\"\n    plt.scatter(_fitted, _resid)\n    plt.axhline(\"0\", color=\"r\")\n    plt.xlabel(\"Fitted Values\")\n    plt.ylabel(\"Residual\")\n    plt.title(\"Residual Vs Fitted\")\n\n\ndef custom_VIF(_MSPEC):\n    \"\"\"Custom function to get the VIF\"\"\"\n    var_names = _MSPEC.exog_names\n    X = _MSPEC.exog\n    _limit = X.shape[1]\n    try:\n        vif_dict = {}\n        for idx in range(_limit):\n            vif = round(sm_oi.variance_inflation_factor(X, idx), 5)\n            vif_dict[var_names[idx]] = vif\n        _DF = pd.DataFrame([vif_dict]).T\n        _DF.columns = [\"VIF\"]\n        _DF = _DF.reset_index()\n        df_sorted = _DF.iloc[1:].sort_values(by=\"VIF\", ascending=False)\n        fig, ax = plt.subplots(figsize=(30, 12))\n        ax = sns.barplot(x=\"index\", y=\"VIF\", data=df_sorted)\n        # Add text labels to the top of each bar\n        for bar in ax.containers[0]:\n            ax.text(\n                bar.get_x() + bar.get_width() / 2,\n                bar.get_height(),\n                int(bar.get_height()),\n                ha=\"center\",\n                va=\"bottom\",\n            )\n        ax.set_xlabel(\"FIELD\")\n        ax.set_ylabel(\"VIF\")\n        # plt.xticks(rotation=45)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n        plt.title(\"VIF\")\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        pass\n\n\ndef custom_statsmodel_OLS(_DF, *vars):\n    \"\"\"fitting OLS on specified independent and dependent variables- DF, dependent_var and independent_var\"\"\"\n    # sm.add_constant\n    try:\n        LOS_COLS = [v for v in vars]\n        _X = LOS_COLS[1:]\n        _Y = LOS_COLS[0]\n        xvars = sm.add_constant(_DF[_X])\n        yvar = _DF[_Y]\n        _model_spec = sm.OLS(yvar, xvars)\n        return _model_spec\n    except Exception as e:\n        print(f\"There is an error while creating a model spec due to:{e}\")\n\n\ndef custom_model_preds(_model, _new_df):\n    \"\"\"Predictions on new data points\"\"\"\n    _feat = sm.add_constant(_new_df)\n    _pred = _model.predict(sm.add_constant(_feat))\n    _df_pred = pd.DataFrame(_pred)\n    _df_pred.columns = [\"predicted_y\"]\n    return _df_pred\n\n\ndef get_six_sigma_caluclator():\n    \"\"\"ISI Custom SixSigma Calucator\"\"\"\n    while True:\n        ### Inputs\n        print(f\"-------------------------------------------------\")\n        print(f\"######### Sigma Caluclator Inputs #########\")\n        print(f\"-------------------------------------------------\")\n        _mean = float(input(\"Enter the mean:\"))\n        _sd = float(input(\"Enter Standard Deviation:\"))\n        _LSL = float(input(\"Enter LSL:\"))\n        _USL = float(input(\"Enter USL:\"))\n        # Formulas and caluclations\n        ZLSL = (_LSL - _mean) / _sd\n        ZUSL = (_USL - _mean) / _sd\n        Area_ZLSL = norm.cdf(ZLSL)\n        Area_ZUSL = 1 - norm.cdf(ZUSL)\n        TOTAL_NC = Area_ZLSL + Area_ZUSL\n        YIELD = 1 - TOTAL_NC\n        CP_ = (_USL - _LSL) / (6 * _sd)\n        _A = (_USL - _mean) / (3 * _sd)\n        _B = (_mean - _LSL) / (3 * _sd)\n        CPK_ = min(_A, _B)\n        SIGMA_LEVEL = round(1.5 + norm.ppf(YIELD), 5)\n        DPMO = TOTAL_NC * 1000000\n        # Output\n        print(f\"-------------------------------------------------\")\n        print(f\"### Summary Report ###\")\n        print(f\"-------------------------------------------------\")\n        print(f\"Total NonConfirmances:{round(TOTAL_NC,5)}\")\n        print(f\"Yield:{round(YIELD,5)}\")\n        print(f\"CP:{round(CP_,5)}\")\n        print(f\"CPK:{round(CPK_,5)}\")\n        print(f\"SIGMA_LEVEL:{round(SIGMA_LEVEL,5)}\")\n        print(f\"DPMO:{round(DPMO,5)}\")\n        print(f\"-------------------------------------------------\")\n        _next = input(\n            \"Would you like to continue to use sigma caluclator type 'yes' if so :\"\n        )\n        if _next.lower() == \"yes\":\n            continue\n        else:\n            print(f\"Thanks for using Sigma Caluclator..\")\n            print(f\"### END ###\")\n            break\n\n\ndef custom_classification_metrics_report(_model_type, _actual, _predicted):\n    print(f\"Model:{_model_type} Classification Report..\")\n    print(f\"Accuary:{round(accuracy_score(_actual,_predicted),5)}\")\n    print(f\"ROC_AUC:{round(roc_auc_score(_actual,_predicted),5)}\")\n    _plt = ConfusionMatrixDisplay(confusion_matrix(_actual, _predicted))\n    _plt.plot()\n\n\ndef get_regression_plots(_MODEL_FIT):\n    df_Betas_with_alpha = pd.DataFrame(_MODEL_FIT.params).reset_index().iloc[1:, :]\n    df_Betas_with_alpha.columns = [\"betas\", \"val\"]\n    df_Betas_with_SE = (\n        pd.concat([_MODEL_FIT.params, _MODEL_FIT.bse], axis=1).reset_index().iloc[1:, :]\n    )\n    df_Betas_with_SE.columns = [\"betas\", \"val\", \"se_\"]\n    sns.barplot(\n        y=\"betas\", x=\"val\", data=df_Betas_with_SE.sort_values(\"val\", ascending=False)\n    )\n    plt.show()\n\n\ndef custom_regression_metrics(_model_type, _actual, _predicted):\n    print(f\"{_model_type}:# R-Squared:{round(r2_score(_actual,_predicted),5)}\")\n    print(f\"{_model_type}:# MSE:{round(mean_squared_error(_actual,_predicted),5)}\")\n    print(f\"{_model_type}:# MAE:{round(mean_absolute_error(_actual,_predicted),5)}\")\n    print(\n        f\"{_model_type}:# RMSE:{round(root_mean_squared_error(_actual,_predicted),5)}\"\n    )\n\n\ndef custom_regression_resid_predict_plot(_model_spec, X_train, y_train, X_test, y_test):\n    # Residual Visualizer\n    visualizer = ResidualsPlot(_model_spec)\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.show()\n    plt.show()\n    ############################\n    visualizer = PredictionError(_model_spec)\n    visualizer.fit(X_train, y_train)\n    visualizer.score(X_test, y_test)\n    visualizer.show()\n    plt.show()\n\n\ndef custom_sillaoute_score_plot(_model_spec, df_):\n    visualizer = SilhouetteVisualizer(_model_spec, colors=\"yellowbrick\")\n    visualizer.fit(df_)\n    visualizer.show()\n    plt.show()\n\ndef custom_regression_metrics(_model_type, _actual, _predicted):\n    print(f\"{_model_type}:# R-Squared:{round(r2_score(_actual,_predicted),5)}\")\n    print(f\"{_model_type}:# MSE:{round(mean_squared_error(_actual,_predicted),5)}\")\n    print(f\"{_model_type}:# MAE:{round(mean_absolute_error(_actual,_predicted),5)}\")\n    print(\n        f\"{_model_type}:# RMSE:{round(root_mean_squared_error(_actual,_predicted),5)}\"\n    )\n\ndef custom_classification_metrics_report(_model_type, _actual, _predicted):\n    _metric_dict = {\n        \"Accuracy\": [],\n        \"ROC-AUC\": [],\n        \"F1-Score\": [],\n        \"Precision\": [],\n        \"Recall\": [],\n    }\n    _metric_dict[\"Accuracy\"].append(round(accuracy_score(_actual, _predicted), 5))\n    _metric_dict[\"ROC-AUC\"].append(round(roc_auc_score(_actual, _predicted), 5))\n    _metric_dict[\"F1-Score\"].append(round(f1_score(_actual, _predicted), 5))\n    _metric_dict[\"Precision\"].append(round(precision_score(_actual, _predicted), 5))\n    _metric_dict[\"Recall\"].append(round(recall_score(_actual, _predicted), 5))\n    print(\"--------------------------------------\")\n    print(f\"{_model_type}:ConfusionMatrix and ROC-AUC Curve\")\n    print(\"--------------------------------------\")\n    _plt = ConfusionMatrixDisplay(confusion_matrix(_actual, _predicted))\n    _plt.plot()\n    RocCurveDisplay.from_predictions(_actual, _predicted)\n    plt.show()\n    _df = pd.DataFrame(_metric_dict)\n    _df.index = [f\"{_model_type}\"]\n    return _df"
  },
  {
    "objectID": "ISI_SSBB_BA_FinalExam_Code_2024.html#data-importing-and-preparations.",
    "href": "ISI_SSBB_BA_FinalExam_Code_2024.html#data-importing-and-preparations.",
    "title": "Final Examination",
    "section": "1.Data Importing and Preparations.",
    "text": "1.Data Importing and Preparations.\n\ndf_nfhs_mpce_rural = pd.read_excel(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/FinalExam/data/National Family Household Survey Data.xlsx\",\n    sheet_name=\"Statewise -Rural Data\",\n)\n\ndf_nfhs_mpce_urban = pd.read_excel(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/FinalExam/data/National Family Household Survey Data.xlsx\",\n    sheet_name=\"Statewise-Urban Data\",\n)\n\ndf_nfhs_mpce_itemwise = pd.read_excel(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/FinalExam/data/National Family Household Survey Data.xlsx\",\n    sheet_name=\"Itemwise-Percentage MPCE\",\n)\n\ndf_nfhs_mpce_rural.columns = cols_tidy(list(df_nfhs_mpce_rural.columns))\ndf_nfhs_mpce_urban.columns = cols_tidy(list(df_nfhs_mpce_urban.columns))\ndf_nfhs_mpce_itemwise.columns = cols_tidy(list(df_nfhs_mpce_itemwise.columns))\n\ndf_state_codes = pd.read_excel(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/FinalExam/data/india_state_codes.xlsx\"\n)\ndf_state_codes.columns = [\"state\", \"state_short_code\"]\n\nIn preparation for data analysis, the MPCE data has been loaded into three distinct tabular dataframes. I have reviewed the initial ten rows of each dataframe to become familiar with the data structure.\nAverage MPCE of each state in the year 2022-2023 (Rural India)\n\ndf_nfhs_mpce_rural.head(10)\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_rural\n\n\n\n\n0\nAndhra Pradesh\n95813\n4996\n\n\n1\nArunachal Pradesh\n1953\n5300\n\n\n2\nAssam\n63174\n3546\n\n\n3\nBihar\n198464\n3454\n\n\n4\nChhattisgarh\n47120\n2575\n\n\n5\nDelhi\n510\n6595\n\n\n6\nGoa\n1569\n7388\n\n\n7\nGujarat\n73816\n3820\n\n\n8\nHaryana\n33394\n4912\n\n\n9\nHimachal Pradesh\n17260\n5573\n\n\n\n\n\n\n\n\nAverage MPCE of each state in the year 2022-2023 (Urban India)\n\ndf_nfhs_mpce_urban.head(10)\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_urban\n\n\n\n\n0\nAndhra Pradesh\n45443\n6877\n\n\n1\nArunachal Pradesh\n602\n8649\n\n\n2\nAssam\n9279\n6210\n\n\n3\nBihar\n18955\n4819\n\n\n4\nChhattisgarh\n12298\n4557\n\n\n5\nDelhi\n30965\n8250\n\n\n6\nGoa\n2270\n8761\n\n\n7\nGujarat\n58955\n6630\n\n\n8\nHaryana\n22330\n7948\n\n\n9\nHimachal Pradesh\n2601\n8083\n\n\n\n\n\n\n\n\nPercentage Cotribution of MPCE to Each Item Group\n\ndf_nfhs_mpce_itemwise.head(10)\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\n\n\n\n\n0\ncereal\n6.90\n4.49\n\n\n1\ncereal substitutes\n0.02\n0.02\n\n\n2\ngram\n0.24\n0.18\n\n\n3\npulses and pulse products*\n1.73\n1.20\n\n\n4\nsugar & salt\n0.92\n0.60\n\n\n5\nmilk and milk products\n8.14\n7.15\n\n\n6\nvegetables\n5.26\n3.76\n\n\n7\nfruits (fresh)\n2.48\n2.48\n\n\n8\nfruits (dry)\n1.15\n1.29\n\n\n9\negg, fish & meat\n4.80\n3.54"
  },
  {
    "objectID": "ISI_SSBB_BA_FinalExam_Code_2024.html#exploratory-data-analysis.",
    "href": "ISI_SSBB_BA_FinalExam_Code_2024.html#exploratory-data-analysis.",
    "title": "Final Examination",
    "section": "2. Exploratory Data Analysis.",
    "text": "2. Exploratory Data Analysis.\n\n2.1 How are the house holds data distributed in Rural Area?.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_nfhs_mpce_rural[\"no_of_house_holds\"], binwidth=20000, ax=ax1)\np2 = sns.boxplot(df_nfhs_mpce_rural[\"no_of_house_holds\"], ax=ax2)\nplt.suptitle(\"Rural:No of hourse holds\")\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(df_nfhs_mpce_rural[\"no_of_house_holds\"].describe()).T.apply(\n    lambda x: round(x, 2)\n)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nno_of_house_holds\n36.0\n53644.44\n69193.94\n26.0\n1484.0\n25765.0\n87894.5\n307870.0\n\n\n\n\n\n\n\n\nInferences:\nThese points are drawn based out of sample collected across 36 states in india.\n\nThe average number of households is 53,644, but there is significant variation across states as indicated by the standard deviation.\nHalf (50%) of the households have roughly 25,000.\nThree-quarters (75%) of the households have 87,000 or fewer.\nA small number of states have households exceeding 100,000.\n\n\n\n2.2 How are the average MPCE data distributed collected in Rural Area?.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_nfhs_mpce_rural[\"avg_mpce_rural\"], ax=ax1)\np2 = sns.boxplot(df_nfhs_mpce_rural[\"avg_mpce_rural\"], ax=ax2)\nplt.suptitle(\"Rural:avg_mpce_rural\")\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(df_nfhs_mpce_rural[\"avg_mpce_rural\"].describe()).T.apply(\n    lambda x: round(x, 2)\n)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\navg_mpce_rural\n36.0\n4833.22\n1391.89\n2575.0\n3751.5\n4649.5\n5486.0\n7787.0\n\n\n\n\n\n\n\n\nInferences:\nThese points are drawn based out of sample collected across 36 states in india.\n\nThe average Monthly Per Capita Expenditure (MPCE) in India is 4,833. However, there is significant variation in spending across households, as indicated by the standard deviation.\nHalf Spend Less, Half Spend More: The median MPCE is 4,649, which is slightly lower than the average. This means that half of the households spend more than ₹4,649 per month on consumption, and the other half spend less.\nThe MPCE follows a near-normal distribution, indicating a bell-shaped curve where most households’ spending falls around the average.\n\n\n\n2.3 Grouping the no of house hold data as in Rural Area and See how are their avg MPCE per each group?\n\ndf_nfhs_mpce_rural.loc[\n    (df_nfhs_mpce_rural[\"no_of_house_holds\"] &lt; 1500), \"house_hold_group\"\n] = \"A:1-1499\"\n\ndf_nfhs_mpce_rural.loc[\n    (df_nfhs_mpce_rural[\"no_of_house_holds\"] &gt;= 1500)\n    & (df_nfhs_mpce_rural[\"no_of_house_holds\"] &lt; 26000),\n    \"house_hold_group\",\n] = \"B:1500-25999\"\n\ndf_nfhs_mpce_rural.loc[\n    (df_nfhs_mpce_rural[\"no_of_house_holds\"] &gt;= 26000)\n    & (df_nfhs_mpce_rural[\"no_of_house_holds\"] &lt; 87999),\n    \"house_hold_group\",\n] = \"C:26000-87999\"\n\ndf_nfhs_mpce_rural.loc[\n    (df_nfhs_mpce_rural[\"no_of_house_holds\"] &gt;= 88000), \"house_hold_group\"\n] = \"D:&gt;=88000\"\n\nThe previous analysis highlighted inconsistencies in the number of households surveyed across different states. Since states with larger populations likely have more households, a consistent sample size per state might not have been achievable.\nGrouping States by Household Count:\nTo address this, I have categorized the states into four groups based on the number of households in each state. The specific thresholds used for this grouping are as follows:\n\nA:1-1499\nB:1500-25999\nC:26000-87999\nD:&gt;=88000\n\n\nsns.countplot(df_nfhs_mpce_rural, x=\"house_hold_group\")\nplt.title(\"HouseHold Groups-Rural\")\nplt.show()\n\n\n\n\n\n\n\n\n\nInterestingly Each group has the same number of data points (9) for balanced representation.\n\n\ndf_nfhs_mpce_rural.groupby(\"house_hold_group\").agg(\n    {\"avg_mpce_rural\": [min, max, np.mean, np.median]}\n).reset_index().apply(lambda x: round(x, 2))\n\n\n\n\n\n\n\n\n\nhouse_hold_group\navg_mpce_rural\n\n\n\n\nmin\nmax\nmean\nmedian\n\n\n\n\n0\nA:1-1499\n4062\n7787\n6146.78\n6595.0\n\n\n1\nB:1500-25999\n3530\n7388\n4999.67\n4721.0\n\n\n2\nC:26000-87999\n2575\n5960\n4103.00\n3820.0\n\n\n3\nD:&gt;=88000\n3158\n5457\n4083.44\n4076.0\n\n\n\n\n\n\n\n\nInferences\n\nInterestingly, Group A (states with the fewest households, ranging from 1 to 1,499) has the highest average MPCE at 6,146.\nIn contrast, Groups C and D exhibit a similar average MPCE of around 4,100.\nGroup B (states with 1,500 to 25,999 households) falls in between, with an average MPCE of 5,000.\n\n\n\n2.4 How are the house holds data distributed in Urban Area?.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_nfhs_mpce_urban[\"no_of_house_holds\"], binwidth=20000, ax=ax1)\np2 = sns.boxplot(df_nfhs_mpce_urban[\"no_of_house_holds\"], ax=ax2)\nplt.suptitle(\"Urban:No of hourse holds\")\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(df_nfhs_mpce_urban[\"no_of_house_holds\"].describe()).T.apply(\n    lambda x: round(x, 2)\n)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nno_of_house_holds\n36.0\n24861.89\n31406.13\n79.0\n1672.5\n10788.5\n41642.25\n119824.0\n\n\n\n\n\n\n\n\nInferences\nThese points are drawn based out of sample collected across 36 states in india.\n\nThe average number of households is 24,861 but there is significant variation across states as indicated by the standard deviation.\nHalf (50%) of the households have roughly 10788.\nThree-quarters (75%) of the households have 42,000 members or fewer.\nA small number of states have households exceeding 100,000 members.\n\n\n\n2.5 How are the average MPCE data distributed collected in Urban Area?.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_nfhs_mpce_urban[\"avg_mpce_urban\"], ax=ax1)\np2 = sns.boxplot(df_nfhs_mpce_urban[\"avg_mpce_urban\"], ax=ax2)\nplt.suptitle(\"Urban:avg_mpce_urban\")\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(df_nfhs_mpce_urban[\"avg_mpce_urban\"].describe()).T.apply(\n    lambda x: round(x, 2)\n)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\navg_mpce_urban\n36.0\n7062.14\n1853.39\n4557.0\n5855.25\n6780.0\n7822.75\n12577.0\n\n\n\n\n\n\n\n\nInferences:\nThese points are drawn based out of sample collected across 36 states in india.\n\nThe average Monthly Per Capita Expenditure (MPCE) in Urban is 7062. However, there is significant variation in spending across households, as indicated by the standard deviation.\nHalf Spend Less, Half Spend More: The median MPCE is 6780, which is slightly lower than the average. This means that half of the households spend more than 6,780 per month on consumption, and the other half spend less.\nThre are few assignable causes found in MPCE of Urban, The MPCE seems to not following near-normal distribution and the data exhibits a right skewed distribution.\n\n\n\n2.6 Grouping the no of house hold data as in Urban Area and See how are their avg MPCE per each group?\n\ndf_nfhs_mpce_urban.loc[\n    (df_nfhs_mpce_urban[\"no_of_house_holds\"] &lt; 1500), \"house_hold_group\"\n] = \"A:1-1499\"\n\ndf_nfhs_mpce_urban.loc[\n    (df_nfhs_mpce_urban[\"no_of_house_holds\"] &gt;= 1500)\n    & (df_nfhs_mpce_urban[\"no_of_house_holds\"] &lt; 26000),\n    \"house_hold_group\",\n] = \"B:1500-25999\"\n\ndf_nfhs_mpce_urban.loc[\n    (df_nfhs_mpce_urban[\"no_of_house_holds\"] &gt;= 26000)\n    & (df_nfhs_mpce_urban[\"no_of_house_holds\"] &lt; 87999),\n    \"house_hold_group\",\n] = \"C:26000-87999\"\n\ndf_nfhs_mpce_urban.loc[\n    (df_nfhs_mpce_urban[\"no_of_house_holds\"] &gt;= 88000), \"house_hold_group\"\n] = \"D:&gt;=88000\"\n\n\nsns.countplot(df_nfhs_mpce_urban, x=\"house_hold_group\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIn contrast to the rural household groups with equal data points, the urban groups exhibit variation. For instance, Group A (1-1499 households) has 9 data points, while Group B has a higher number of data points at 15. This implies there are more urban states with a household count between 1,500 and 25,999 compared to other urban household categories.\n\n\ndf_nfhs_mpce_urban.groupby(\"house_hold_group\").agg(\n    {\"avg_mpce_urban\": [min, max, np.mean, np.median]}\n).reset_index().apply(lambda x: round(x, 2))\n\n\n\n\n\n\n\n\n\nhouse_hold_group\navg_mpce_urban\n\n\n\n\nmin\nmax\nmean\nmedian\n\n\n\n\n0\nA:1-1499\n5511\n12125\n7817.67\n7159.0\n\n\n1\nB:1500-25999\n4557\n12577\n6870.07\n6577.0\n\n\n2\nC:26000-87999\n5011\n8251\n6640.20\n6753.5\n\n\n3\nD:&gt;=88000\n6683\n7742\n7212.50\n7212.5\n\n\n\n\n\n\n\n\n\n\n2.7 How is the data:the percentage contribution of MPCE is distributed in Rural ?\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_nfhs_mpce_itemwise[\"rural_india_in_202223\"], ax=ax1)\np2 = sns.boxplot(df_nfhs_mpce_itemwise[\"rural_india_in_202223\"], ax=ax2)\nplt.suptitle(\"Rural:Percentage Cotribution of MPCE to Each Item Group \")\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(df_nfhs_mpce_itemwise[\"rural_india_in_202223\"].describe()).T.apply(\n    lambda x: round(x, 2)\n)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nrural_india_in_202223\n27.0\n3.7\n2.69\n0.02\n1.1\n3.52\n5.22\n9.41\n\n\n\n\n\n\n\n\nInferences:\n\nIn Rural areas, the average MPCE across different item groups is roughly similar, regardless of whether we consider the mean or median values. This suggests a symmetrical distribution of spending patterns. On average, individuals in urban areas spend approximately 3.7 per item group.\n\n\n\n2.8 How is the data:the percentage contribution of MPCE is distributed in Urban ?\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_nfhs_mpce_itemwise[\"urban_india_in_202223\"], ax=ax1)\np2 = sns.boxplot(df_nfhs_mpce_itemwise[\"urban_india_in_202223\"], ax=ax2)\nplt.suptitle(\"Urban:Percentage Cotribution of MPCE to Each Item Group \")\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(df_nfhs_mpce_itemwise[\"urban_india_in_202223\"].describe()).T.apply(\n    lambda x: round(x, 2)\n)\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nurban_india_in_202223\n27.0\n3.7\n2.79\n0.02\n1.43\n3.54\n5.8\n10.53\n\n\n\n\n\n\n\n\nInferences\n\nIn urban areas too, the average MPCE across different item groups is roughly similar, regardless of whether we consider the mean or median values. This suggests a symmetrical distribution of spending patterns. On average, individuals in urban areas spend approximately 3.7 per item group.\n\n\nx1 = df_nfhs_mpce_itemwise[[\"item_group\", \"urban_india_in_202223\"]].sort_values(\n    \"urban_india_in_202223\", ascending=False\n)\nx2 = df_nfhs_mpce_itemwise[[\"item_group\", \"rural_india_in_202223\"]].sort_values(\n    \"rural_india_in_202223\", ascending=False\n)\n\n\n\n2.9 How is the the percentage contribution of MPCE per Eachh Item Group in Rural. ?\n\nsns.barplot(x2, y=\"item_group\", x=\"rural_india_in_202223\")\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\n\nHigher Spending: The data suggests that Indians, particularly in urban areas allocate a significant portion of their monthly expenditure (MPCE) towards Food and Beverages, particularly milk and milk products. Additionally, conveyance appears to be another major spending category.\nLower Spending: Rent, footwear, taxes & cesses, entertainment, and dry fruits seem to represent a smaller share of MPCE.\nHealthcare Concerns: The data also indicates potentially concerning levels of spending on hospitalizations.\n\n\n\n2.10 How is the the percentage contribution of MPCE per Eachh Item Group in Urban. ?\n\nsns.barplot(x1, y=\"item_group\", x=\"urban_india_in_202223\")\nplt.show()\n\n\n\n\n\n\n\n\nInferences\n\nWhile urban and rural populations allocate MPCE to the same item groups, the spending levels for these groups vary slightly, with urban residents spending somewhat more.\n\n\n\n2.11 How does the Pareto Principle apply to MPCE in Rural areas? What percentage of Item Groups contribute to 80% of total spending?.\n\nplt.style.use(\"classic\")\n# Pareto Diagram\nsorted_data = df_nfhs_mpce_itemwise.sort_values(\n    by=[\"rural_india_in_202223\"], ascending=False\n)\nsorted_data[\"cumulative_freq\"] = sorted_data[\"rural_india_in_202223\"].cumsum()\nsorted_data[\"cumulative_pct\"] = (\n    sorted_data[\"cumulative_freq\"] / sorted_data[\"rural_india_in_202223\"].sum() * 100\n)\n\n# Visualizations\nfig, ax1 = plt.subplots(figsize=(22, 8))\nax2 = ax1.twinx()\nax2.plot(\n    sorted_data[\"item_group\"],\n    sorted_data[\"cumulative_pct\"],\n    color=\"green\",\n    marker=\"o\",\n    linestyle=\"solid\",\n)\nax1.set_xlabel(\"ItemGroup\")\nax1.set_ylabel(\"Frequency\", color=\"red\")\nax2.set_ylabel(\"Cumulative Percentage\", color=\"red\")\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\nplt.title(\"Pareto Analysis- MPCE - RURAL\")\nplt.show()\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\n\n\n\n\n\n\n\nInferences\n\nIn rural areas In 80%, around 50% of the average monthly expenditure is allocated towards essential goods like beverages, milk and milk products, conveyance, cereals, durable goods, fuel and light, vegetables, and clothing and bedding.\nThe remaining 30% of the monthly expenditure covers a broader range of categories, including toilet articles & other household consumables, consumer services (such as repairs or hairdressing), egg, fish & meat, tobacco products, and medical expenses (both hospitalization and non-hospitalization).\n\n\n\n2.12 How does the Pareto Principle apply to MPCE in Urban areas? What percentage of Item Groups contribute to 80% of total spending?.\n\nplt.style.use(\"classic\")\n# Pareto Diagram\nsorted_data = df_nfhs_mpce_itemwise.sort_values(\n    by=[\"urban_india_in_202223\"], ascending=False\n)\nsorted_data[\"cumulative_freq\"] = sorted_data[\"urban_india_in_202223\"].cumsum()\nsorted_data[\"cumulative_pct\"] = (\n    sorted_data[\"cumulative_freq\"] / sorted_data[\"urban_india_in_202223\"].sum() * 100\n)\n\n# Visualizations\nfig, ax1 = plt.subplots(figsize=(22, 8))\nax2 = ax1.twinx()\nax2.plot(\n    sorted_data[\"item_group\"],\n    sorted_data[\"cumulative_pct\"],\n    color=\"black\",\n    marker=\"o\",\n    linestyle=\"-\",\n)\nax1.set_xlabel(\"ItemGroup\")\nax1.set_ylabel(\"Frequency\", color=\"red\")\nax2.set_ylabel(\"Cumulative Percentage\", color=\"red\")\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\nplt.title(\"Pareto Analysis- MPCE - Urban\")\nplt.show()\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\n\n\n\n\n\n\n\nInferences\n\nIn Urban areas In 80%, around 50% of the average monthly expenditure is allocated towards essential goods like beverages, milk and milk products, conveyance, durable goods, fuel and light, and consumer services.\nThe remaining 30% of the monthly expenditure covers a broader range of categories, education, toilet articles, clothing & bedding, cereal, vegetables and medical.\n\nIn a conclusion\n\nHouseholds in both rural and urban areas dedicate roughly half of their monthly spending to essential goods like food, transportation, housing, utilities, and some basic services. The remaining portion is allocated to a wider range of categories, with some variation between the two areas. Rural areas spend more on necessities like clothing and fuel, while urban areas prioritize education and potentially more diverse services."
  },
  {
    "objectID": "ISI_SSBB_BA_FinalExam_Code_2024.html#unsupervised-analysis.",
    "href": "ISI_SSBB_BA_FinalExam_Code_2024.html#unsupervised-analysis.",
    "title": "Final Examination",
    "section": "3.Unsupervised Analysis.",
    "text": "3.Unsupervised Analysis.\n\nIntroduction to Unsupervised Learning Algorithms.\nUnsupervised learning is about finding insights & patterns hidden in the data, Unlike regression or classification, we don’t care about splitting our data into train / test sets and making predictions, we just care about understanding the relationships in our data.\n1.K-Means:\nK-Means Clustering is a popular algorithm which assigns each observation in a data set to a specific cluster, where “K” represents the number of clusters\nHere’s how it works:\n\nSelect “K” arbitrary locations in a scatter plot as cluster centers (or centroids), and assign each observation to a cluster based on the closest centroid.\nRecalculate and relocate each centroid to the mean of the observations assigned to it, then reassign each observation to its new closest centroid\nRepeat the process until observations no longer change clusters.\n\n2.Hierarchical Clustering:\nHierarchical Clustering is a clustering technique that creates clusters by grouping similar data points together\n3.DBSCAN:\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering technique that identifies clusters based on the density of data points\nHere’s how it works:\n\nSelect a radius (eps) and a minimum number of points (min_samples)\nWithin a scatter plot, label each point as one of the following:\n\nCore point – has the minimum number of points within its radius (in a dense region)\nBorder point – does not have the minimum number of points within its radius, but has at least one core point within its radius (on the outskirts of clusters)\nNoise point (outlier) – does not have a core point within its radius (isolated points)\n\n\n\n\nRural - No of house holds and Average MPCE\n\nI have choses K as 2 to do carry out K-means clustering on the Rural data of no of house holds and average MPCE as below.\n\n\ndf_X1 = df_nfhs_mpce_rural.drop([\"state\", \"house_hold_group\"], axis=1)\n\n\n_model_spec_kmeans = KMeans(n_clusters=2, n_init=\"auto\", random_state=2024)\n\n\n_model_spec_kmeans.fit(df_X1)\n\nKMeans(n_clusters=2, random_state=2024)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=2, random_state=2024) \n\n\n\n# combine the data and cluster labels\ncluster_labels = pd.Series(_model_spec_kmeans.labels_, name=\"cluster\")\n# create a clean dataframe\ndf_clean = pd.concat([df_X1, cluster_labels], axis=1)\n\n\nsns.scatterplot(df_clean, x=\"no_of_house_holds\", y=\"avg_mpce_rural\", hue=\"cluster\")\nplt.title(\"Rural:K-means-2 clusters\")\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\n\nFrom the above scatterplot it is seen that the house holds between 1-50K and their AVG MPCE falls between 1500 and 8000 are clustered into one.\nThe remaining larger no of housing i.e more than 80K are fit into 2nd cluster.\n\n\n# view the cluster centers in a heatmap\ncluster_centers2 = pd.DataFrame(\n    _model_spec_kmeans.cluster_centers_, columns=df_X1.columns\n)\ncluster_centers2.apply(lambda x: round(x, 2))\n\n\n\n\n\n\n\n\n\nno_of_house_holds\navg_mpce_rural\n\n\n\n\n0\n143205.00\n3974.70\n\n\n1\n19198.08\n5163.42\n\n\n\n\n\n\n\n\nInferences:\n\nCluster 2 has a higher average MPCE (5163) compared to Cluster 1 (3974). This suggests a difference of 1189 in average spending.\nWhile the average MPCE in Cluster 1 is 3974, Cluster 2 has a slightly higher average of 5163. The difference between the two clusters is around 1189. More neutral phrasing:\nThe average MPCE in Cluster 1 is 3974, while in Cluster 2 it is 5163.\n\nTo gain insights from the data, I will identify the optimal number of clusters (K) by minimizing the within-cluster sum of squares (WCSS). WCSS measures how spread out the data points are within each cluster, which can be done through Inertia Metric.\n\n# create an empty list to hold many inertia values\ninertia_values = []\n# create 2 - 15 clusters, and add the intertia scores to the list\nfor k in range(2, 16):\n    kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n    kmeans.fit(df_X1)\n    inertia_values.append(kmeans.inertia_)\n\n\n# turn the list into a series for plotting\ninertia_series = pd.Series(inertia_values, index=range(2, 16))\n# plot the data\ninertia_series.plot(marker=\"o\")\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Number of Clusters vs. Inertia\");\n\n\n\n\n\n\n\n\n\nBased on the inertia plot, using 5 clusters seems effective for revealing patterns in the rural data.\n\n\n# now for 5 clusters\nkmeans4 = KMeans(n_clusters=5, n_init=\"auto\", random_state=2024)\nkmeans4.fit(df_X1)\n\nKMeans(n_clusters=5, random_state=2024)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=5, random_state=2024) \n\n\n\n# view the cluster centers in a dataframe\ncluster_centers4 = pd.DataFrame(kmeans4.cluster_centers_, columns=df_X1.columns)\ncluster_centers4 = cluster_centers4.apply(lambda x: round(x, 2))\n\n\ncustom_sillaoute_score_plot(kmeans4, df_X1)\n\n\n\n\n\n\n\n\n\nThe silhouette plot helps visualize how well data points are clustered in k-means. By analyzing the distribution of silhouette values, we can assess the quality of the clustering and potentially identify areas for improvement.\nThe silhouette plot indicates a positive outcome for the 5-cluster solution. Four clusters have silhouette scores above 0.7, signifying that points within those clusters are well-positioned and have a significant separation from points in other clusters. This suggests good cluster separation and well-defined clusters.\n\n\n# combine the data and cluster labels\ncluster_labels = pd.Series(kmeans4.labels_, name=\"cluster\")\n\n# create a clean dataframe\ndf_clean_4 = pd.concat([df_X1, cluster_labels], axis=1)\n\ndf_nfhs_mpce_rural_tidy = pd.concat(\n    [\n        df_nfhs_mpce_rural,\n        df_clean_4[\"cluster\"],\n        df_state_codes.iloc[:36,].loc[:, \"state_short_code\"],\n    ],\n    axis=1,\n)\n\n\nsns.countplot(df_nfhs_mpce_rural_tidy, x=\"cluster\")\nplt.show()\n\n\n\n\n\n\n\n\n\ncluster_centers4\n\n\n\n\n\n\n\n\n\nno_of_house_holds\navg_mpce_rural\n\n\n\n\n0\n172293.67\n3645.67\n\n\n1\n48997.86\n4301.57\n\n\n2\n307870.00\n3277.00\n\n\n3\n4574.94\n5573.22\n\n\n4\n97302.14\n4193.29\n\n\n\n\n\n\n\n\nInferences:\n\nThe cluster number 3 has the highest average mpce i.e 5573 compare to the remaining 4 clusters.\nThe cluster number 2 has lowest average mpce i.e 3277 within the 5 clusters.\n\nLet’s explore the data patterns within each of the five clusters by examining the rural states belonging to those clusters.\nRural-Cluster#0:\n\ndf_nfhs_mpce_rural_tidy.loc[df_nfhs_mpce_rural_tidy[\"cluster\"] == 0, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_rural\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n3\nBihar\n198464\n3454\nD:&gt;=88000\n0\nBH\n\n\n14\nMaharashtra\n150360\n4076\nD:&gt;=88000\n0\nMH\n\n\n28\nWest Bengal\n168057\n3407\nD:&gt;=88000\n0\nWB\n\n\n\n\n\n\n\n\nRural households from Bihar, Maharashtra, and West Bengal appear to be grouped together in one cluster. This cluster has a larger number of households and a lower average MPCE of 3645, suggesting a potential association with a lower standard of living.\nRural-Cluster#1:\n\ndf_nfhs_mpce_rural_tidy.loc[df_nfhs_mpce_rural_tidy[\"cluster\"] == 1, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_rural\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n2\nAssam\n63174\n3546\nC:26000-87999\n1\nAS\n\n\n4\nChhattisgarh\n47120\n2575\nC:26000-87999\n1\nCT\n\n\n8\nHaryana\n33394\n4912\nC:26000-87999\n1\nHR\n\n\n10\nJharkhand\n55031\n2796\nC:26000-87999\n1\nJH\n\n\n12\nKerala\n46172\n5960\nC:26000-87999\n1\nKL\n\n\n20\nPunjab\n40039\n5363\nC:26000-87999\n1\nPB\n\n\n24\nTelangana\n58055\n4959\nC:26000-87999\n1\nTS\n\n\n\n\n\n\n\n\nRural households from Assam, Chattisgarh, Haryana, Jharkhand,Kerala, punjab and Telangana appear to be grouped together in one cluster. This cluster has moderate average MPCE of 4301, suggesting a potential association with a better standard of living.\nRural-Cluster#2:\n\ndf_nfhs_mpce_rural_tidy.loc[df_nfhs_mpce_rural_tidy[\"cluster\"] == 2, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_rural\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n27\nUttar Pradesh\n307870\n3277\nD:&gt;=88000\n2\nUP\n\n\n\n\n\n\n\n\nRural households from Uttar Pradesh only to be grouped together in one cluster. This cluster has the least average MPCE of 3277, suggesting a potential association with a poor standard of living.\nRural-Cluster#3:\n\ndf_nfhs_mpce_rural_tidy.loc[df_nfhs_mpce_rural_tidy[\"cluster\"] == 3, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_rural\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n1\nArunachal Pradesh\n1953\n5300\nB:1500-25999\n3\nAR\n\n\n5\nDelhi\n510\n6595\nA:1-1499\n3\nDL\n\n\n6\nGoa\n1569\n7388\nB:1500-25999\n3\nGA\n\n\n9\nHimachal Pradesh\n17260\n5573\nB:1500-25999\n3\nHP\n\n\n15\nManipur\n4440\n4370\nB:1500-25999\n3\nMN\n\n\n16\nMeghalaya\n5610\n3530\nB:1500-25999\n3\nME\n\n\n17\nMizoram\n1229\n5243\nA:1-1499\n3\nMZ\n\n\n18\nNagaland\n2341\n4457\nB:1500-25999\n3\nNL\n\n\n22\nSikkim\n1189\n7787\nA:1-1499\n3\nSK\n\n\n25\nTripura\n7032\n5301\nB:1500-25999\n3\nTR\n\n\n26\nUttarakhand\n18136\n4721\nB:1500-25999\n3\nUK\n\n\n29\nAndaman & N Islands\n625\n7332\nA:1-1499\n3\nAN\n\n\n30\nChandigarh\n82\n7467\nA:1-1499\n3\nCH\n\n\n31\nDadra & Nagar Haveli and Daman & Diu\n716\n4229\nA:1-1499\n3\nDD\n\n\n32\nJammu & Kashmir\n18123\n4357\nB:1500-25999\n3\nJK\n\n\n33\nLadakh\n371\n4062\nA:1-1499\n3\nLA* (unofficial)\n\n\n34\nLakshadweep\n26\n5979\nA:1-1499\n3\nLD\n\n\n35\nPuducherry\n1137\n6627\nA:1-1499\n3\nPY\n\n\n\n\n\n\n\n\nRural households from the smaller states to be grouped together in one cluster. This cluster has the the best average MPCE of 5573, suggesting a potential association with a standard of living in small states.\nRural-Cluster#4:\n\ndf_nfhs_mpce_rural_tidy.loc[df_nfhs_mpce_rural_tidy[\"cluster\"] == 4, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_rural\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n0\nAndhra Pradesh\n95813\n4996\nD:&gt;=88000\n4\nAP\n\n\n7\nGujarat\n73816\n3820\nC:26000-87999\n4\nGJ\n\n\n11\nKarnataka\n89231\n4578\nD:&gt;=88000\n4\nKA\n\n\n13\nMadhya Pradesh\n114334\n3158\nD:&gt;=88000\n4\nMP\n\n\n19\nOdisha\n87449\n2996\nC:26000-87999\n4\nOR\n\n\n21\nRajasthan\n110176\n4348\nD:&gt;=88000\n4\nRJ\n\n\n23\nTamil Nadu\n110296\n5457\nD:&gt;=88000\n4\nTN\n\n\n\n\n\n\n\n\nRural households from Andhrpradhesh, gujaraj, karnataka, MP, Odisha, Rajasthan and tamilnadu appear to be grouped together in one cluster. This cluster has second better average MPCE of 4193, suggesting a potential association with a better standard of living.\n\n\nUrban - No of house holds and Average MPCE.\nI will carry out the same clustering analysis on Urban data as follows.\n\ndf_X2 = df_nfhs_mpce_urban.drop([\"state\", \"house_hold_group\"], axis=1)\n\n\n# create an empty list to hold many inertia values\ninertia_values = []\n# create 2 - 15 clusters, and add the intertia scores to the list\nfor k in range(2, 16):\n    kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n    kmeans.fit(df_X2)\n    inertia_values.append(kmeans.inertia_)\n\n\n# turn the list into a series for plotting\ninertia_series = pd.Series(inertia_values, index=range(2, 16))\n\n# plot the data\ninertia_series.plot(marker=\"o\")\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Number of Clusters vs. Inertia\");\n\n\n\n\n\n\n\n\n\nBased on the inertia plot, using 5 clusters seems effective for revealing patterns in the urban data as well.\n\n\n# now for 5 clusters\nkmeans4_urban = KMeans(n_clusters=5, n_init=\"auto\", random_state=42)\nkmeans4_urban.fit(df_X2)\n\nKMeans(n_clusters=5, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=5, random_state=42) \n\n\n\n# view the cluster centers in a dataframe\ncluster_centers_urban = pd.DataFrame(\n    kmeans4_urban.cluster_centers_, columns=df_X2.columns\n)\ncluster_centers_urban = cluster_centers_urban.apply(lambda x: round(x, 2))\n\n\ncustom_sillaoute_score_plot(kmeans4_urban, df_X2)\n\n\n\n\n\n\n\n\n\nThe silhouette plot indicates a positive outcome for the 5-cluster solution. Four clusters have silhouette scores above 0.6, signifying that points within those clusters are well-positioned and have a significant separation from points in other clusters. This suggests good cluster separation and well-defined clusters of Urban data as well.\n\n\n# combine the data and cluster labels\ncluster_labels = pd.Series(kmeans4_urban.labels_, name=\"cluster\")\n\n# create a clean dataframe\ndf_clean_urban = pd.concat([df_X2, cluster_labels], axis=1)\n\ndf_nfhs_mpce_urban_tidy = pd.concat(\n    [\n        df_nfhs_mpce_urban,\n        df_clean_urban[\"cluster\"],\n        df_state_codes.iloc[:36,].loc[:, \"state_short_code\"],\n    ],\n    axis=1,\n)\n\n\nsns.countplot(df_nfhs_mpce_urban_tidy, x=\"cluster\")\nplt.show()\n\n\n\n\n\n\n\n\n\ncluster_centers_urban\n\n\n\n\n\n\n\n\n\nno_of_house_holds\navg_mpce_urban\n\n\n\n\n0\n41933.83\n6910.17\n\n\n1\n1868.18\n7831.18\n\n\n2\n110502.00\n7212.50\n\n\n3\n68454.25\n6235.25\n\n\n4\n16692.14\n5754.29\n\n\n\n\n\n\n\n\nInferences:\n\nThe cluster number 1 has the highest average mpce i.e 7831 compare to the remaining 4 clusters.\nThe cluster number 4 has lowest average mpce i.e 5744 within the 5 clusters.\n\nLet’s explore the data patterns within each of the five clusters by examining the urban states belonging to those clusters.\nCluster#0:\n\ndf_nfhs_mpce_urban_tidy.loc[df_nfhs_mpce_urban_tidy[\"cluster\"] == 0, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_urban\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n0\nAndhra Pradesh\n45443\n6877\nC:26000-87999\n0\nAP\n\n\n5\nDelhi\n30965\n8250\nC:26000-87999\n0\nDL\n\n\n12\nKerala\n42846\n7102\nC:26000-87999\n0\nKL\n\n\n13\nMadhya Pradesh\n41241\n5011\nC:26000-87999\n0\nMP\n\n\n21\nRajasthan\n39106\n5970\nC:26000-87999\n0\nRJ\n\n\n24\nTelangana\n52002\n8251\nC:26000-87999\n0\nTS\n\n\n\n\n\n\n\n\nUrban households from AP, Delhi, Kerala,MP,Rajasthan and Telangana appear to be grouped together in one cluster. This cluster has a moderate average MPCE of 6910, suggesting a potential association with a better standard of living.\nCluster#1:\n\ndf_nfhs_mpce_urban_tidy.loc[df_nfhs_mpce_urban_tidy[\"cluster\"] == 1, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_urban\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n1\nArunachal Pradesh\n602\n8649\nA:1-1499\n1\nAR\n\n\n6\nGoa\n2270\n8761\nB:1500-25999\n1\nGA\n\n\n9\nHimachal Pradesh\n2601\n8083\nB:1500-25999\n1\nHP\n\n\n15\nManipur\n1767\n4902\nB:1500-25999\n1\nMN\n\n\n16\nMeghalaya\n1112\n6450\nA:1-1499\n1\nME\n\n\n17\nMizoram\n1033\n7664\nA:1-1499\n1\nMZ\n\n\n18\nNagaland\n979\n7159\nA:1-1499\n1\nNL\n\n\n22\nSikkim\n608\n12125\nA:1-1499\n1\nSK\n\n\n25\nTripura\n2083\n7473\nB:1500-25999\n1\nTR\n\n\n26\nUttarakhand\n5353\n7034\nB:1500-25999\n1\nUK\n\n\n29\nAndaman & N Islands\n498\n10268\nA:1-1499\n1\nAN\n\n\n30\nChandigarh\n2521\n12577\nB:1500-25999\n1\nCH\n\n\n31\nDadra & Nagar Haveli and Daman & Diu\n1389\n6306\nA:1-1499\n1\nDD\n\n\n32\nJammu & Kashmir\n6347\n6200\nB:1500-25999\n1\nJK\n\n\n33\nLadakh\n79\n6227\nA:1-1499\n1\nLA* (unofficial)\n\n\n34\nLakshadweep\n80\n5511\nA:1-1499\n1\nLD\n\n\n35\nPuducherry\n2437\n7741\nB:1500-25999\n1\nPY\n\n\n\n\n\n\n\n\nUrban households from Small states appear to be grouped together in one cluster. This cluster has a moderate average MPCE of 7831, suggesting a potential association with the best standard of living.\nCluster#2:\n\ndf_nfhs_mpce_urban_tidy.loc[df_nfhs_mpce_urban_tidy[\"cluster\"] == 2, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_urban\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n14\nMaharashtra\n119824\n6683\nD:&gt;=88000\n2\nMH\n\n\n23\nTamil Nadu\n101180\n7742\nD:&gt;=88000\n2\nTN\n\n\n\n\n\n\n\n\nUrban households from Maharasthra and Tamilnadu to be grouped together in one cluster. This cluster has a moderate average MPCE of 7212, suggesting a potential association with a better standard of living.\nCluster#3:\n\ndf_nfhs_mpce_urban_tidy.loc[df_nfhs_mpce_urban_tidy[\"cluster\"] == 3, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_urban\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n7\nGujarat\n58955\n6630\nC:26000-87999\n3\nGJ\n\n\n11\nKarnataka\n61965\n7781\nC:26000-87999\n3\nKA\n\n\n27\nUttar Pradesh\n85072\n5104\nC:26000-87999\n3\nUP\n\n\n28\nWest Bengal\n67825\n5426\nC:26000-87999\n3\nWB\n\n\n\n\n\n\n\n\nUrban households from Gujarat,Karnataka,UP and WB to be grouped together in one cluster. This cluster has a moderate average MPCE of 6235, suggesting a potential association with a nominal standard of living.\nCluster#4:\n\ndf_nfhs_mpce_urban_tidy.loc[df_nfhs_mpce_urban_tidy[\"cluster\"] == 4, :]\n\n\n\n\n\n\n\n\n\nstate\nno_of_house_holds\navg_mpce_urban\nhouse_hold_group\ncluster\nstate_short_code\n\n\n\n\n2\nAssam\n9279\n6210\nB:1500-25999\n4\nAS\n\n\n3\nBihar\n18955\n4819\nB:1500-25999\n4\nBH\n\n\n4\nChhattisgarh\n12298\n4557\nB:1500-25999\n4\nCT\n\n\n8\nHaryana\n22330\n7948\nB:1500-25999\n4\nHR\n\n\n10\nJharkhand\n14021\n4946\nB:1500-25999\n4\nJH\n\n\n19\nOdisha\n15796\n5223\nB:1500-25999\n4\nOR\n\n\n20\nPunjab\n24166\n6577\nB:1500-25999\n4\nPB\n\n\n\n\n\n\n\n\nUrban households from Assam,Bihar Chattisgarh,Haryana,Jharkhand,Odisha and Punjab be grouped together in one cluster. This cluster has a lease average MPCE of 5754, suggesting a potential association with a poor standard of living.\n\n\nItem Group: Percentage Cotribution of MPCE to Each Item.\nGlance at item group data\n\ndf_nfhs_mpce_itemwise.head()\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\n\n\n\n\n0\ncereal\n6.90\n4.49\n\n\n1\ncereal substitutes\n0.02\n0.02\n\n\n2\ngram\n0.24\n0.18\n\n\n3\npulses and pulse products*\n1.73\n1.20\n\n\n4\nsugar & salt\n0.92\n0.60\n\n\n\n\n\n\n\n\nThis item-group data, segmented by rural and urban locations, will be analyzed using various clustering techniques to identify potential patterns.\n\n\nKMeans.\n\ndf_X3 = df_nfhs_mpce_itemwise.drop([\"item_group\"], axis=1)\n\n\n# create an empty list to hold many inertia values\ninertia_values = []\n# create 2 - 15 clusters, and add the intertia scores to the list\nfor k in range(2, 16):\n    kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n    kmeans.fit(df_X3)\n    inertia_values.append(kmeans.inertia_)\n\n\n# turn the list into a series for plotting\ninertia_series = pd.Series(inertia_values, index=range(2, 16))\n\n# plot the data\ninertia_series.plot(marker=\"o\")\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Number of Clusters vs. Inertia\");\n\n\n\n\n\n\n\n\n\nBased on the inertia plot, using 5 clusters seems effective for revealing patterns in the item-group data.\n\n\n# now for 5 clusters\nkmeans_ITEM = KMeans(n_clusters=5, n_init=\"auto\", random_state=2024)\nkmeans_ITEM.fit(df_X3)\n\nKMeans(n_clusters=5, random_state=2024)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=5, random_state=2024) \n\n\n\ncustom_sillaoute_score_plot(kmeans_ITEM, df_X3)\n\n\n\n\n\n\n\n\n\nThe silhouette plot indicates a positive outcome for the 5-cluster solution. Four clusters have silhouette scores above 0.5, signifying that points within those clusters are well-positioned and have a significant separation from points in other clusters.\n\n\n# view the cluster centers in a dataframe\ncluster_centers_ITEM = pd.DataFrame(kmeans_ITEM.cluster_centers_, columns=df_X3.columns)\ncluster_centers_ITEM = cluster_centers_ITEM.apply(lambda x: round(x, 2))\n\n\n# combine the data and cluster labels\ncluster_labels = pd.Series(kmeans_ITEM.labels_, name=\"cluster\")\n\n# create a clean dataframe\ndf_clean_item = pd.concat([df_X3, cluster_labels], axis=1)\n\ndf_nfhs_mpce_item_tidy = pd.concat(\n    [\n        df_nfhs_mpce_itemwise,\n        df_clean_item[\"cluster\"],\n    ],\n    axis=1,\n)\n\n\nsns.countplot(df_nfhs_mpce_item_tidy, x=\"cluster\")\nplt.show()\n\n\n\n\n\n\n\n\n\ncluster_centers_ITEM\n\n\n\n\n\n\n\n\n\nrural_india_in_202223\nurban_india_in_202223\n\n\n\n\n0\n5.17\n4.79\n\n\n1\n0.76\n0.73\n\n\n2\n7.93\n8.33\n\n\n3\n2.99\n2.25\n\n\n4\n0.76\n6.49\n\n\n\n\n\n\n\n\nInferences:\n\nThe cluster number 2 has the highest average percentage of contributions from MPCE to the item-groups i.e 7.93 in rural and 8.33 in urban compare to the remaining 4 clusters.\nThe cluster number 1 and 2 have least average percentage of contributions i.e 0.76 in rural ad 0.73 in urban within the 5 clusters.\n\nCluster#0\n\ndf_nfhs_mpce_item_tidy.loc[df_nfhs_mpce_item_tidy[\"cluster\"] == 0, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n0\ncereal\n6.90\n4.49\n0\n\n\n6\nvegetables\n5.26\n3.76\n0\n\n\n9\negg, fish & meat\n4.80\n3.54\n0\n\n\n14\nfuel and light\n6.51\n6.20\n0\n\n\n15\ntoilet articles & other household consumables\n5.01\n4.93\n0\n\n\n16\neducation\n3.23\n5.73\n0\n\n\n18\nmedical (non- hospitalization)\n4.66\n3.96\n0\n\n\n20\nconsumer services excluding conveyance\n4.96\n5.86\n0\n\n\n24\nclothing & bedding\n5.18\n4.62\n0\n\n\n\n\n\n\n\n\nCluster 0 appears to group item categories (cereals, vegetables, egg, fish & meat, fuel & light, toilets, education, non-hospitalization medication, consumer services, and clothing & bedding) where the average MPCE contributions from rural and urban areas are similar.\nCluster#1\n\ndf_nfhs_mpce_item_tidy.loc[df_nfhs_mpce_item_tidy[\"cluster\"] == 1, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n1\ncereal substitutes\n0.02\n0.02\n1\n\n\n2\ngram\n0.24\n0.18\n1\n\n\n3\npulses and pulse products*\n1.73\n1.20\n1\n\n\n4\nsugar & salt\n0.92\n0.60\n1\n\n\n8\nfruits (dry)\n1.15\n1.29\n1\n\n\n21\nentertainment\n1.06\n1.57\n1\n\n\n23\nother taxes & cesses\n0.12\n0.24\n1\n\n\n25\nfootwear\n0.85\n0.76\n1\n\n\n\n\n\n\n\n\nCluster 1 appears to group item categories (cereals substistues, gram, pulse product, sugar &salt, fruits, enteraiment, other taxes and footwear) where the average MPCE contributions from rural and urban areas are similar.\nCluster#2\n\ndf_nfhs_mpce_item_tidy.loc[df_nfhs_mpce_item_tidy[\"cluster\"] == 2, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n5\nmilk and milk products\n8.14\n7.15\n2\n\n\n12\nbeverages, processed food#, etc.\n9.41\n10.53\n2\n\n\n19\nconveyance\n7.38\n8.51\n2\n\n\n26\ndurable goods\n6.79\n7.13\n2\n\n\n\n\n\n\n\n\nCluster 2 appears to group item categories (milk product, beverages, processed foods, conveynace and durable goods) where the average MPCE contributions from urban areas is little higher than rural\nCluster#3\n\ndf_nfhs_mpce_item_tidy.loc[df_nfhs_mpce_item_tidy[\"cluster\"] == 3, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n7\nfruits (fresh)\n2.48\n2.48\n3\n\n\n10\nedible oil\n3.52\n2.35\n3\n\n\n11\nspices\n2.92\n2.11\n3\n\n\n13\npan, tobacco & intoxicants\n3.70\n2.41\n3\n\n\n17\nmedical (hospitalization)\n2.31\n1.89\n3\n\n\n\n\n\n\n\n\nCluster 3 appears to group item categories (fresh fruits, edibile oil, spices, tobaco and medical hospitalization) where the average MPCE contributions from rural and urban areas are similar.\nCluster#4\n\ndf_nfhs_mpce_item_tidy.loc[df_nfhs_mpce_item_tidy[\"cluster\"] == 4, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n22\nrent\n0.76\n6.49\n4\n\n\n\n\n\n\n\n\nInterestingly, Cluster 4 consists solely of the “rent” category. The MPCE contribution for rent appears to be higher in urban areas compared to rural areas within this cluster.\nI will now use hierarchical and DBSCAN clustering to see if they reveal similar patterns.\n\n\nAgglomarative.\n\n_agg_cluster = AgglomerativeClustering(n_clusters=5, metric=\"euclidean\", linkage=\"ward\")\n\n\n_agg_cluster.fit(df_X3)\n\nAgglomerativeClustering(n_clusters=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  AgglomerativeClustering?Documentation for AgglomerativeClusteringiFittedAgglomerativeClustering(n_clusters=5) \n\n\n\ndf_agg = pd.DataFrame({\"cluster\": list(_agg_cluster.labels_)})\n\n\ndf_items_clusters = pd.concat([df_nfhs_mpce_itemwise, df_agg], axis=1)\n\nCluster#0\n\ndf_items_clusters.loc[df_items_clusters[\"cluster\"] == 0, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n5\nmilk and milk products\n8.14\n7.15\n0\n\n\n12\nbeverages, processed food#, etc.\n9.41\n10.53\n0\n\n\n14\nfuel and light\n6.51\n6.20\n0\n\n\n19\nconveyance\n7.38\n8.51\n0\n\n\n26\ndurable goods\n6.79\n7.13\n0\n\n\n\n\n\n\n\n\nCluster#1\n\ndf_items_clusters.loc[df_items_clusters[\"cluster\"] == 1, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n1\ncereal substitutes\n0.02\n0.02\n1\n\n\n2\ngram\n0.24\n0.18\n1\n\n\n3\npulses and pulse products*\n1.73\n1.20\n1\n\n\n4\nsugar & salt\n0.92\n0.60\n1\n\n\n8\nfruits (dry)\n1.15\n1.29\n1\n\n\n21\nentertainment\n1.06\n1.57\n1\n\n\n23\nother taxes & cesses\n0.12\n0.24\n1\n\n\n25\nfootwear\n0.85\n0.76\n1\n\n\n\n\n\n\n\n\nCluster#2\n\ndf_items_clusters.loc[df_items_clusters[\"cluster\"] == 2, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n0\ncereal\n6.90\n4.49\n2\n\n\n6\nvegetables\n5.26\n3.76\n2\n\n\n9\negg, fish & meat\n4.80\n3.54\n2\n\n\n15\ntoilet articles & other household consumables\n5.01\n4.93\n2\n\n\n16\neducation\n3.23\n5.73\n2\n\n\n18\nmedical (non- hospitalization)\n4.66\n3.96\n2\n\n\n20\nconsumer services excluding conveyance\n4.96\n5.86\n2\n\n\n24\nclothing & bedding\n5.18\n4.62\n2\n\n\n\n\n\n\n\n\nCluster#3\n\ndf_items_clusters.loc[df_items_clusters[\"cluster\"] == 3, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n7\nfruits (fresh)\n2.48\n2.48\n3\n\n\n10\nedible oil\n3.52\n2.35\n3\n\n\n11\nspices\n2.92\n2.11\n3\n\n\n13\npan, tobacco & intoxicants\n3.70\n2.41\n3\n\n\n17\nmedical (hospitalization)\n2.31\n1.89\n3\n\n\n\n\n\n\n\n\nCluster#4\n\ndf_items_clusters.loc[df_items_clusters[\"cluster\"] == 4, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n22\nrent\n0.76\n6.49\n4\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\nresults = []\n\n# define a range of eps and min_samples values to loop through\neps_values = np.arange(0.1, 2, 0.1)\nmin_samples_values = np.arange(2, 10, 1)\n\n# loop through the combinations of eps and min_samples\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        dbscan.fit(df_X3)\n        labels = dbscan.labels_\n\n        # count the number of clusters (excluding noise points labeled as -1)\n        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n\n        # count the number of noise points (labeled as -1)\n        n_noise = list(labels).count(-1)\n\n        # calculate the silhouette score\n        if n_clusters &gt; 1:  # silhouette score requires at least 2 clusters\n            silhouette = silhouette_score(\n                df_X3, labels, metric=\"euclidean\", sample_size=None\n            )\n        else:\n            silhouette = None\n\n        results.append([eps, min_samples, n_clusters, n_noise, silhouette])\n\n# put the results in a dataframe\ndbscan_results = pd.DataFrame(\n    results,\n    columns=[\n        \"Eps\",\n        \"Min Samples\",\n        \"Number of Clusters\",\n        \"Number of Noise Points\",\n        \"Silhouette Score\",\n    ],\n)\n\n\n# view only the top result for each silhouette score\n(\n    dbscan_results.sort_values(\"Silhouette Score\", ascending=False)\n    .groupby(\"Silhouette Score\")\n    .head(1)\n).head()\n\n\n\n\n\n\n\n\n\nEps\nMin Samples\nNumber of Clusters\nNumber of Noise Points\nSilhouette Score\n\n\n\n\n141\n1.8\n7\n2\n7\n0.425032\n\n\n67\n0.9\n5\n2\n14\n0.397568\n\n\n64\n0.9\n2\n3\n9\n0.390227\n\n\n116\n1.5\n6\n2\n8\n0.386525\n\n\n112\n1.5\n2\n3\n5\n0.372150\n\n\n\n\n\n\n\n\n\n# DBSCAN with three clusters\ndbscan_v2 = DBSCAN(eps=1.8, min_samples=7)\ndbscan_v2.fit(df_X3)\n\nDBSCAN(eps=1.8, min_samples=7)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DBSCAN?Documentation for DBSCANiFittedDBSCAN(eps=1.8, min_samples=7) \n\n\n\ndf_dbscan_labels = pd.DataFrame({\"cluster\": list(dbscan_v2.labels_)})\n\n\ndf_items_clusters_db = pd.concat([df_nfhs_mpce_itemwise, df_dbscan_labels], axis=1)\n\nCluster#0\n\ndf_items_clusters_db.loc[df_items_clusters_db[\"cluster\"] == 0, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n1\ncereal substitutes\n0.02\n0.02\n0\n\n\n2\ngram\n0.24\n0.18\n0\n\n\n3\npulses and pulse products*\n1.73\n1.20\n0\n\n\n4\nsugar & salt\n0.92\n0.60\n0\n\n\n7\nfruits (fresh)\n2.48\n2.48\n0\n\n\n8\nfruits (dry)\n1.15\n1.29\n0\n\n\n10\nedible oil\n3.52\n2.35\n0\n\n\n11\nspices\n2.92\n2.11\n0\n\n\n13\npan, tobacco & intoxicants\n3.70\n2.41\n0\n\n\n17\nmedical (hospitalization)\n2.31\n1.89\n0\n\n\n21\nentertainment\n1.06\n1.57\n0\n\n\n23\nother taxes & cesses\n0.12\n0.24\n0\n\n\n25\nfootwear\n0.85\n0.76\n0\n\n\n\n\n\n\n\n\nCluster#1\n\ndf_items_clusters_db.loc[df_items_clusters_db[\"cluster\"] == 1, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n0\ncereal\n6.90\n4.49\n1\n\n\n6\nvegetables\n5.26\n3.76\n1\n\n\n9\negg, fish & meat\n4.80\n3.54\n1\n\n\n15\ntoilet articles & other household consumables\n5.01\n4.93\n1\n\n\n18\nmedical (non- hospitalization)\n4.66\n3.96\n1\n\n\n20\nconsumer services excluding conveyance\n4.96\n5.86\n1\n\n\n24\nclothing & bedding\n5.18\n4.62\n1\n\n\n\n\n\n\n\n\nCluster#Noise Points\n\ndf_items_clusters_db.loc[df_items_clusters_db[\"cluster\"] == -1, :]\n\n\n\n\n\n\n\n\n\nitem_group\nrural_india_in_202223\nurban_india_in_202223\ncluster\n\n\n\n\n5\nmilk and milk products\n8.14\n7.15\n-1\n\n\n12\nbeverages, processed food#, etc.\n9.41\n10.53\n-1\n\n\n14\nfuel and light\n6.51\n6.20\n-1\n\n\n16\neducation\n3.23\n5.73\n-1\n\n\n19\nconveyance\n7.38\n8.51\n-1\n\n\n22\nrent\n0.76\n6.49\n-1\n\n\n26\ndurable goods\n6.79\n7.13\n-1\n\n\n\n\n\n\n\n\n\n\nComparing models.\n\n# silhouette_score(df_X3,_agg_cluster.labels_)\n\n\n\n\nmodel\nno_of_clusters\nslhhoutte_score\n\n\n\n\nKmeans\n5\n0.48\n\n\nagglomorative\n5\n0.48\n\n\nDBSCAN\n3\n0.42\n\n\n\nModel Summary:\n\nK-means and Agglomerative Clustering have very similar results. They both found 5 clusters with an identical silhouette score of 0.48.\nSilhouette score is a measure of how well data points are separated within their assigned clusters. A score closer to 1 indicates better separation. While 0.48 isn’t a perfect score, it suggests an acceptable cluster separation.\nDBSCAN identified a different cluster structure (3 clusters) with a lower silhouette score (0.42). This could indicate that DBSCAN might not be ideal for this particular dataset.\n\nConclusion\nThis analysis of MPCE contributions across different item groups identified several key patterns:\n\nCluster 2: Stands out with the highest overall contribution percentage (around 8%) in both rural and urban areas.\nClusters 0, 1, and 3: Show similar contribution patterns across rural and urban areas, suggesting these item groups don’t have significant spending differences between locations.\n\nCluster 0: Includes cereals, vegetables, various food items, fuel & light, and basic necessities.\nCluster 1: Focuses on staples like cereals and pulses, along with sugar, salt, fruits, and entertainment.\nCluster 3: Groups fresh fruits, edible oils, spices, medical hospitalization, and tobacco.\nCluster 4: Uniquely consists of “rent,” with a noticeably higher contribution from MPCE in urban areas compared to rural areas."
  },
  {
    "objectID": "ISI_SSBB_BA_FinalExam_Code_2024.html#data-preparation",
    "href": "ISI_SSBB_BA_FinalExam_Code_2024.html#data-preparation",
    "title": "Final Examination",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nData Importing and Glance at Data\n\ndf_insurance_raw = pd.read_excel(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/FinalExam/data/Insurance Data-Supervised Learning.xlsx\"\n)\ndf_insurance_raw.columns = cols_tidy(df_insurance_raw.columns)\n\n\ndf_insurance_raw.head()\n\n\n\n\n\n\n\n\n\nregion\nlocation_code\ngender\nmarital_status\nemployment_status\neducation\nincome\nvehicle_class\nvehicle_size\nmonthly_insurance_premium_\n...\nnumber_of_open_complaints\nnumber_of_policies\npolicy_type\npolicy_level\nrenew_offer_type\nsales_channel\ncoverage\ntotal_claim_amount\ncustomer_lifetime_value\ndiscontinued\n\n\n\n\n0\nSouth\nSuburban\nM\nMarried\nRetired\nCollege\n26806\nFour-Door Car\nMedsize\n63\n...\n0\n2\nCorporate Auto\nCorporate L3\nOffer1\nAgent\nBasic\n302.400000\n7019\nYes\n\n\n1\nEast\nRural\nM\nMarried\nEmployed\nBachelor\n35322\nLuxury Car\nSmall\n244\n...\n0\n5\nPersonal Auto\nPersonal L3\nOffer1\nAgent\nExtended\n76.826503\n19185\nNo\n\n\n2\nSouth\nUrban\nF\nMarried\nEmployed\nCollege\n46855\nSUV\nSmall\n119\n...\n0\n9\nPersonal Auto\nPersonal L3\nOffer3\nBranch\nBasic\n472.391125\n9501\nNo\n\n\n3\nSouth\nSuburban\nM\nMarried\nRetired\nHigh School or Below\n19683\nSports Car\nMedsize\n117\n...\n1\n9\nPersonal Auto\nPersonal L2\nOffer1\nWeb\nBasic\n561.600000\n8383\nYes\n\n\n4\nEast\nSuburban\nF\nDivorced\nDisabled\nBachelor\n19864\nFour-Door Car\nMedsize\n63\n...\n0\n1\nCorporate Auto\nCorporate L3\nOffer1\nWeb\nBasic\n352.394515\n2359\nYes\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\n\nData Type Checkings and Glance at Data.\n\ndf_insurance_raw.dtypes\n\nregion                            object\nlocation_code                     object\ngender                            object\nmarital_status                    object\nemployment_status                 object\neducation                         object\nincome                             int64\nvehicle_class                     object\nvehicle_size                      object\nmonthly_insurance_premium_         int64\nmonths_since_last_claim            int64\nmonths_since_policy_inception      int64\nnumber_of_open_complaints          int64\nnumber_of_policies                 int64\npolicy_type                       object\npolicy_level                      object\nrenew_offer_type                  object\nsales_channel                     object\ncoverage                          object\ntotal_claim_amount               float64\ncustomer_lifetime_value            int64\ndiscontinued                      object\ndtype: object\n\n\n\n\nChecking for Missing Values.\n\ndf_null_cols = pl.from_pandas(df_insurance_raw).null_count().to_pandas().T.reset_index()\ndf_null_cols.columns = [\"column\", \"total_NULL\"]\nsns.barplot(df_null_cols, y=\"column\", x=\"total_NULL\")\nplt.show()\n\n\n\n\n\n\n\n\nInference\nNo Nulls and Empty values are present in the given dataset features and it will be easier to carry out the statistical analysis/modeling. not required to eliminate any of observations."
  },
  {
    "objectID": "ISI_SSBB_BA_FinalExam_Code_2024.html#a-and-b-perform-basic-descriptive-analytics-and-visualization-on-the-insurance-dataset-and-give-your-inferences-for-the-management-to-initiate-improvement-actions.",
    "href": "ISI_SSBB_BA_FinalExam_Code_2024.html#a-and-b-perform-basic-descriptive-analytics-and-visualization-on-the-insurance-dataset-and-give-your-inferences-for-the-management-to-initiate-improvement-actions.",
    "title": "Final Examination",
    "section": "2. (A and B) Perform basic descriptive analytics and visualization on the Insurance Dataset and give your inferences for the management to initiate improvement actions.",
    "text": "2. (A and B) Perform basic descriptive analytics and visualization on the Insurance Dataset and give your inferences for the management to initiate improvement actions.\n\nUnderstanding Categorical and Continous Features.\nThere are 14 categorical and 8 continous variables are present in the insurance dataset.\nCategoical features:\n\ndf_insurance_raw.select_dtypes(\"object\").head()\n\n\n\n\n\n\n\n\n\nregion\nlocation_code\ngender\nmarital_status\nemployment_status\neducation\nvehicle_class\nvehicle_size\npolicy_type\npolicy_level\nrenew_offer_type\nsales_channel\ncoverage\ndiscontinued\n\n\n\n\n0\nSouth\nSuburban\nM\nMarried\nRetired\nCollege\nFour-Door Car\nMedsize\nCorporate Auto\nCorporate L3\nOffer1\nAgent\nBasic\nYes\n\n\n1\nEast\nRural\nM\nMarried\nEmployed\nBachelor\nLuxury Car\nSmall\nPersonal Auto\nPersonal L3\nOffer1\nAgent\nExtended\nNo\n\n\n2\nSouth\nUrban\nF\nMarried\nEmployed\nCollege\nSUV\nSmall\nPersonal Auto\nPersonal L3\nOffer3\nBranch\nBasic\nNo\n\n\n3\nSouth\nSuburban\nM\nMarried\nRetired\nHigh School or Below\nSports Car\nMedsize\nPersonal Auto\nPersonal L2\nOffer1\nWeb\nBasic\nYes\n\n\n4\nEast\nSuburban\nF\nDivorced\nDisabled\nBachelor\nFour-Door Car\nMedsize\nCorporate Auto\nCorporate L3\nOffer1\nWeb\nBasic\nYes\n\n\n\n\n\n\n\n\nContinous features:\n\ndf_insurance_raw.select_dtypes([\"int\", \"float\"]).head()\n\n\n\n\n\n\n\n\n\nincome\nmonthly_insurance_premium_\nmonths_since_last_claim\nmonths_since_policy_inception\nnumber_of_open_complaints\nnumber_of_policies\ntotal_claim_amount\ncustomer_lifetime_value\n\n\n\n\n0\n26806\n63\n32\n19\n0\n2\n302.400000\n7019\n\n\n1\n35322\n244\n4\n33\n0\n5\n76.826503\n19185\n\n\n2\n46855\n119\n13\n94\n0\n9\n472.391125\n9501\n\n\n3\n19683\n117\n17\n41\n1\n9\n561.600000\n8383\n\n\n4\n19864\n63\n22\n96\n0\n1\n352.394515\n2359\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis.\n\nEDA-1: what are the policy type ?\n\nsns.countplot(df_insurance_raw, x=\"policy_type\")\nplt.show()\n\n\n\n\n\n\n\n\nInferences\n\nPersonal Auto policy type propportion is higher than the corporate and special auto.\n\n\n\nEDA-2: Which type of customers are taking the policies?\n\nsns.countplot(df_insurance_raw, x=\"employment_status\")\nplt.show()\n\n\n\n\n\n\n\n\ninferences\nEmployeed persons are only taking the pocilies. and there is a tiny contributions from unemployeed group, the remaining retired, disables and medical leave are equally takig the policies.\n\n\nEDA-3: From which region the policies are purchased ?\n\nsns.countplot(df_insurance_raw, y=\"region\")\nplt.show()\n\n\n\n\n\n\n\n\nInferences\n\nSouth and West\n\n\n\nEDA-4: Are the rural customers buying more policies than the suburban and urban ?\n\nsns.countplot(df_insurance_raw, y=\"location_code\")\nplt.show()\n\n\n\n\n\n\n\n\ninferences\n\nNo, Sububran peoples are buying the more policies than Rural.\n\n\n\nEDA-5: For which type of vehicles customers are taking the policies ?\n\nsns.countplot(df_insurance_raw, y=\"vehicle_class\")\nplt.show()\n\n\n\n\n\n\n\n\nInfeences\n\nCustomers are preferring to purchase policies for four door and two-door cars\n\n\n\nEDA-6: Through which channel policy purchases are happening ?\n\nsns.countplot(df_insurance_raw, y=\"sales_channel\")\nplt.show()\n\n\n\n\n\n\n\n\nInferences\n\nAgents are helping to bring more customers to buy policies\n\n\n\nEDA-7: Whare are the policy coverage?\n\nsns.countplot(df_insurance_raw, y=\"coverage\")\nplt.show()\n\n\n\n\n\n\n\n\nInferences\n\nCustomers are taking only basic coverages\n\n\n\nEDA-8:How many number of customers are discontinued ?\n\nsns.countplot(df_insurance_raw, x=\"discontinued\")\nplt.show()\n\n\n\n\n\n\n\n\n\n40% of customers are discontinued.\n\n\n\nEDA-9: How are the incomes of customers?\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_insurance_raw[\"income\"], binwidth=20000, ax=ax1)\np2 = sns.boxplot(df_insurance_raw[\"income\"], ax=ax2)\nplt.suptitle(\"Income Distributions?.\")\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(df_insurance_raw[\"income\"].describe()).T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nincome\n500.0\n37700.586\n29233.9678\n0.0\n14099.0\n32591.0\n62142.0\n99845.0\n\n\n\n\n\n\n\n\ninferences\n\n50% of customers are falling in the income range of 32K and an average income of customers is 37K, mean is little higher than median as its a right skewed.\n\n\n\nEDA-10: How are the claim amounts paid to customers?\n\nsns.histplot(df_insurance_raw[\"total_claim_amount\"])\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(df_insurance_raw[\"total_claim_amount\"].describe()).T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ntotal_claim_amount\n500.0\n429.300851\n268.254306\n4.845348\n291.860184\n381.91703\n542.4\n1778.4\n\n\n\n\n\n\n\n\nInferences - 75% of claim amounts are below the prices:545 - There are claims reached out more than 750-1750\n\n\nEDA-11: any relation ship between num of policies hold and claim amounts?\n\nsns.scatterplot(\n    df_insurance_raw[[\"number_of_policies\", \"total_claim_amount\"]],\n    x=\"number_of_policies\",\n    y=\"total_claim_amount\",\n)\nplt.show()\n\n\n\n\n\n\n\n\nInferences\n\nNo sort of relations, holiding 1 policy’s customers are claiming more.\n\n\n\nEDA-12: Is there any difference i CLTV of contiued and discontinued customers?.\n\nsns.boxplot(df_insurance_raw, x=\"discontinued\", y=\"customer_lifetime_value\")\nplt.show()\n\n\n\n\n\n\n\n\nInferences\nDiscontinued customers and continued customers lifetime value are differents.\n\n\n\nStatisticalAnalysis-Hypothesis testings.\n\nHypothesis 1:\nHypothesis\n        Ho: The mean of customer_lifetime_value is 8500\n        Ha: The mean of customer_lifetime_value is not 8500\n\n_stat, _pval = ttest_1samp(df_insurance_raw[\"customer_lifetime_value\"], popmean=8500)\n\nprint(\n    f\"P-Value Caluclated is:{round(_pval,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.702 which is greater than to 0.05, hence we failed to reject the null hypothesis.\n\n\ninferences - The mean value of customer lifetime value is about 8500**\n\n\nHypothesis 2:\nHypothesis\n        Ho: There is no difference between mean of customer_lifetime_value contined and discontinued\n        Ha: There is a difference between mean of customer_lifetime_value contined and discontinued\n\ndf_a = df_insurance_raw[[\"discontinued\", \"customer_lifetime_value\"]]\n_yes = df_a.loc[df_a[\"discontinued\"] == \"Yes\", \"customer_lifetime_value\"]\n_no = df_a.loc[df_a[\"discontinued\"] == \"No\", \"customer_lifetime_value\"]\n# Perform 2-sample t-test on male and female ages\n_stat, _pval = ttest_ind(_yes, _no)\nprint(\n    f\"P-Value Caluclated is:{round(_pval,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.151 which is greater than to 0.05, hence we failed to reject the null hypothesis.\n\n\ninfereces\nWe have significant evidence to say that there is no difference between mean of customer_lifetime_value contined and discontinued\n\n\nHypothesis 3:\nHypothesis\n        Ho: The true proportion of discontied customers is 40\n        Ha: The true proportion of discontied customers is not 40\n\n# Number of satisfied customers\nsuccesses = 200\n# sample size\ntrials = 500\n# Expected proportion of satisfied customers\nhypothesized_proportion = 0.4\n\n# Perform the test\nz_statistic, p_value = proportions_ztest(\n    count=successes, nobs=trials, value=hypothesized_proportion\n)\n\nprint(\n    f\"P-Value Caluclated is:{round(p_value,3)} which is less than to 0.05, hence we can reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:1.0 which is less than to 0.05, hence we can reject the null hypothesis.\n\n\nInferences\n\nThe true proportion of discontined customers is 40%.\n\n\n\nHypothesis 4: ChiSuare Associaton test: vehicle_class and Discontinued\nHypothesis\n        Ho: There is no association between vechicle class and discontinued\n        Ha: There is an association between vechicle class and discontinued\n\ndf_3h_agg1 = (\n    df_insurance_raw[[\"vehicle_class\", \"discontinued\"]]\n    .groupby([\"vehicle_class\", \"discontinued\"])\n    .value_counts()\n    .reset_index()\n)\nsns.barplot(df_3h_agg1, x=\"vehicle_class\", y=\"count\", hue=\"discontinued\")\nplt.ylabel(\"count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n## Chi-Square Test-1\ndf_ecom_CT = (\n    pd.crosstab(df_insurance_raw[\"vehicle_class\"], df_insurance_raw[\"discontinued\"])\n    .reset_index()\n    .drop([\"vehicle_class\"], axis=1)\n)\n\nCS_LVEL = df_ecom_CT.to_numpy()\n\n_chi2, _pvalue, _ddof, _expected = chi2_contingency(CS_LVEL)\n\nget_hypothesis(_pvalue)\n\nP-Value:0.05088 is greater than 0.05 hence we failed to reject the null hypothesis.\n\n\nInferences\n\nCustomer discontinuation rates appear independent of the vehicle class they choose."
  },
  {
    "objectID": "ISI_SSBB_BA_FinalExam_Code_2024.html#c-and-d-regression-problem.",
    "href": "ISI_SSBB_BA_FinalExam_Code_2024.html#c-and-d-regression-problem.",
    "title": "Final Examination",
    "section": "2.(C AND D): Regression Problem.",
    "text": "2.(C AND D): Regression Problem.\nUsing CLTV as the target variable perform exploratory analytics to identify the most important features contributing significantly to the variation observed in CLTV.\nBuild 3 most appropriate machine learning algorithms which can be best tried/attempted to build a predictive model for future customers\n\nPre-Modeling Activities: Feature Creation and Encoding and Relationship checks\n\nfor _col in df_insurance_raw.select_dtypes(\"object\").columns:\n    _lev = len(df_insurance_raw[f\"{_col}\"].unique())\n    print(f\"Field:{_col} has levels:{_lev}\")\n\nField:region has levels:5\nField:location_code has levels:3\nField:gender has levels:2\nField:marital_status has levels:3\nField:employment_status has levels:5\nField:education has levels:5\nField:vehicle_class has levels:6\nField:vehicle_size has levels:3\nField:policy_type has levels:3\nField:policy_level has levels:9\nField:renew_offer_type has levels:4\nField:sales_channel has levels:4\nField:coverage has levels:3\nField:discontinued has levels:2\n\n\nOur dataset contains 14 categorical features, each with more than one level. To encode these features for machine learning analysis, we will employ One-Hot Encoding.\n\n_onehot_encoder = OneHotEncoder(\n    variables=[\n        \"region\",\n        \"location_code\",\n        \"gender\",\n        \"marital_status\",\n        \"employment_status\",\n        \"education\",\n        \"vehicle_class\",\n        \"vehicle_size\",\n        \"policy_type\",\n        \"policy_level\",\n        \"renew_offer_type\",\n        \"sales_channel\",\n        \"coverage\",\n    ],\n    drop_last=True,\n)\n\n\ndf_insurance_encoded = _onehot_encoder.fit_transform(df_insurance_raw)\n\n\n_my = OneHotEncoder(\n    variables=\"discontinued\",\n    drop_last=False,\n)\ndf_insurance_tidy = _my.fit_transform(df_insurance_encoded)\n\n\nX_predictors = df_insurance_tidy.drop(\"customer_lifetime_value\", axis=1)\nY_target = df_insurance_tidy[\"customer_lifetime_value\"]\n\n\nlabels = list(X_predictors.columns)\n\n\nAfter one-hot encoding the categorical variables, we now have separate features for each category within those variables. These encoded features, along with the original non-categorical features, will be used to create the X_predictor (features used for prediction) and Y_target (variable to be predicted) for our model.\n\n\nX_predictors.head()\n\n\n\n\n\n\n\n\n\nincome\nmonthly_insurance_premium_\nmonths_since_last_claim\nmonths_since_policy_inception\nnumber_of_open_complaints\nnumber_of_policies\ntotal_claim_amount\nregion_South\nregion_East\nregion_West\n...\nrenew_offer_type_Offer1\nrenew_offer_type_Offer3\nrenew_offer_type_Offer2\nsales_channel_Agent\nsales_channel_Branch\nsales_channel_Web\ncoverage_Basic\ncoverage_Extended\ndiscontinued_Yes\ndiscontinued_No\n\n\n\n\n0\n26806\n63\n32\n19\n0\n2\n302.400000\n1\n0\n0\n...\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n1\n35322\n244\n4\n33\n0\n5\n76.826503\n0\n1\n0\n...\n1\n0\n0\n1\n0\n0\n0\n1\n0\n1\n\n\n2\n46855\n119\n13\n94\n0\n9\n472.391125\n1\n0\n0\n...\n0\n1\n0\n0\n1\n0\n1\n0\n0\n1\n\n\n3\n19683\n117\n17\n41\n1\n9\n561.600000\n1\n0\n0\n...\n1\n0\n0\n0\n0\n1\n1\n0\n1\n0\n\n\n4\n19864\n63\n22\n96\n0\n1\n352.394515\n0\n1\n0\n...\n1\n0\n0\n0\n0\n1\n1\n0\n1\n0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\nY_target.head()\n\n0     7019\n1    19185\n2     9501\n3     8383\n4     2359\nName: customer_lifetime_value, dtype: int64\n\n\n\n# Create a list of the feature names\nfeatures = np.array(X_predictors.columns)\n# Instantiate the visualizer\nvisualizer = FeatureCorrelation(labels=features, size=(800, 1200))\nvisualizer.fit(X_predictors, Y_target)\nvisualizer.show()\nplt.show()\n\n\n\n\n\n\n\n\nInferences\n\nWeak Correlations: The analysis reveals that none of the correlation coefficients between features and the target variable exceed 0.4 in either positive or negative direction. This suggests that no single feature has an extremely strong linear relationship with the target variable.\nPositive Correlations with Target Variable: Among continuous variables, monthly income premium and a luxury vehicle class exhibit the strongest positive correlations with the target variable.\nNegative Correlations with Target Variable: Conversely, unemployment status and having a basic coverage policy are negatively correlated with the target variable. This suggests that being unemployed and having basic coverage are associated with lower values for the target variable.\n\n\n\nBasic Regression modeling.\n\nX = sm.add_constant(X_predictors)\nY = Y_target\n_model_spec = sm.OLS(Y, X)\n_MODEL_FIT = _model_spec.fit()\n\n\nModel Summary\n\n# print(f\"Trial-1#The Regression Table:##\\n{_MODEL_FIT.summary2()}\")\n\nInferences\n\nThe R-squared value of 0.29 and Adjusted R-squared of 0.21 suggest that the model explains a portion of the variation in the target variable (Y) that can be predicted by the predictor variables (X).\n\n\n\nRegression Model Betas.\n\ndf_Betas_with_alpha = pd.DataFrame(_MODEL_FIT.params).reset_index().iloc[1:, :]\ndf_Betas_with_alpha.columns = [\"betas\", \"val\"]\ndf_Betas_with_SE = (\n    pd.concat([_MODEL_FIT.params, _MODEL_FIT.bse], axis=1).reset_index().iloc[1:, :]\n)\ndf_Betas_with_SE.columns = [\"betas\", \"val\", \"se_\"]\nfig, ax = plt.subplots(figsize=(10, 18))\nsns.barplot(\n    y=\"betas\", x=\"val\", data=df_Betas_with_SE.sort_values(\"val\", ascending=False)\n)\nplt.show()\n\n\n\n\n\n\n\n\nInferences\n\nThe beta coefficients for half of the predictor variables are negative. In other words, a one-unit increase in these variables is likely to lead to a decrease in the value of the response variable, but the magnitude of that decrease is not specified here.\n\n\n\nVIF.\n\ncustom_VIF(_model_spec)\n\n\n\n\n\n\n\n\n\nvar_names = _model_spec.exog_names\nX = _model_spec.exog\n_limit = X.shape[1]\nvif_dict = {}\nfor idx in range(_limit):\n    vif = round(sm_oi.variance_inflation_factor(X, idx), 5)\n    vif_dict[var_names[idx]] = vif\n_DF = pd.DataFrame([vif_dict]).T\n_DF.columns = [\"VIF\"]\n_DF = _DF.reset_index()\ndf_sorted = _DF.iloc[1:].sort_values(by=\"VIF\", ascending=False)\n\nFeatures having VIF values below 5.\n\ndf_sorted.iloc[:21]\n\n\n\n\n\n\n\n\n\nindex\nVIF\n\n\n\n\n51\ndiscontinued_No\ninf\n\n\n40\npolicy_level_Corporate L1\ninf\n\n\n32\npolicy_type_Corporate Auto\ninf\n\n\n33\npolicy_type_Personal Auto\ninf\n\n\n34\npolicy_level_Corporate L3\ninf\n\n\n36\npolicy_level_Personal L2\ninf\n\n\n37\npolicy_level_Personal L1\ninf\n\n\n39\npolicy_level_Corporate L2\ninf\n\n\n35\npolicy_level_Personal L3\ninf\n\n\n50\ndiscontinued_Yes\ninf\n\n\n25\nvehicle_class_Four-Door Car\n113.41392\n\n\n29\nvehicle_class_Two-Door Car\n78.71848\n\n\n27\nvehicle_class_SUV\n39.47769\n\n\n2\nmonthly_insurance_premium_\n33.12355\n\n\n48\ncoverage_Basic\n19.21469\n\n\n28\nvehicle_class_Sports Car\n17.00781\n\n\n49\ncoverage_Extended\n9.07340\n\n\n23\neducation_High School or Below\n7.43945\n\n\n21\neducation_College\n6.82422\n\n\n22\neducation_Bachelor\n6.71496\n\n\n18\nemployment_status_Employed\n6.09022\n\n\n\n\n\n\n\n\nFeatures having VIF values greater than or equal to 5 .\n\ndf_sorted.iloc[21:]\n\n\n\n\n\n\n\n\n\nindex\nVIF\n\n\n\n\n7\ntotal_claim_amount\n4.46738\n\n\n20\nemployment_status_Unemployed\n4.28022\n\n\n42\nrenew_offer_type_Offer1\n4.08476\n\n\n44\nrenew_offer_type_Offer2\n3.70694\n\n\n1\nincome\n3.42983\n\n\n24\neducation_Master\n3.27873\n\n\n10\nregion_West\n3.07734\n\n\n8\nregion_South\n3.00389\n\n\n12\nlocation_code_Suburban\n2.78891\n\n\n13\nlocation_code_Rural\n2.60509\n\n\n26\nvehicle_class_Luxury Car\n2.58534\n\n\n9\nregion_East\n2.45961\n\n\n43\nrenew_offer_type_Offer3\n2.41943\n\n\n31\nvehicle_size_Small\n2.41205\n\n\n41\npolicy_level_Special L2\n2.40396\n\n\n17\nemployment_status_Retired\n2.37559\n\n\n30\nvehicle_size_Medsize\n2.30278\n\n\n45\nsales_channel_Agent\n2.25997\n\n\n11\nregion_Central\n1.92946\n\n\n46\nsales_channel_Branch\n1.92263\n\n\n19\nemployment_status_Disabled\n1.88747\n\n\n15\nmarital_status_Married\n1.83136\n\n\n47\nsales_channel_Web\n1.68305\n\n\n16\nmarital_status_Divorced\n1.66289\n\n\n38\npolicy_level_Special L1\n1.49380\n\n\n14\ngender_M\n1.15027\n\n\n3\nmonths_since_last_claim\n1.12905\n\n\n6\nnumber_of_policies\n1.10814\n\n\n4\nmonths_since_policy_inception\n1.10311\n\n\n5\nnumber_of_open_complaints\n1.09775\n\n\n\n\n\n\n\n\n\n\n\nPre-Modeling Activities: Train/Test validation sets creations and Feature Scaling/Standardization.\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_predictors, Y_target, test_size=0.2, random_state=2024\n)\n\nTrain/Test Validation\nIn statistical modeling, a crucial step involves assessing how well a model performs on unseen data.\n\nThis is achieved by splitting the data into two sets: a training set and a testing set.\nThe model is trained on the training set, allowing it to learn the underlying relationships within the data. Subsequently, the model’s performance is evaluated on the completely separate testing set.\nThis train-test split approach is a well-established technique to ensure a model’s generalizability, meaning its ability to perform well on new, unseen data that wasn’t used for training.\nfor this analysis i’m using 80-20 trin/test split approch as follows.\n\n\nprint(f\"Trainset:{X_train.shape} and testing:{X_test.shape}\")\n\nTrainset:(400, 51) and testing:(100, 51)\n\n\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nStandardScaler for Effective Scaling\n\nIn machine learning algorithms, the scale of our input data can significantly impact the training process and ultimately the model’s performance. This is because many algorithms rely on the distances or magnitudes between data points to make predictions.\nIf features have vastly different scales (e.g., one feature has values ranging from 0 to 10, while another ranges from 1000 to 5000), algorithms like Support Vector Machines (SVM) or k-Nearest Neighbors (k-NN) can be misled. Features with larger scales might dominate the distance calculations, even if they aren’t as predictive as features with smaller scales.\nTo address this issue and ensure all features contribute equally to the model’s learning process, This technique performs standardization, which transforms each feature by subtracting the mean and dividing by the standard deviation. The result is a new set of features where:\n    The mean is 0 (centered around the mean)\n    The standard deviation is 1 (normalized)\nBy applying standardization, all features are placed on a common scale, ensuring that algorithms treat them with equal importance during training.\nI applied standard scaling to the data before using machine learning algorithms.\n\n\n\nExperiments:Supervised MachineLearning Models.\n\nLinear Regression.\n\nlinear_reg_spec = LinearRegression()\nlinear_reg_spec.fit(X_train, y_train)\nlos_preds_tr = linear_reg_spec.predict(X_train)\nlos_preds_tes = linear_reg_spec.predict(X_test)\n\n\ncustom_regression_resid_predict_plot(linear_reg_spec, X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferences\n\nThe R-squared values for the training set (0.25) and testing set (0.07) indicate a significant difference in the model’s performance between the data it was trained on and unseen data. This suggests that the model might be overfitting the training data, leading to a good fit on that specific data but poor generalizability to new data\n\n\n\nDecision Trees.\n\nviz = ValidationCurve(\n    DecisionTreeRegressor(),\n    param_name=\"max_depth\",\n    param_range=np.arange(1, 11),\n    cv=10,\n    scoring=\"r2\",\n)\n\n# Fit and show the visualizer\nviz.fit(X_train, y_train)\nviz.show()\nplt.show()\n\n\n\n\n\n\n\n\n\n_model_spec_DT_reg = DecisionTreeRegressor(max_depth=2)\n\n\n_model_spec_DT_reg.fit(X_train, y_train)\n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(max_depth=2) \n\n\n\ncustom_regression_resid_predict_plot(\n    _model_spec_DT_reg, X_train, y_train, X_test, y_test\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferences\n\nThe R-squared values of 0.47 for the training set and 0.62 for the testing set are interesting. While a higher testing R-squared (0.62) might seem positive, it’s important to consider that a good model typically exhibits a similar or slightly lower R-squared on the testing set compared to the training set.\nIn our case, the training R-squared (0.47) suggests we could do improvement in the model’s ability to learn from the training data. Further investigation such as feature selection, addressing potential biases could be beneficial.\n\n\nviz = FeatureImportances(_model_spec_DT_reg, labels=labels, size=(1200, 800))\nviz.fit(X_train, y_train)\nviz.show()\nplt.show()\n\n\n\n\n\n\n\n\n\nDecisionTrees Feature Importance gives us only 2 variables are having relative importance as showed in the above plot such as number of policies and monthly insurance premium.\n\n\n\nRandomForest.\n\n_model_spec_RF_REG = RandomForestRegressor(random_state=2024)\n\n\n# Define the parameter grid\nparam_grid = {\n    \"n_estimators\": [100, 200],\n    \"max_depth\": [10, 20, 30],\n    # 'min_samples_split': [2, 5, 10, 15],\n    # 'min_samples_leaf': [1, 2, 4, 8],\n    \"max_features\": [2, 5, 8, 12, 15, 17, 21],\n}\n\n# Create the random forest classifier\nrf = RandomForestRegressor()\n\n# Use GridSearchCV or RandomizedSearchCV\ngrid_search = GridSearchCV(rf, param_grid, cv=3, scoring=\"r2\")\n\n# Fit the grid search to your data\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=3, estimator=RandomForestRegressor(),\n             param_grid={'max_depth': [10, 20, 30],\n                         'max_features': [2, 5, 8, 12, 15, 17, 21],\n                         'n_estimators': [100, 200]},\n             scoring='r2')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=3, estimator=RandomForestRegressor(),\n             param_grid={'max_depth': [10, 20, 30],\n                         'max_features': [2, 5, 8, 12, 15, 17, 21],\n                         'n_estimators': [100, 200]},\n             scoring='r2') estimator: RandomForestRegressorRandomForestRegressor()  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor() \n\n\n\n_model_spec_RF_REG = RandomForestRegressor(\n    random_state=2024, max_depth=10, max_features=21, n_estimators=100\n)\n_model_spec_RF_REG.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=10, max_features=21, random_state=2024)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(max_depth=10, max_features=21, random_state=2024) \n\n\n\ncustom_regression_resid_predict_plot(\n    _model_spec_RF_REG, X_train, y_train, X_test, y_test\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferences\n\nThe training and testing R-squared scores (0.94 and 0.72, respectively) reveal a substantial difference in performance, suggesting the model may be overfitting the training data.\n\n\nviz = FeatureImportances(_model_spec_RF_REG, labels=labels, size=(1200, 800))\nviz.fit(X_train, y_train)\nviz.show()\nplt.show()\n\n\n\n\n\n\n\n\n\n_FEAT = pd.DataFrame(\n    {\"feature\": viz.features_, \"importance_\": viz.feature_importances_}\n).sort_values(\"importance_\", ascending=False)\n\n\n_FEAT = _FEAT[_FEAT[\"importance_\"] &gt; 2.6]\n\nRandomforest Regression suggests the below are the potential predictiors to predict the targe variables.\n\nlist(_FEAT[\"feature\"])\n\n['number_of_policies',\n 'monthly_insurance_premium_',\n 'months_since_last_claim',\n 'income',\n 'total_claim_amount',\n 'months_since_policy_inception',\n 'number_of_open_complaints',\n 'vehicle_class_Sports Car',\n 'vehicle_class_SUV',\n 'vehicle_class_Luxury Car',\n 'policy_level_Personal L1',\n 'gender_M',\n 'coverage_Basic',\n 'vehicle_class_Four-Door Car',\n 'marital_status_Married',\n 'renew_offer_type_Offer1']\n\n\nTo gain a deeper understanding of the relationships between specific features and the target variable, I plan to create a new dataset containing only the following variables listed above and I will then conduct experiments using various regression models to assess their effectiveness in predicting the target variable based on this subset of data.\n\nX_predictors_sel = X_predictors[list(_FEAT[\"feature\"])]\n\n\n\nData with subset of predictors.\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_predictors_sel, Y_target, test_size=0.2, random_state=2024\n)\n\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nprint(f\"Trainset:{X_train.shape} and testing:{X_test.shape}\")\nlabels = list(X_predictors_sel.columns)\n\nTrainset:(400, 16) and testing:(100, 16)\n\n\n\nAs we can see in the subset of data we do have only 16 predictors.\n\n\n\nExperiment-2: DecisionTrees\n\nviz = ValidationCurve(\n    DecisionTreeRegressor(),\n    param_name=\"max_depth\",\n    param_range=np.arange(1, 11),\n    cv=10,\n    scoring=\"r2\",\n)\n\n# Fit and show the visualizer\nviz.fit(X_train, y_train)\nviz.show()\nplt.show()\n\n\n\n\n\n\n\n\n\n_model_spec_DT_reg = DecisionTreeRegressor(max_depth=2)\n\n\n_model_spec_DT_reg.fit(X_train, y_train)\n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(max_depth=2) \n\n\n\nlos_preds_dt_train = _model_spec_DT_reg.predict(X_train)\nlos_preds_dt_test =_model_spec_DT_reg.predict(X_test)\n\n\ncustom_regression_resid_predict_plot(\n    _model_spec_DT_reg, X_train, y_train, X_test, y_test\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferences\n\nThe training R-squared (0.47) and testing R-squared (0.62) are similar to our previous results, even after removing features from the dataset. This suggests that decision trees are still capturing important patterns in the data, even without all the original features.\n\n\n\nExperiment-2: RandomForest\n\n_model_spec_RF_REG = RandomForestRegressor(\n    random_state=2024, max_depth=30, max_features=15, n_estimators=200\n)\n_model_spec_RF_REG.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=30, max_features=15, n_estimators=200,\n                      random_state=2024)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(max_depth=30, max_features=15, n_estimators=200,\n                      random_state=2024) \n\n\n\ncustom_regression_resid_predict_plot(\n    _model_spec_RF_REG, X_train, y_train, X_test, y_test\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferences - While training and testing R-squared aren’t directly comparable, the current model still seems overfitted. This is because the testing R-squared remains high even after removing irrelevant features from the random forest.\n\n\nExperiment-2: LinearRegression\n\nlinear_reg_spec = LinearRegression()\nlinear_reg_spec.fit(X_train, y_train)\nlos_preds_tr = linear_reg_spec.predict(X_train)\nlos_preds_tes = linear_reg_spec.predict(X_test)\n\n\ncustom_regression_resid_predict_plot(linear_reg_spec, X_train, y_train, X_test, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInferences\n\nBoth the original model (without feature removal) and the current model (with feature removal) achieved similar R-squared values in training (0.23) and testing (0.19).\n\n\n\nExperiment-2: Boosting-Regression\n\nparam_grid = {\n    \"n_estimators\": hp.quniform(\"n_estimators\", 200, 2500, 100),\n    \"max_depth\": hp.quniform(\"max_depth\", 1, 10, 1),\n    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.001), np.log(1)),\n    \"booster\": hp.choice(\"booster\", [\"gbtree\", \"dart\"]),\n    \"gamma\": hp.loguniform(\"gamma\", np.log(0.01), np.log(10)),\n    \"subsample\": hp.uniform(\"subsample\", 0.50, 0.90),\n    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.50, 0.99),\n    \"colsample_bylevel\": hp.uniform(\"colsample_bylevel\", 0.50, 0.99),\n    \"colsample_bynode\": hp.uniform(\"colsample_bynode\", 0.50, 0.99),\n    \"reg_lambda\": hp.uniform(\"reg_lambda\", 1, 20),\n}\n\ndef objective(params):\n\n    # we need a dictionary to indicate which value from the space\n    # to attribute to each value of the hyperparameter in the xgb\n    params_dict = {\n        # important int, as it takes integers only\n        \"n_estimators\": int(params[\"n_estimators\"]),\n        # important int, as it takes integers only\n        \"max_depth\": int(params[\"max_depth\"]),\n        \"learning_rate\": params[\"learning_rate\"],\n        \"booster\": params[\"booster\"],\n        \"gamma\": params[\"gamma\"],\n        \"subsample\": params[\"subsample\"],\n        \"colsample_bytree\": params[\"colsample_bytree\"],\n        \"colsample_bylevel\": params[\"colsample_bylevel\"],\n        \"colsample_bynode\": params[\"colsample_bynode\"],\n        \"random_state\": 1000,\n    }\n\n    # with ** we pass the items in the dictionary as parameters\n    # to the xgb\n    gbm = xgb.XGBRFRegressor(**params_dict)\n\n    # train with cv\n    score = cross_val_score(gbm, X_train, y_train, scoring=\"r2\", cv=3, n_jobs=4).mean()\n\n    # to minimize, we negate the score\n    return -score\n\n\n# random_search = fmin(\n#     fn=objective,\n#     space=param_grid,\n#     max_evals=50,\n#     rstate=np.random.default_rng(2024),\n#     algo=rand.suggest,  # randomized search\n# )\n\n\n# {'booster': 0,\n#  'colsample_bylevel': 0.8049403095842097,\n#  'colsample_bynode': 0.9831360208702888,\n#  'colsample_bytree': 0.6266590323014749,\n#  'gamma': 1.494561968586083,\n#  'learning_rate': 0.8007241876828338,\n#  'max_depth': 6.0,\n#  'n_estimators': 2200.0,\n#  'reg_lambda': 4.576754746598313,\n#  'subsample': 0.6988738316413677}\n\n\n_XGB_REG_spec_tuned = xgb.XGBRFRegressor(\n    gamma=1.5,\n    learning_rate=0.8,\n    max_depth=6,\n    n_estimators=2200,\n    reg_lambda=4.5,\n    subsample=0.69,\n    colsample_bylevel=0.8,\n    colsample_bynode=0.98,\n    colsample_bytree=0.62,\n)\n\n\n_XGB_REG_spec_tuned.fit(X_train, y_train)\n\nXGBRFRegressor(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=0.8, colsample_bynode=0.98,\n               colsample_bytree=0.62, device=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=1.5, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=0.8, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=6, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               multi_strategy=None, n_estimators=2200, n_jobs=None,\n               num_parallel_tree=None, objective='reg:squarederror', ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBRFRegressoriFittedXGBRFRegressor(base_score=None, booster=None, callbacks=None,\n               colsample_bylevel=0.8, colsample_bynode=0.98,\n               colsample_bytree=0.62, device=None, early_stopping_rounds=None,\n               enable_categorical=False, eval_metric=None, feature_types=None,\n               gamma=1.5, grow_policy=None, importance_type=None,\n               interaction_constraints=None, learning_rate=0.8, max_bin=None,\n               max_cat_threshold=None, max_cat_to_onehot=None,\n               max_delta_step=None, max_depth=6, max_leaves=None,\n               min_child_weight=None, missing=nan, monotone_constraints=None,\n               multi_strategy=None, n_estimators=2200, n_jobs=None,\n               num_parallel_tree=None, objective='reg:squarederror', ...) \n\n\n\nlos_pred_xgb_train=_XGB_REG_spec_tuned.predict(X_train)\nlos_pred_xgb_test=_XGB_REG_spec_tuned.predict(X_test)\n\n\ncustom_regression_resid_predict_plot(\n    _XGB_REG_spec_tuned, X_train, y_train, X_test, y_test\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInfereces\n\nThe XGBRegressor achieved a training R-squared of 0.51 and a testing R-squared of 0.45. This performance suggests it might be a better choice compared to the other 3 models.\n\nI will also take a look at other metrics such as MAE, RMSE of these models and see.\n\n\nRegression Conclusion\nThe table summarizes the performance of two different machine learning models (XGBoost and Trees) on a regression task. The table shows the R-Squared, Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for both the training and testing sets of each model.\n\n# custom_regression_metrics('XGB-Training',y_train,los_pred_xgb_train)\n# custom_regression_metrics('XGB-Testing',y_test,los_pred_xgb_test)\n# custom_regression_metrics('DT-Training',y_train,los_preds_dt_train)\n# custom_regression_metrics('DT-Testing',y_test,los_preds_dt_test)\n\n\n\n\nModel\nType\nR-Squared\nMSE\nMAE\nRMSE\n\n\n\n\nxgb\ntraininig\n0.50\n273333475.39\n2697.63\n5228.14\n\n\nxgb\ntesting\n0.45\n24882119.63\n3223.14\n4988.19\n\n\nTrees\ntraininig\n0.47644\n29179327.39\n3001.25\n5401.78\n\n\nTress\ntesting\n0.62\n17118030.90\n2748.26\n4137.39\n\n\n\nHere’s a breakdown of the metrics for each model:\nXGBoost:\nTraining: XGBoost achieved a training R-Squared of 0.50, which indicates a moderately good fit on the training data. The MSE is 273,333,475.39, the MAE is 2,697.63, and the RMSE is 5,228.14. These values suggest that the model is able to capture some of the underlying patterns in the training data, but there is still some room for improvement.\nTesting: The testing R-Squared is 0.45, which is lower than the training R-Squared. This suggests that the model may be overfitting the training data to some extent. The MSE is 248,821,196.30, the MAE is 3,223.14, and the RMSE is 4,988.19. These values are all slightly lower than the corresponding values for the training set, but the overall trend suggests that the model may not generalize well to unseen data.\nTrees:\nTraining: The Trees model achieved a training R-Squared of 0.476, which is slightly lower than the training R-Squared of XGBoost. The MSE is 291,793,273.39, the MAE is 3,001.25, and the RMSE is 5,401.78. These values suggest that the Trees model may be slightly less accurate than XGBoost on the training data.\nTesting: Surprisingly, the Trees model achieved a higher testing R-Squared of 0.62 compared to XGBoost. This could be an indication of overfitting on the training data for XGBoost or underfitting for Trees. The MSE is 171,180,309.00, the MAE is 2,748.26, and the RMSE is 4,137.39. These values are all lower than the corresponding values for both the training set and the XGBoost testing set. This suggests that the Trees model may be generalizing better to unseen data."
  },
  {
    "objectID": "ISI_SSBB_BA_FinalExam_Code_2024.html#e-and-f-classification-problem.",
    "href": "ISI_SSBB_BA_FinalExam_Code_2024.html#e-and-f-classification-problem.",
    "title": "Final Examination",
    "section": "2.(E AND F): Classification Problem.",
    "text": "2.(E AND F): Classification Problem.\n\nfrom sklearn import preprocessing\nfrom sklearn import utils\nX_predictors = df_insurance_encoded.drop(['discontinued'],axis=1)\nY_target = df_insurance_encoded['discontinued']\nlab_enc = preprocessing.LabelEncoder()\nencoded = lab_enc.fit_transform(Y_target)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_predictors, encoded, test_size=0.2, random_state=0\n)\n\n\nprint(f'Trainset:{X_train.shape} and testing:{X_test.shape}')\n\nTrainset:(400, 50) and testing:(100, 50)\n\n\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nLogisticRgression\n\n_model_LR_A = LogisticRegression(random_state=2024)\n\n\n_model_LR_A.fit(X_train, y_train)\n\nLogisticRegression(random_state=2024)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(random_state=2024) \n\n\n\nlos_tr_preds_A = _model_LR_A.predict(X_train)\nlos_tes_preds_A = _model_LR_A.predict(X_test)\n\n\ncustom_classification_metrics_report('Logistic-training',y_train, los_tr_preds_A)\n\n--------------------------------------\nLogistic-training:ConfusionMatrix and ROC-AUC Curve\n--------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nLogistic-training\n0.7675\n0.74896\n0.69307\n0.73427\n0.65625\n\n\n\n\n\n\n\n\n\ncustom_classification_metrics_report(\n    \"LogisticRegression-Testing:\", y_test, los_tes_preds_A\n)\n\n--------------------------------------\nLogisticRegression-Testing::ConfusionMatrix and ROC-AUC Curve\n--------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nLogisticRegression-Testing:\n0.7\n0.65833\n0.54545\n0.69231\n0.45\n\n\n\n\n\n\n\n\n\n\nDecisionTrees-Classification\n\n## create a decisiont tree and fit it to the training data\n_model_DT_B = DecisionTreeClassifier(random_state=42)\n_model_DT_B.fit(X_train, y_train)\n\nDecisionTreeClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(random_state=42) \n\n\n\nlos_tr_preds_B = _model_DT_B.predict(X_train)\nlos_tes_preds_B = _model_DT_B.predict(X_test)\n\n\nCost Complexity Pruning.\n\npath = _model_DT_B.cost_complexity_pruning_path(\n    X_train, y_train\n)  # determine values for alpha\nccp_alphas = path.ccp_alphas  # extract different values for alpha\nccp_alphas = ccp_alphas[:-1]  # exclude the maximum value for alpha\n\nclf_dts = []  # create an array that we will put decision trees into\n\n## now create one decision tree per value for alpha and store it in the array\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf_dt.fit(X_train, y_train)\n    clf_dts.append(clf_dt)\n\n\ntrain_scores = [clf_dt.score(X_train, y_train) for clf_dt in clf_dts]\ntest_scores = [clf_dt.score(X_test, y_test) for clf_dt in clf_dts]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nIn the graph above, we see that the accuracy for the Testing Dataset hits its maximum value when alpha is about 0.014. After this value for alpha, the accuracy of the Training Dataset drops off and that suggests we should set ccp_alpha=0.014.\n\n\n\nDecision tree pruning.\n\n## Build and train a new decision tree, only this time use the optimal value for alpha\nclf_dt_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=0.014)\nclf_dt_pruned = clf_dt_pruned.fit(X_train, y_train)\n\n\nplt.figure(figsize=(15, 7.5))\nplot_tree(\n    clf_dt_pruned,\n    filled=True,\n    rounded=True,\n    class_names=[\"No\", \"Yes\"],\n    feature_names=X_predictors.columns,\n);\n\n\n\n\n\n\n\n\n\nlos_preds_old = clf_dt_pruned.predict(X_train)\nlos_preds_new = clf_dt_pruned.predict(X_test)\n\n\ncustom_classification_metrics_report(\"DecisionTree-Pruned-Traininig:\", y_train, los_preds_old)\n\n--------------------------------------\nDecisionTree-Pruned-Traininig::ConfusionMatrix and ROC-AUC Curve\n--------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nDecisionTree-Pruned-Traininig:\n0.67\n0.58854\n0.30526\n0.96667\n0.18125\n\n\n\n\n\n\n\n\n\ncustom_classification_metrics_report(\"DecisionTree-Pruned:Testing\", y_test, los_preds_new)\n\n--------------------------------------\nDecisionTree-Pruned:Testing:ConfusionMatrix and ROC-AUC Curve\n--------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nDecisionTree-Pruned:Testing\n0.66\n0.575\n0.26087\n1.0\n0.15\n\n\n\n\n\n\n\n\n\n\nRandomForest-Classification\n\n# Define the parameter grid\nparam_grid = {\n    \"n_estimators\": [100, 200],\n    \"max_depth\": [5, 8, 10],\n    # 'min_samples_split': [2, 5, 10, 15],\n    # 'min_samples_leaf': [1, 2, 4, 8],\n    \"max_features\": [int(np.sqrt(50) * 0.75), int(np.sqrt(50))],\n}\n\n# Create the random forest classifier\nrf = RandomForestClassifier()\n\ngrid_search = GridSearchCV(\n    rf, param_grid, cv=5, scoring=\"accuracy\"\n) \n\n# Fit the grid search to your data\ngrid_search.fit(X_train, y_train)  # Replace X_train and y_train with your training data\n\nGridSearchCV(cv=5, estimator=RandomForestClassifier(),\n             param_grid={'max_depth': [5, 8, 10], 'max_features': [5, 7],\n                         'n_estimators': [100, 200]},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=5, estimator=RandomForestClassifier(),\n             param_grid={'max_depth': [5, 8, 10], 'max_features': [5, 7],\n                         'n_estimators': [100, 200]},\n             scoring='accuracy') estimator: RandomForestClassifierRandomForestClassifier()  RandomForestClassifier?Documentation for RandomForestClassifierRandomForestClassifier() \n\n\n\ngrid_search.best_params_\n\n{'max_depth': 8, 'max_features': 7, 'n_estimators': 100}\n\n\n\n## create a decisiont tree and fit it to the training data\n_model_RF_C_tuned = RandomForestClassifier(\n    max_depth=10, max_features=7, n_estimators=100, random_state=2024\n)\n\n\n_model_RF_C_tuned.fit(X_train, y_train)\n\nRandomForestClassifier(max_depth=10, max_features=7, random_state=2024)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(max_depth=10, max_features=7, random_state=2024) \n\n\n\nlos_tr_preds_C_tuned = _model_RF_C_tuned.predict(X_train)\nlos_tes_preds_C_tuned = _model_RF_C_tuned.predict(X_test)\n\n\ncustom_classification_metrics_report(\"Baggin-Training\", y_train, los_tr_preds_C_tuned)\n\n--------------------------------------\nBaggin-Training:ConfusionMatrix and ROC-AUC Curve\n--------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nBaggin-Training\n0.9975\n0.99688\n0.99687\n1.0\n0.99375\n\n\n\n\n\n\n\n\n\ncustom_classification_metrics_report(\"Bagging-Testing\", y_test, los_tes_preds_C_tuned)\n\n--------------------------------------\nBagging-Testing:ConfusionMatrix and ROC-AUC Curve\n--------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nBagging-Testing\n0.84\n0.80417\n0.75758\n0.96154\n0.625\n\n\n\n\n\n\n\n\n\n\nXGB-Classification\n\nparam_grid = {\n    'n_estimators': hp.quniform('n_estimators', 200, 2500, 100),\n    'max_depth': hp.quniform('max_depth', 1, 10, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(1)),\n    'booster': hp.choice('booster', ['gbtree', 'dart']),\n    'gamma': hp.loguniform('gamma', np.log(0.01), np.log(10)),\n    'subsample': hp.uniform('subsample', 0.50, 0.90),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.50, 0.99),\n    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.50, 0.99),\n    'colsample_bynode': hp.uniform('colsample_bynode', 0.50, 0.99),\n    'reg_lambda': hp.uniform('reg_lambda', 1, 20)\n}\n\n# the objective function takes the hyperparameter space\n# as input\n\ndef objective(params):\n\n    # we need a dictionary to indicate which value from the space\n    # to attribute to each value of the hyperparameter in the xgb\n    params_dict = {\n        # important int, as it takes integers only\n        'n_estimators': int(params['n_estimators']),\n        # important int, as it takes integers only\n        'max_depth': int(params['max_depth']),\n        'learning_rate': params['learning_rate'],\n        'booster': params['booster'],\n        'gamma': params['gamma'],\n        'subsample': params['subsample'],\n        'colsample_bytree': params['colsample_bytree'],\n        'colsample_bylevel': params['colsample_bylevel'],\n        'colsample_bynode': params['colsample_bynode'],\n        'random_state': 1000,\n    }\n\n    # with ** we pass the items in the dictionary as parameters\n    # to the xgb\n    gbm = xgb.XGBClassifier(**params_dict)\n\n    # train with cv\n    score = cross_val_score(gbm, X_train, y_train,\n                            scoring='accuracy', cv=3, n_jobs=4).mean()\n\n    # to minimize, we negate the score\n    return -score\n\n\n# fmin performs the minimization\n# rand.suggest samples the parameters at random\n# i.e., performs the random search\n\nrandom_search = fmin(\n    fn=objective,\n    space=param_grid,\n    max_evals=1,\n    rstate=np.random.default_rng(2024),\n    algo=rand.suggest,  # randomized search\n)\n\n100%|██████████| 1/1 [00:18&lt;00:00, 18.16s/trial, best loss: -0.6674148056708936]\n\n\n\nrandom_search\n\n{'booster': 1,\n 'colsample_bylevel': 0.546007358061281,\n 'colsample_bynode': 0.5187931176469971,\n 'colsample_bytree': 0.7322586366977841,\n 'gamma': 8.110007681126655,\n 'learning_rate': 0.021701657654466605,\n 'max_depth': 3.0,\n 'n_estimators': 300.0,\n 'reg_lambda': 9.465235229189696,\n 'subsample': 0.6343753989670202}\n\n\n\n_XGB_CLS_spec_tuned = xgb.XGBClassifier(\n    objective=\"binary:logistic\",\n    booster='dart',\n    gamma=8.11,\n    learning_rate=0.02,\n    max_depth=3,\n    n_estimators=300,\n    reg_lambda=9.46,\n    subsample=0.63,\n    colsample_bylevel=0.5,\n    colsample_bynode=0.5,\n    colsample_bytree=0.7,\n)\n\n\n_XGB_CLS_spec_tuned.fit(X_train,y_train)\n\nXGBClassifier(base_score=None, booster='dart', callbacks=None,\n              colsample_bylevel=0.5, colsample_bynode=0.5, colsample_bytree=0.7,\n              device=None, early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, feature_types=None, gamma=8.11,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.02, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=3, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=300, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriFittedXGBClassifier(base_score=None, booster='dart', callbacks=None,\n              colsample_bylevel=0.5, colsample_bynode=0.5, colsample_bytree=0.7,\n              device=None, early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=None, feature_types=None, gamma=8.11,\n              grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.02, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=3, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=300, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) \n\n\n\nlos_tr_preds_xgb_tuned = _XGB_CLS_spec_tuned.predict(X_train)\nlos_tes_preds_xgb_tuned = _XGB_CLS_spec_tuned.predict(X_test)\n\n\ncustom_classification_metrics_report(\"Boosting-Training\", y_train, los_tr_preds_xgb_tuned)\n\n--------------------------------------\nBoosting-Training:ConfusionMatrix and ROC-AUC Curve\n--------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nBoosting-Training\n0.655\n0.56979\n0.25\n0.95833\n0.14375\n\n\n\n\n\n\n\n\n\ncustom_classification_metrics_report(\"Boosting-Testing\", y_test, los_tes_preds_xgb_tuned)\n\n--------------------------------------\nBoosting-Testing:ConfusionMatrix and ROC-AUC Curve\n--------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nBoosting-Testing\n0.64\n0.55\n0.18182\n1.0\n0.1\n\n\n\n\n\n\n\n\n\n\nClassification ModelSummary\n\n\n\nmodel\ntraining-Accuracy\ntesting-Accuracy\n\n\n\n\nLogisticRegression\n0.76\n0.72\n\n\nTree\n0.67\n0.66\n\n\nBagging\n0.99\n0.84\n\n\nBoosting\n0.65\n0.64\n\n\n\n1. Logistic Regression: This model achieved a good training accuracy of 76% but dropped slightly to 72% on testing data. This indicates a reasonable fit on the training data but suggests some room for improvement in generalizing to unseen data.\n2.Tree: This model has a lower training accuracy (67%) compared to Logistic Regression. Its testing accuracy is also lower (66%) suggesting it might not be capturing the underlying patterns in the data as effectively.\n3.Bagging: This model shows a very high training accuracy (99%). However, the testing accuracy drops significantly to 84%. This is a classic sign of overfitting. The model memorized the training data too well and performs poorly on unseen data.\n4.Boosting: This model has the lowest training accuracy (65%) among all. However, its testing accuracy is similar (64%). This suggests the model might not be the most accurate but it generalizes reasonably well to unseen data.\nIn a conclusion - Logistic Regression seems like a balanced choice with decent performance on both training and testing data.\n\nTree might need further exploration or tuning to improve its accuracy.\nBagging suffers from severe overfitting and should not be used without further adjustments.\nBoosting, despite lower training accuracy, generalizes well and could be a good option depending on the specific needs.\n\n\n\nSuggest an optimal model (s) for predicting the CLF and Discontinuance.\n\nXGBoost Regressor proved to be the most effective model for predicting the continuous CLF target variable. Additionally, XGBoost Classifier emerged as the best model for classifying customer churn, meaning it can best predict whether a customer will discontinue their service or not."
  },
  {
    "objectID": "SSBBA_W4.html",
    "href": "SSBBA_W4.html",
    "title": "Statical Methods: Chi-Square, ANNOVA and Regression Analysis.",
    "section": "",
    "text": "import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.graphics.gofplots as gof\nfrom scipy.stats import chi2_contingency, chisquare\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats import outliers_influence as sm_oi\nfrom statsmodels.stats.anova import anova_lm\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_theme(\"notebook\", \"whitegrid\")"
  },
  {
    "objectID": "SSBBA_W4.html#custom-functions",
    "href": "SSBBA_W4.html#custom-functions",
    "title": "Statical Methods: Chi-Square, ANNOVA and Regression Analysis.",
    "section": "Custom Functions",
    "text": "Custom Functions\n\ndef custom_statsmodel_OLS(_DF, *vars):\n    \"\"\"fitting OLS on specified independent and dependent variables- DF, dependent_var and independent_var\"\"\"\n    # sm.add_constant\n    try:\n        LOS_COLS = [v for v in vars]\n        _X = LOS_COLS[1:]\n        _Y = LOS_COLS[0]\n        xvars = sm.add_constant(_DF[_X])\n        yvar = _DF[_Y]\n        _model_spec = sm.OLS(yvar, xvars)\n        return _model_spec\n    except Exception as e:\n        print(f\"There is an error while creating a model spec due to:{e}\")\n\n\ndef custom_model_preds(_model, _new_df):\n    \"\"\"Predictions on new data points\"\"\"\n    _feat = sm.add_constant(_new_df)\n    _pred = _model.predict(sm.add_constant(_feat))\n    _df_pred = pd.DataFrame(_pred)\n    _df_pred.columns = [\"predicted_y\"]\n    return _df_pred\n\n\ndef custom_VIF(_MSPEC):\n    \"\"\"Custom function to get the VIF\"\"\"\n    var_names = _MSPEC.exog_names\n    X = _MSPEC.exog\n    _limit = X.shape[1]\n    try:\n        vif_dict = {}\n        for idx in range(_limit):\n            vif = round(sm_oi.variance_inflation_factor(X, idx), 5)\n            vif_dict[var_names[idx]] = vif\n        _DF = pd.DataFrame([vif_dict]).T\n        _DF.columns = [\"VIF\"]\n        _DF = _DF.reset_index()\n        df_sorted = _DF.iloc[1:].sort_values(by=\"VIF\", ascending=False)\n        ax = sns.barplot(x=\"index\", y=\"VIF\", data=df_sorted)\n        # Add text labels to the top of each bar\n        for bar in ax.containers[0]:\n            ax.text(\n                bar.get_x() + bar.get_width() / 2,\n                bar.get_height(),\n                int(bar.get_height()),\n                ha=\"center\",\n                va=\"bottom\",\n            )\n        ax.set_xlabel(\"FIELD\")\n        ax.set_ylabel(\"VIF\")\n        plt.xticks(rotation=45)\n        plt.title(\"VIF\")\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        pass\n\n\ndef custom_ols_qqplot(_resid):\n    \"\"\"Q-Q Plot of residuals\"\"\"\n    gof.qqplot(_resid, line=\"s\")\n    plt.xlabel(\"Standard Normal Quantiles\")\n    plt.ylabel(\"Standardized Residuals\")\n    plt.title(\"Normal Q-Q plot\")\n    plt.show()\n\n\ndef custom_ols_res_vs_fitted(_fitted, _resid):\n    \"\"\"Fitted Vs Residuals Plot\"\"\"\n    plt.scatter(_fitted, _resid)\n    plt.axhline(\"0\", color=\"r\")\n    plt.xlabel(\"Fitted Values\")\n    plt.ylabel(\"Residual\")\n    plt.title(\"Residual Vs Fitted\")"
  },
  {
    "objectID": "SSBBA_W4.html#chi-square-and-annova--cases",
    "href": "SSBBA_W4.html#chi-square-and-annova--cases",
    "title": "Statical Methods: Chi-Square, ANNOVA and Regression Analysis.",
    "section": "Chi-Square and Annova- Cases",
    "text": "Chi-Square and Annova- Cases\n\n# Importing data\ndf_ecom = pd.read_excel(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/w4/data/ecom.xlsx\"\n)\ndf_smoke = pd.read_csv(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/w4/data/smoking.csv\"\n)\ndf_health = pd.read_excel(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/w4/data/HealthStats_SSBB.xlsx\"\n)\n\n\ndf_ecom.head()\n\n\n\n\n\n\n\n\n\nGender\nAge\nOverall Use_Level\nAmazon_Level\nFlipkart_Level\nSwiggy_Level\nZomato_Level\nOthers_Level\nOverall CS_Rating\nOverall CS_Level\n\n\n\n\n0\nMale\n34\nMedium\nHigh\nMedium\nMedium\nMedium\nLow\n7\nMedium\n\n\n1\nMale\n33\nHigh\nHigh\nLow\nMedium\nMedium\nMedium\n9\nHigh\n\n\n2\nFemale\n34\nHigh\nHigh\nLow\nLow\nLow\nMedium\n8\nMedium\n\n\n3\nMale\n32\nLow\nMedium\nMedium\nLow\nMedium\nLow\n8\nMedium\n\n\n4\nFemale\n40\nLow\nLow\nMedium\nLow\nLow\nLow\n7\nLow\n\n\n\n\n\n\n\n\n\ndf_health.head()\n\n\n\n\n\n\n\n\n\nParticipant No.\nData Segment\nIndustry\nStress-Per\nStress-Pro\nActivity_Level\nAge\nSex\nHeight_cm\nWeight_Kg\n...\nBMI\nBMI-Category\nBody-Fat\nBody Fat Level\nBody-Age\nBody Age Level\nBody Age Level.1\nCal-K\nHappiness-Index\nHappiness-Index-State\n\n\n\n\n0\n1\nGroup-1\nITES\nMedium\nHigh\nHigh\n30\nF\n148.0\n51.5\n...\n23.5\nNormal\n31.2\nHigh\n37.0\n7.0\nNormal\n1132.0\n49\nHappy\n\n\n1\n2\nGroup-1\nMfg & Process\nLow\nHigh\nMedium\n40\nM\n163.0\n79.4\n...\n29.9\nHigh\n31.8\nHigh\n56.0\n16.0\nAbove\n1698.0\n36\nHappy\n\n\n2\n3\nGroup-1\nITES\nMedium\nMedium\nHigh\n42\nF\n143.5\n59.7\n...\n28.8\nHigh\n36.5\nHigh\n55.0\n13.0\nAbove\n1232.0\n24\nHappy\n\n\n3\n4\nGroup-1\nITES\nMedium\nMedium\nHigh\n34\nM\n170.0\n78.4\n...\n27.1\nHigh\n26.6\nNormal\n47.0\n13.0\nAbove\n1718.0\n47\nHappy\n\n\n4\n5\nGroup-1\nMfg & Process\nLow\nMedium\nHigh\n31\nM\n170.0\n96.2\n...\n33.3\nObese\n29.6\nNormal\n55.0\n24.0\nAbove\n1987.0\n38\nHappy\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\ndf_ecom_sel = df_ecom[[\"Overall Use_Level\", \"Overall CS_Level\"]]\n\n\nExperiment 1\nHypothesis\n        Ho: There is no association between Overall Usage and Customer CS Score(Independent)\n        Ha: There is an association between Overall Usage and Customer CS Score(Dependent)\n\ndf_ecom_CT = (\n    pd.crosstab(df_ecom_sel[\"Overall Use_Level\"], df_ecom_sel[\"Overall CS_Level\"])\n    .reset_index()\n    .drop([\"Overall Use_Level\"], axis=1)\n)\n\nCS_LVEL = df_ecom_CT.to_numpy()\n\n\n_chi2, _pvalue, _ddof, _expected = chi2_contingency(CS_LVEL)\n\n\nprint(\n    f\"P-Value Caluclated is:{round(_pvalue,3)} which is lesser than to 0.05, hence we can reject the null hypothesis, degrees of freedom:{_ddof} and chi-square stat:{round(_chi2,5)}\"\n)\n\nP-Value Caluclated is:0.041 which is lesser than to 0.05, hence we can reject the null hypothesis, degrees of freedom:4 and chi-square stat:9.96292\n\n\n\n\nExperiment 2:\nHypothesis\n        Ho: There is no association between Maritual Status and Smoking\n        Ha: There is an association between Maritual Status and Smoking\nHypothesis\n        Ho: There is no association between Gender and Smoking\n        Ha: There is an association between Gender and Smoking\n\ndf_smoke_tidy = df_smoke[[\"marital_status\", \"smoke\"]]\ndf_gender_tidy = df_smoke[[\"gender\", \"smoke\"]]\ndf_smoke_CT = (\n    pd.crosstab(df_smoke_tidy[\"marital_status\"], df_smoke_tidy[\"smoke\"])\n    .reset_index()\n    .drop([\"marital_status\"], axis=1)\n)\ndf_gender_CT = (\n    pd.crosstab(df_gender_tidy[\"gender\"], df_gender_tidy[\"smoke\"])\n    .reset_index()\n    .drop([\"gender\"], axis=1)\n)\nsmoke_NUM = df_smoke_CT.to_numpy()\ngender_NUM = df_gender_CT.to_numpy()\n\n\n# ChiSquare ex1\n_chi2, _pvalue, _ddof, _expected = chi2_contingency(smoke_NUM)\n\n\nprint(\n    f\"P-Value Caluclated is:{round(_pvalue,3)} which is lesser than to 0.05, hence we can reject the null hypothesis, degrees of freedom:{_ddof} and chi-square stat:{round(_chi2,5)}\"\n)\n\nP-Value Caluclated is:0.0 which is lesser than to 0.05, hence we can reject the null hypothesis, degrees of freedom:4 and chi-square stat:74.97979\n\n\n\n# ChiSquare ex2\n_chi2, _pvalue, _ddof, _expected = chi2_contingency(gender_NUM)\n\n\nprint(\n    f\"P-Value Caluclated is:{round(_pvalue,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis, degrees of freedom:{_ddof} and chi-square stat:{round(_chi2,5)}\"\n)\n\nP-Value Caluclated is:0.513 which is greater than to 0.05, hence we failed to reject the null hypothesis, degrees of freedom:1 and chi-square stat:0.42699\n\n\n\n\nExperiment 3\nHypothesis\n        Ho: There is no association between Industry and HappyNess Index\n        Ha: There is an association between Industry and HappyNess Index\n\ndf_health_CT = (\n    pd.crosstab(df_health[\"Industry\"], df_health[\"Happiness-Index-State\"])\n    .reset_index()\n    .drop([\"Industry\"], axis=1)\n)\n\nhealth_NUM = df_health_CT.to_numpy()\n\n\n# ChiSquare ex2\n_chi2, _pvalue, _ddof, _expected = chi2_contingency(health_NUM)\n\n\nprint(\n    f\"P-Value Caluclated is:{round(_pvalue,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis, degrees of freedom:{_ddof} and chi-square stat:{round(_chi2,5)}\"\n)\n\nP-Value Caluclated is:0.08 which is greater than to 0.05, hence we failed to reject the null hypothesis, degrees of freedom:2 and chi-square stat:5.05299\n\n\nConclusions\n\nThere is an association between Maritual Status and Smoking\nThere is no association between Gender and Smoking\nThere is no association between Industry and HappyNess Index\n\n\n\nExperiment 4:\nHypothesis\n        Ho: The average age of all marital status are equal\n        Ha: The average age of all marital status are not equal\nHypothesis\n        Ho: The average age of all usage level customers are equal\n        Ha: The average age of all usage level customers are not equal\n        \nHypothesis\n        Ho: The average BMI of All industry participants are equal\n        Ha: The average BMI of All industry participants are not equal\n\ndf_msage = df_smoke[[\"marital_status\", \"age\"]]\n\n# Define the model formula\nmodel_1 = ols(\"age ~ marital_status\", data=df_msage).fit()\n\n# Perform ANOVA\nanova_table_ex1 = anova_lm(model_1)\n\n# Print ANOVA results\nprint(anova_table_ex1)\n\n                    df         sum_sq       mean_sq           F         PR(&gt;F)\nmarital_status     4.0  234048.777309  58512.194327  274.597439  7.189135e-182\nResidual        1686.0  359258.847765    213.083540         NaN            NaN\n\n\n\nprint(\n    f\"P-Value Caluclated is:{round(anova_table_ex1['PR(&gt;F)'][0],3)} which is lesser than to 0.05, hence we can reject the null hypothesis\"\n)\n\nP-Value Caluclated is:0.0 which is lesser than to 0.05, hence we can reject the null hypothesis\n\n\n\ndf_cs = df_ecom[[\"Overall Use_Level\", \"Age\"]]\ndf_cs.columns = [\"usage\", \"age\"]\n\n# Define the model formula\nmodel_2 = ols(\"age ~ usage\", data=df_cs).fit()\n\n# Perform ANOVA\nanova_table_ex2 = anova_lm(model_2)\n\n# Print ANOVA results\nprint(anova_table_ex2)\n\n            df       sum_sq    mean_sq        F    PR(&gt;F)\nusage      2.0    23.580519  11.790260  0.17447  0.840748\nResidual  30.0  2027.328571  67.577619      NaN       NaN\n\n\n\nprint(\n    f\"P-Value Caluclated is:{round(anova_table_ex2['PR(&gt;F)'][0],3)} which is greater than to 0.05, hence we failed to reject the null hypothesis\"\n)\n\nP-Value Caluclated is:0.841 which is greater than to 0.05, hence we failed to reject the null hypothesis\n\n\n\ndf_h = df_health[[\"Industry\", \"BMI\"]]\n\n# Define the model formula\nmodel_3 = ols(\"BMI ~ Industry\", data=df_h).fit()\n\n# Perform ANOVA\nanova_table_ex3 = anova_lm(model_3)\n\n# Print ANOVA results\nprint(anova_table_ex3)\n\n             df       sum_sq    mean_sq         F    PR(&gt;F)\nIndustry    2.0   173.765060  86.882530  5.370873  0.005568\nResidual  153.0  2475.021863  16.176613       NaN       NaN\n\n\n\nprint(\n    f\"P-Value Caluclated is:{round(anova_table_ex3['PR(&gt;F)'][0],3)} which is lesser than to 0.05, hence we reject the null hypothesis\"\n)\n\nP-Value Caluclated is:0.006 which is lesser than to 0.05, hence we reject the null hypothesis\n\n\nConclusions\n\nThe average age of all usage level customers are equal\nThe average age of all marital status are not equal\nThe average BMI of All industry participants are not equal"
  },
  {
    "objectID": "SSBBA_W4.html#linear-regression-analysis---cases",
    "href": "SSBBA_W4.html#linear-regression-analysis---cases",
    "title": "Statical Methods: Chi-Square, ANNOVA and Regression Analysis.",
    "section": "Linear Regression Analysis - Cases",
    "text": "Linear Regression Analysis - Cases\n\nCase 1: BikeShare\n\nData Importing and Data Preparation\n\ndf_dcbikes = pd.read_csv(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/w4/data/dcbikeshares.csv\"\n)\n\n\ndf_dcbikes.head()\n\n\n\n\n\n\n\n\n\ninstant\ndteday\nseason\nyr\nmnth\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n0\n1\n1/1/2011\n1\n0\n1\n0\n6\n0\n2\n0.344167\n0.363625\n0.805833\n0.160446\n331\n654\n985\n\n\n1\n2\n1/2/2011\n1\n0\n1\n0\n0\n0\n2\n0.363478\n0.353739\n0.696087\n0.248539\n131\n670\n801\n\n\n2\n3\n1/3/2011\n1\n0\n1\n0\n1\n1\n1\n0.196364\n0.189405\n0.437273\n0.248309\n120\n1229\n1349\n\n\n3\n4\n1/4/2011\n1\n0\n1\n0\n2\n1\n1\n0.200000\n0.212122\n0.590435\n0.160296\n108\n1454\n1562\n\n\n4\n5\n1/5/2011\n1\n0\n1\n0\n3\n1\n1\n0.226957\n0.229270\n0.436957\n0.186900\n82\n1518\n1600\n\n\n\n\n\n\n\n\n\ndf_dcbikes_tidy = df_dcbikes[\n    [\n        \"season\",\n        \"holiday\",\n        \"workingday\",\n        \"weathersit\",\n        \"temp\",\n        \"atemp\",\n        \"hum\",\n        \"windspeed\",\n        \"casual\",\n        \"registered\",\n        \"cnt\",\n    ]\n]\n\n\ndf_dcbikes_tidy.head()\n\n\n\n\n\n\n\n\n\nseason\nholiday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n0\n1\n0\n0\n2\n0.344167\n0.363625\n0.805833\n0.160446\n331\n654\n985\n\n\n1\n1\n0\n0\n2\n0.363478\n0.353739\n0.696087\n0.248539\n131\n670\n801\n\n\n2\n1\n0\n1\n1\n0.196364\n0.189405\n0.437273\n0.248309\n120\n1229\n1349\n\n\n3\n1\n0\n1\n1\n0.200000\n0.212122\n0.590435\n0.160296\n108\n1454\n1562\n\n\n4\n1\n0\n1\n1\n0.226957\n0.229270\n0.436957\n0.186900\n82\n1518\n1600\n\n\n\n\n\n\n\n\nEDA 1: Visualize the distribution of daily bike rentals and temperature as well as the relationship between these two variables.\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\nv1 = sns.regplot(x=\"temp\", y=\"cnt\", data=df_dcbikes_tidy, ax=ax1)\nv2 = sns.heatmap(df_dcbikes_tidy[[\"temp\", \"cnt\"]].corr(), annot=True, ax=ax2)\nplt.show()\n\n\n\n\n\n\n\n\nEDA2: Visualize the distribution of daily bike rentals and temperature per season\n\ndf_eda2 = df_dcbikes_tidy[[\"season\", \"temp\", \"cnt\"]]\n\n\nvis_season_grid = sns.FacetGrid(df_dcbikes_tidy, col=\"season\")\nvis_season_grid.map_dataframe(sns.regplot, x=\"temp\", y=\"cnt\")\nvis_season_grid.add_legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\nv1 = sns.heatmap(\n    df_eda2.loc[df_eda2[\"season\"] == 1, [\"temp\", \"cnt\"]].corr(),\n    annot=True,\n    ax=axes[0, 0],\n)\nv2 = sns.heatmap(\n    df_eda2.loc[df_eda2[\"season\"] == 2, [\"temp\", \"cnt\"]].corr(),\n    annot=True,\n    ax=axes[0, 1],\n)\nv3 = sns.heatmap(\n    df_eda2.loc[df_eda2[\"season\"] == 3, [\"temp\", \"cnt\"]].corr(),\n    annot=True,\n    ax=axes[1, 0],\n)\nv4 = sns.heatmap(\n    df_eda2.loc[df_eda2[\"season\"] == 4, [\"temp\", \"cnt\"]].corr(),\n    annot=True,\n    ax=axes[1, 1],\n)\nv1.set_title(\"Season-1(Winter) TEMP VS CNT\")\nv2.set_title(\"Season-2(Spring) TEMP VS CNT\")\nv3.set_title(\"Season-3(Summer) TEMP VS CNT\")\nv4.set_title(\"Season-4(Fall) TEMP VS CNT\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\nv1 = sns.heatmap(\n    df_dcbikes_tidy.loc[\n        df_dcbikes_tidy[\"season\"] == 1, [\"temp\", \"atemp\", \"hum\", \"windspeed\", \"cnt\"]\n    ].corr(),\n    annot=True,\n    ax=axes[0, 0],\n)\nv2 = sns.heatmap(\n    df_dcbikes_tidy.loc[\n        df_dcbikes_tidy[\"season\"] == 2, [\"temp\", \"atemp\", \"hum\", \"windspeed\", \"cnt\"]\n    ].corr(),\n    annot=True,\n    ax=axes[0, 1],\n)\nv3 = sns.heatmap(\n    df_dcbikes_tidy.loc[\n        df_dcbikes_tidy[\"season\"] == 3, [\"temp\", \"atemp\", \"hum\", \"windspeed\", \"cnt\"]\n    ].corr(),\n    annot=True,\n    ax=axes[1, 0],\n)\nv4 = sns.heatmap(\n    df_dcbikes_tidy.loc[\n        df_dcbikes_tidy[\"season\"] == 4, [\"temp\", \"atemp\", \"hum\", \"windspeed\", \"cnt\"]\n    ].corr(),\n    annot=True,\n    ax=axes[1, 1],\n)\nv1.set_title(\"Season-1(Winter)\")\nv2.set_title(\"Season-2(Spring)\")\nv3.set_title(\"Season-3(Summer)\")\nv4.set_title(\"Season-4(Fall)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRegression Model\n\nExperiment 1:One Numerical\nRegression Equaltion: \\(bikecounts = {\\beta_0} + {\\beta_1}*temp+e\\)\n\nOLS_M1 = custom_statsmodel_OLS(df_eda2, \"cnt\", \"temp\")\n\n\nOLS_M1_fit = OLS_M1.fit()\n\n\nprint(OLS_M1_fit.summary2())\n\n                  Results: Ordinary least squares\n===================================================================\nModel:              OLS              Adj. R-squared:     0.393     \nDependent Variable: cnt              AIC:                12777.5357\nDate:               2024-03-29 11:14 BIC:                12786.7245\nNo. Observations:   731              Log-Likelihood:     -6386.8   \nDf Model:           1                F-statistic:        473.5     \nDf Residuals:       729              Prob (F-statistic): 2.81e-81  \nR-squared:          0.394            Scale:              2.2783e+06\n--------------------------------------------------------------------\n            Coef.    Std.Err.     t     P&gt;|t|     [0.025     0.975] \n--------------------------------------------------------------------\nconst     1214.6421  161.1635   7.5367  0.0000   898.2421  1531.0421\ntemp      6640.7100  305.1880  21.7594  0.0000  6041.5577  7239.8623\n-------------------------------------------------------------------\nOmnibus:              20.477        Durbin-Watson:           0.468 \nProb(Omnibus):        0.000         Jarque-Bera (JB):        12.566\nSkew:                 0.167         Prob(JB):                0.002 \nKurtosis:             2.452         Condition No.:           7     \n===================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors\nis correctly specified.\n\n\n\\(\\hat{bikecounts} = {1214.64} + {6640.71}*\\hat{temp}+e\\)\nInferences:\n\nThe p-value represents the probability of observing a slope as extreme or more extreme as the one we calculated in the sample, assuming there is truly no relationship between X and Y i.e null hypothesis. In our case p-value caluclated for temp is 0 hence we can reject the null in favor of alternate i.e there is a truly relationship between temp and counts.\nFor every increase of 1 unit in temperature, there is an associated increase of, on average, 6640 units of bike counts.\nAn average of bike counts 1214 when the temp is 0\nThe proportion of variability in the outcome variable i.e bike count explained by this model is about 0.39.\n\n\ncustom_ols_qqplot(OLS_M1_fit.resid)\n\n\n\n\n\n\n\n\nComment: The Q-Q plot indicates that the residuals are approximately normally distributed.\n\ncustom_ols_res_vs_fitted(OLS_M1_fit.fittedvalues, OLS_M1_fit.resid)\n\n\n\n\n\n\n\n\nComment: The fitted vs. residual plot suggests a random scatter of residuals, with no apparent trends.\n\ncustom_VIF(OLS_M1)\n\n\n\n\n\n\n\n\nComment: The VIF suggests that temp might have a relatively low collinearity with other independent variables, making it a potentially good choice for exploration.\n\n_new_df = pd.DataFrame({\"temp\": [0.35, 0.28, 0.34]})\ncustom_model_preds(OLS_M1_fit, _new_df)\n\n\n\n\n\n\n\n\n\npredicted_y\n\n\n\n\n0\n3538.890619\n\n\n1\n3074.040919\n\n\n2\n3472.483519\n\n\n\n\n\n\n\n\n\n\nExperiment 2: One numerical and One Categorical\n\\(bikecounts = {\\beta_0} + {\\beta_1}*temp+{\\beta_2}*season2+{\\beta_1}*season3+{\\beta_1}*season4+e\\)\n\ndf_eda2_encoded = pd.get_dummies(\n    df_eda2, columns=[\"season\"], dtype=\"int\", drop_first=True\n)\n\n\nOLS_M2 = custom_statsmodel_OLS(\n    df_eda2_encoded, \"cnt\", \"temp\", \"season_2\", \"season_3\", \"season_4\"\n)\n\n\nOLS_M2_fit = OLS_M2.fit()\n\n\nprint(OLS_M2_fit.summary2())\n\n                  Results: Ordinary least squares\n===================================================================\nModel:              OLS              Adj. R-squared:     0.453     \nDependent Variable: cnt              AIC:                12704.6549\nDate:               2024-03-29 11:14 BIC:                12727.6269\nNo. Observations:   731              Log-Likelihood:     -6347.3   \nDf Model:           4                F-statistic:        152.0     \nDf Residuals:       726              Prob (F-statistic): 2.05e-94  \nR-squared:          0.456            Scale:              2.0537e+06\n--------------------------------------------------------------------\n            Coef.    Std.Err.     t     P&gt;|t|     [0.025     0.975] \n--------------------------------------------------------------------\nconst      745.7873  187.4757   3.9780  0.0001   377.7282  1113.8465\ntemp      6241.3453  518.1419  12.0456  0.0000  5224.1099  7258.5806\nseason_2   848.7236  197.0817   4.3065  0.0000   461.8056  1235.6416\nseason_3   490.1956  259.0055   1.8926  0.0588   -18.2936   998.6848\nseason_4  1342.8730  164.5878   8.1590  0.0000  1019.7482  1665.9978\n-------------------------------------------------------------------\nOmnibus:                7.571        Durbin-Watson:           0.523\nProb(Omnibus):          0.023        Jarque-Bera (JB):        5.112\nSkew:                   0.011        Prob(JB):                0.078\nKurtosis:               2.591        Condition No.:           14   \n===================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors\nis correctly specified.\n\n\n\\(\\hat{bikecounts}={745.7873}+6241.34*\\hat{temp}+848.72*\\hat{season2}+490.19*\\hat{season3}+1342.87*\\hat{season4}+e\\)\nInferences: 1. P-value caluclated - For temp is 0, we can reject the null in favor of alternate i.e there is a truly relationship between temp and counts. - For season_2 is 0.001, we can reject the null in favor of alternate i.e there is a truly relationship between Spring season and bike counts. - For season_3 is 0.05, we failed to reject the null i.e there is a truly no relationship between summer season and bike counts. - For season_4 is 0.0000, we can reject the null in favor of alternate i.e there is a truly relationship between Fall season and bike counts.\n\nTaking into account all the other explanatory variables in our model, For every increase of 1 unit in temperature, there is an associated increase of, on average, 6241 units of bike counts.\nTaking into account all the other explanatory variables in our model,\n\nIn ‘Season2(Spring)’ the average number of bike counts 848 units higher on average compared to the Season1(Winter).\nIn ‘Season4(Fall)’ the average number of bike counts 1342 units higher on average compared to the Season1(Winter).\n\nAn average of bike counts 745 when all the exploratory variables are zero\nThe proportion of variability in the outcome variable i.e bike count explained by this model is about 0.45.\n\n\ncustom_ols_qqplot(OLS_M2_fit.resid)\n\n\n\n\n\n\n\n\n\ncustom_ols_res_vs_fitted(OLS_M2_fit.fittedvalues, OLS_M2_fit.resid)\n\n\n\n\n\n\n\n\n\ncustom_VIF(OLS_M2)\n\n\n\n\n\n\n\n\n\n_new_df = df_eda2_encoded.loc[:10, [\"temp\", \"season_2\", \"season_3\", \"season_4\"]]\n\n\ncustom_model_preds(OLS_M2_fit, _new_df)\n\n\n\n\n\n\n\n\n\npredicted_y\n\n\n\n\n0\n2893.852416\n\n\n1\n3014.379035\n\n\n2\n1971.362862\n\n\n3\n1994.056394\n\n\n4\n2162.304338\n\n\n5\n2021.193763\n\n\n6\n1972.348995\n\n\n7\n1775.609309\n\n\n8\n1609.171355\n\n\n9\n1687.188171\n\n\n10\n1801.142653\n\n\n\n\n\n\n\n\n\n\n\nExperiment 3: More than one numerical: cnt = f(temp, atemp, hum, windspeed)\n\ndf_exp_3 = df_dcbikes_tidy[[\"temp\", \"atemp\", \"hum\", \"windspeed\", \"cnt\"]]\n\n\nsns.heatmap(df_exp_3.corr(), annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nOLS_M3 = custom_statsmodel_OLS(df_exp_3, \"cnt\", \"temp\", \"atemp\", \"hum\", \"windspeed\")\n\n\nOLS_M3_fit = OLS_M3.fit()\n\n\nprint(OLS_M3_fit.summary2())\n\n                  Results: Ordinary least squares\n===================================================================\nModel:              OLS              Adj. R-squared:     0.461     \nDependent Variable: cnt              AIC:                12693.7344\nDate:               2024-03-29 11:14 BIC:                12716.7065\nNo. Observations:   731              Log-Likelihood:     -6341.9   \nDf Model:           4                F-statistic:        157.0     \nDf Residuals:       726              Prob (F-statistic): 9.23e-97  \nR-squared:          0.464            Scale:              2.0232e+06\n-------------------------------------------------------------------\n            Coef.     Std.Err.    t    P&gt;|t|    [0.025     0.975]  \n-------------------------------------------------------------------\nconst      3860.3685  355.3890 10.8624 0.0000  3162.6557  4558.0812\ntemp       2111.8136 2282.1976  0.9253 0.3551 -2368.6810  6592.3083\natemp      5139.1524 2576.9972  1.9942 0.0465    79.8964 10198.4085\nhum       -3149.1098  383.9943 -8.2009 0.0000 -3902.9815 -2395.2380\nwindspeed -4528.6748  721.0854 -6.2804 0.0000 -5944.3362 -3113.0134\n-------------------------------------------------------------------\nOmnibus:                7.790        Durbin-Watson:           0.410\nProb(Omnibus):          0.020        Jarque-Bera (JB):        6.102\nSkew:                   0.124        Prob(JB):                0.047\nKurtosis:               2.628        Condition No.:           91   \n===================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors\nis correctly specified.\n\n\n\\(\\hat{bikecounts}={3860.36}+2111.81*\\hat{temp}+5139*\\hat{atemp}+(-3149.10)*\\hat{hum}+(-4528.67)*\\hat{windspeed}+e\\)\nInferences 1. P-value caluclated - For temp is 30, we failed to reject the null in favor of alternate i.e there is NO truly relationship between temp and counts. - For atemp is 0.04, we can reject the null in favor of alternate i.e there is a truly relationship between atemp and bike counts. - For hum is 0.05, we can reject the null i.e there is a truly relationship between humidity and bike counts. - For windspeed is 0.0000, we can reject the null in favor of alternate i.e there is a truly relationship between windspeed and bike counts.\n\nTaking into account all the other explanatory variables in our model,\n\nfor every increase of one unit in temperature, there is an associated increase of on average 2111 in bike counts.\nfor every increase of one unit in atemperature, there is an associated increase of on average 5139 in bike counts.\nfor every increase of one unit in humidity, there is an associated decrease of on average 3149 in bike counts.\nfor every increase of one unit in windpspeed, there is an associated decreased of on average 4528 in bike counts.\n\nAn average of bike counts 3860 when all the exploratory variables are zero\nThe proportion of variability in the outcome variable i.e bike count explained by this model is about 0.46.\n\n\ncustom_ols_qqplot(OLS_M3_fit.resid)\n\n\n\n\n\n\n\n\n\ncustom_ols_res_vs_fitted(OLS_M3_fit.fittedvalues, OLS_M3_fit.resid)\n\n\n\n\n\n\n\n\n\ncustom_VIF(OLS_M3)\n\n\n\n\n\n\n\n\n\n\n\nCase 2: Auto\n\nData Importing, Prep and EDA\n\ndf_auto = pd.read_csv(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/w4/data/auto.csv\"\n)\n\n\ndf_auto.head()\n\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\n\n\n\n\n0\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\n\n\n1\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\n\n\n2\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\n\n\n3\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\n\n\n4\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\n\n\n\n\n\n\n\n\n\nsns.heatmap(\n    df_auto[[\"mpg\", \"horsepower\", \"weight\", \"acceleration\", \"displacement\"]].corr(),\n    annot=True,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.pairplot(df_auto[[\"mpg\", \"horsepower\", \"weight\", \"acceleration\", \"displacement\"]])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRegression Model\n\n\nExperiment 1: One Numerical: mpg=f(horsepower)\n\nSL1 = custom_statsmodel_OLS(df_auto, \"mpg\", \"horsepower\")\n\n\nSL1_modelfit = SL1.fit()\n\n\nprint(SL1_modelfit.summary2())\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.605    \nDependent Variable: mpg              AIC:                2361.3237\nDate:               2024-03-29 11:15 BIC:                2369.2662\nNo. Observations:   392              Log-Likelihood:     -1178.7  \nDf Model:           1                F-statistic:        599.7    \nDf Residuals:       390              Prob (F-statistic): 7.03e-81 \nR-squared:          0.606            Scale:              24.066   \n-------------------------------------------------------------------\n               Coef.   Std.Err.     t      P&gt;|t|    [0.025   0.975]\n-------------------------------------------------------------------\nconst         39.9359    0.7175   55.6598  0.0000  38.5252  41.3465\nhorsepower    -0.1578    0.0064  -24.4891  0.0000  -0.1705  -0.1452\n------------------------------------------------------------------\nOmnibus:               16.432       Durbin-Watson:          0.920 \nProb(Omnibus):         0.000        Jarque-Bera (JB):       17.305\nSkew:                  0.492        Prob(JB):               0.000 \nKurtosis:              3.299        Condition No.:          322   \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n\n\n\\(\\hat{mpg}={39.9359}+(-0.15)*\\hat{horsepower}+e\\)\nInferences:\n\nIn our case p-value caluclated for HorsePower is 0 hence we can reject the null in favor of alternate i.e there is a truly relationship between Horsepower and MPG.\nFor every increase of 1 unit in HorsePower, there is an associated decrease of, on average, 0.15 units of MPG.\nAn average of MPG is 39 when the Horsepower is 0\nThe proportion of variability in the outcome variable i.e bike count explained by this model is about 0.60\n\n\ncustom_ols_qqplot(SL1_modelfit.resid)\n\n\n\n\n\n\n\n\n\ncustom_ols_res_vs_fitted(SL1_modelfit.fittedvalues, SL1_modelfit.resid)\n\n\n\n\n\n\n\n\n\n\nExperiment2: two numericals: mpg=f(horsepower,acceleration)\n\nSL2 = custom_statsmodel_OLS(df_auto, \"mpg\", \"horsepower\", \"acceleration\")\n\n\nSL2_modelfit = SL2.fit()\n\n\nprint(SL2_modelfit.summary2())\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.628    \nDependent Variable: mpg              AIC:                2338.2770\nDate:               2024-03-29 11:15 BIC:                2350.1908\nNo. Observations:   392              Log-Likelihood:     -1166.1  \nDf Model:           2                F-statistic:        331.7    \nDf Residuals:       389              Prob (F-statistic): 8.67e-85 \nR-squared:          0.630            Scale:              22.635   \n-------------------------------------------------------------------\n               Coef.   Std.Err.     t      P&gt;|t|    [0.025   0.975]\n-------------------------------------------------------------------\nconst         52.5593    2.5870   20.3164  0.0000  47.4730  57.6457\nhorsepower    -0.1880    0.0086  -21.7883  0.0000  -0.2049  -0.1710\nacceleration  -0.6098    0.1204   -5.0662  0.0000  -0.8464  -0.3731\n------------------------------------------------------------------\nOmnibus:               31.573       Durbin-Watson:          0.984 \nProb(Omnibus):         0.000        Jarque-Bera (JB):       37.488\nSkew:                  0.685        Prob(JB):               0.000 \nKurtosis:              3.647        Condition No.:          1209  \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n[2] The condition number is large, 1.21e+03. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\n\\(\\hat{mpg}={52.5593}+(-0.18)*\\hat{horsepower}+(-0.60)*\\hat{acceleration}+e\\)\nInferences:\n\np-value caluclated\n\nfor HorsePower is 0 hence we can reject the null in favor of alternate i.e there is a truly relationship between Horsepower and MPG.\nfor Accelaration is 0 hence we can reject the null in favor of alternate i.e there is a truly relationship between Accelaration and MPG.\n\nFor every increase of 1 unit in HorsePower, there is an associated decrease of, on average, 0.18 units of MPG.\nFor every increase of 1 unit in Accelaration, there is an associated decrease of, on average, 0.6 units of MPG.\nAn average of MPG is 52 when the Horsepower is 0\nThe proportion of variability in the outcome variable i.e bike count explained by this model is about 0.62\n\n\ncustom_ols_qqplot(SL2_modelfit.resid)\n\n\n\n\n\n\n\n\n\ncustom_ols_res_vs_fitted(SL2_modelfit.fittedvalues, SL1_modelfit.resid)\n\n\n\n\n\n\n\n\n\ncustom_VIF(SL2)\n\n\n\n\n\n\n\n\n\n\nExperiment3: More than 2 numericals: mpg=f(“horsepower”, “weight”, “acceleration”, “displacement”)\n\nSL3 = custom_statsmodel_OLS(\n    df_auto, \"mpg\", \"horsepower\", \"weight\", \"acceleration\", \"displacement\"\n)\n\n\nSL3_modelfit = SL3.fit()\n\n\nprint(SL3_modelfit.summary2())\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.704    \nDependent Variable: mpg              AIC:                2251.1955\nDate:               2024-03-29 11:15 BIC:                2271.0518\nNo. Observations:   392              Log-Likelihood:     -1120.6  \nDf Model:           4                F-statistic:        233.4    \nDf Residuals:       387              Prob (F-statistic): 9.63e-102\nR-squared:          0.707            Scale:              18.035   \n-------------------------------------------------------------------\n                Coef.   Std.Err.     t     P&gt;|t|    [0.025   0.975]\n-------------------------------------------------------------------\nconst          45.2511    2.4560  18.4244  0.0000  40.4223  50.0800\nhorsepower     -0.0436    0.0166  -2.6312  0.0088  -0.0762  -0.0110\nweight         -0.0053    0.0008  -6.5123  0.0000  -0.0069  -0.0037\nacceleration   -0.0231    0.1256  -0.1843  0.8539  -0.2701   0.2238\ndisplacement   -0.0060    0.0067  -0.8944  0.3717  -0.0192   0.0072\n------------------------------------------------------------------\nOmnibus:               38.359       Durbin-Watson:          0.861 \nProb(Omnibus):         0.000        Jarque-Bera (JB):       51.333\nSkew:                  0.715        Prob(JB):               0.000 \nKurtosis:              4.049        Condition No.:          35594 \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n[2] The condition number is large, 3.56e+04. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\n\\(\\hat{mpg}={45.2511}+(-0.04)*\\hat{horsepower}+(0.0053)*\\hat{weight}+(0.0231)*\\hat{acceleration}+(-0.0060)*\\hat{displacement}+e\\)\nInferences:\n\np-value caluclated\n\nfor HorsePower is 0 hence we can reject the null in favor of alternate i.e there is a truly relationship between Horsepower and MPG.\nfor Weight is 0 hence we can reject the null in favor of alternate i.e there is a truly relationship between Weight and MPG.\nfor Accelaration is 0.85 we fail to reject the null i.e there is NO truly relationship between Accelaration and MPG.\nfor Displacement is 0.37 we fail to reject the null i.e there is NO truly relationship between Displacement and MPG.\n\nFor every increase of 1 unit in HorsePower, there is an associated decrease of, on average, 0.04 units of MPG.\nFor every increase of 1 unit in weight, there is an associated decrease of, on average, 0.005 units of MPG.\nAn average of MPG is 45.25 when the Horsepower is 0\nThe proportion of variability in the outcome variable i.e bike count explained by this model is about 0.704\n\n\ncustom_VIF(SL3)\n\n\n\n\n\n\n\n\n\n\n\nCase 3: CarSeats\n\nData Importing and Prep\n\ndf_carseats = pd.read_csv(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/w4/data/carseats.csv\"\n)\n\n\ndf_carseats = pd.get_dummies(\n    df_carseats, columns=[\"ShelveLoc\", \"Urban\", \"US\"], dtype=\"int\", drop_first=True\n)\n\n\ndf_carseats.head()\n\n\n\n\n\n\n\n\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nAge\nEducation\nShelveLoc_Good\nShelveLoc_Medium\nUrban_Yes\nUS_Yes\n\n\n\n\n0\n9.50\n138\n73\n11\n276\n120\n42\n17\n0\n0\n1\n1\n\n\n1\n11.22\n111\n48\n16\n260\n83\n65\n10\n1\n0\n1\n1\n\n\n2\n10.06\n113\n35\n10\n269\n80\n59\n12\n0\n1\n1\n1\n\n\n3\n7.40\n117\n100\n4\n466\n97\n55\n14\n0\n1\n1\n1\n\n\n4\n4.15\n141\n64\n3\n340\n128\n38\n13\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\nsns.heatmap(\n    df_carseats[\n        [\n            \"Sales\",\n            \"CompPrice\",\n            \"Income\",\n            \"Advertising\",\n            \"Population\",\n            \"Price\",\n            \"Age\",\n            \"Education\",\n        ]\n    ].corr(),\n    annot=True,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.pairplot(\n    df_carseats[\n        [\"Sales\", \"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\"]\n    ]\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRegression Model\n\n\nExperiment 1: More than 1 Numericals: Sales=f(“CompPrice”,“Income”,“Advertising”,“Population”,“Price”,“Age”,“Education”)\n\ncar_SL1 = custom_statsmodel_OLS(\n    df_carseats,\n    \"Sales\",\n    \"CompPrice\",\n    \"Income\",\n    \"Advertising\",\n    \"Population\",\n    \"Price\",\n    \"Age\",\n    \"Education\",\n)\n\n\ncar_SL1_fit = car_SL1.fit()\n\n\nprint(car_SL1_fit.summary2())\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.533    \nDependent Variable: Sales            AIC:                1668.6475\nDate:               2024-03-29 11:15 BIC:                1700.5792\nNo. Observations:   400              Log-Likelihood:     -826.32  \nDf Model:           7                F-statistic:        66.18    \nDf Residuals:       392              Prob (F-statistic): 1.41e-62 \nR-squared:          0.542            Scale:              3.7208   \n-------------------------------------------------------------------\n               Coef.   Std.Err.     t      P&gt;|t|    [0.025   0.975]\n-------------------------------------------------------------------\nconst          7.7077    1.1176    6.8965  0.0000   5.5104   9.9050\nCompPrice      0.0939    0.0078   11.9797  0.0000   0.0785   0.1093\nIncome         0.0129    0.0035    3.7034  0.0002   0.0060   0.0197\nAdvertising    0.1309    0.0151    8.6539  0.0000   0.1011   0.1606\nPopulation    -0.0001    0.0007   -0.1802  0.8571  -0.0015   0.0012\nPrice         -0.0925    0.0051  -18.3137  0.0000  -0.1025  -0.0826\nAge           -0.0450    0.0060   -7.4854  0.0000  -0.0568  -0.0332\nEducation     -0.0400    0.0371   -1.0770  0.2821  -0.1130   0.0330\n------------------------------------------------------------------\nOmnibus:               8.263        Durbin-Watson:           1.969\nProb(Omnibus):         0.016        Jarque-Bera (JB):        7.705\nSkew:                  0.288        Prob(JB):                0.021\nKurtosis:              2.639        Condition No.:           4049 \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n[2] The condition number is large, 4.05e+03. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\nInferences 1. P-value caluclated - For CompPrice is 0.00, we can reject the null in favor of alternate i.e there is a truly relationship between ComPrice and Sales. - For Income is 0.00, we can reject the null in favor of alternate i.e there is a truly relationship between Income and Sales. - For Advertising is 0.0002, we can reject the null i.e there is a truly relationship between Advertising and Sales. - For Price is 0.00, we can reject the null i.e there is a truly relationship between Price and Sales. - For Age is 0.00, we can reject the null i.e there is a truly relationship between Age and Sales. - For Education is 0.28, we fail to reject the null i.e there is NO truly relationship between Education and Sales. - For Population is 0.85, we fail to reject the null i.e there is NO truly relationship between Population and Sales.\n\nTaking into account all the other explanatory variables in our model,\n\nfor every increase of one unit in CompPrice, there is an associated increase of on average 0.09 in Sales\nfor every increase of one unit in Income, there is an associated increase of on average 0.01 in Sales\nfor every increase of one unit in Advertising, there is an associated increase of on average 0.13 in Sales.\nfor every increase of one unit in Price, there is an associated decreased of on average 0.09 in Sales.\nfor every increase of one unit in Age, there is an associated decreased of on average 0.04 in Sales.\n\nAn average of Sales 7.7 when all the exploratory variables are zero\nThe proportion of variability in the outcome variable i.e bike count explained by this model is about 0.53\n\n\ncustom_VIF(car_SL1)\n\n\n\n\n\n\n\n\n\ncustom_ols_qqplot(car_SL1_fit.resid)\n\n\n\n\n\n\n\n\n\ncustom_ols_res_vs_fitted(car_SL1_fit.fittedvalues, car_SL1_fit.resid)\n\n\n\n\n\n\n\n\n\n\nExperiment 2: More than 1 Numericals and Categorical: Sales=f(“CompPrice”,“Income”,“Advertising”,“Population”,“Price”,“Age”,“Education”,“ShelveLoc_Good”,“ShelveLoc_Medium”,“Urban_Yes”,“US_Yes”)\n\ncar_SL2 = custom_statsmodel_OLS(\n    df_carseats,\n    \"Sales\",\n    \"CompPrice\",\n    \"Income\",\n    \"Advertising\",\n    \"Population\",\n    \"Price\",\n    \"Age\",\n    \"Education\",\n    \"ShelveLoc_Good\",\n    \"ShelveLoc_Medium\",\n    \"Urban_Yes\",\n    \"US_Yes\",\n)\n\n\ncar_SL2_fit = car_SL2.fit()\n\n\nprint(car_SL2_fit.summary2())\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.870    \nDependent Variable: Sales            AIC:                1161.9744\nDate:               2024-03-29 11:15 BIC:                1209.8719\nNo. Observations:   400              Log-Likelihood:     -568.99  \nDf Model:           11               F-statistic:        243.4    \nDf Residuals:       388              Prob (F-statistic): 1.60e-166\nR-squared:          0.873            Scale:              1.0382   \n------------------------------------------------------------------\n                   Coef.  Std.Err.    t     P&gt;|t|   [0.025  0.975]\n------------------------------------------------------------------\nconst              5.6606   0.6034   9.3805 0.0000  4.4742  6.8471\nCompPrice          0.0928   0.0041  22.3778 0.0000  0.0847  0.1010\nIncome             0.0158   0.0018   8.5647 0.0000  0.0122  0.0194\nAdvertising        0.1231   0.0111  11.0660 0.0000  0.1012  0.1450\nPopulation         0.0002   0.0004   0.5611 0.5750 -0.0005  0.0009\nPrice             -0.0954   0.0027 -35.7002 0.0000 -0.1006 -0.0901\nAge               -0.0460   0.0032 -14.4718 0.0000 -0.0523 -0.0398\nEducation         -0.0211   0.0197  -1.0700 0.2853 -0.0599  0.0177\nShelveLoc_Good     4.8502   0.1531  31.6778 0.0000  4.5492  5.1512\nShelveLoc_Medium   1.9567   0.1261  15.5165 0.0000  1.7088  2.2047\nUrban_Yes          0.1229   0.1130   1.0877 0.2774 -0.0992  0.3450\nUS_Yes            -0.1841   0.1498  -1.2286 0.2200 -0.4787  0.1105\n------------------------------------------------------------------\nOmnibus:               0.811        Durbin-Watson:           2.013\nProb(Omnibus):         0.667        Jarque-Bera (JB):        0.765\nSkew:                  0.107        Prob(JB):                0.682\nKurtosis:              2.994        Condition No.:           4146 \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n[2] The condition number is large, 4.15e+03. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\nInferences 1. P-value caluclated - For CompPrice is 0.00, we can reject the null in favor of alternate i.e there is a truly relationship between ComPrice and Sales. - For Income is 0.00, we can reject the null in favor of alternate i.e there is a truly relationship between Income and Sales. - For Advertising is 0.00, we can reject the null i.e there is a truly relationship between Advertising and Sales. - For Price is 0.00, we can reject the null i.e there is a truly relationship between Price and Sales. - For Age is 0.00, we can reject the null i.e there is a truly relationship between Age and Sales. - For Education is 0.28, we fail to reject the null i.e there is NO truly relationship between Education and Sales. - For Population is 0.85, we fail to reject the null i.e there is NO truly relationship between Population and Sales. - For Uran Yes, we fail to reject the null i.e there is NO truly relationship between Urban and Sales. - For US Yes, we fail to reject the null i.e there is NO truly relationship between US and Sales.\n\nTaking into account all the other explanatory variables in our model,\n\nfor every increase of one unit in CompPrice, there is an associated increase of on average 0.09 in Sales\nfor every increase of one unit in Income, there is an associated increase of on average 0.01 in Sales\nfor every increase of one unit in Advertising, there is an associated increase of on average 0.12 in Sales.\nfor every increase of one unit in Price, there is an associated decreased of on average 0.09 in Sales.\nfor every increase of one unit in Age, there is an associated decreased of on average 0.04 in Sales.\nFor ShelveLoc_Good the average number of Sales 4.85 units higher on average compared to the ShelveLoc_Bad\nFor ShelveLoc_Medium the average number of Sales 1.95 units higher on average compared to the ShelveLoc_Bad\n\nAn average of Sales 5.6 when all the exploratory variables are zero\nThe proportion of variability in the outcome variable i.e bike count explained by this model is about 0.87\n\n\ncustom_VIF(car_SL2)\n\n\n\n\n\n\n\n\n\n\n\nCase 4: BodyFat\n\nData Importing and Prep\n\ndf_bodyfat = pd.read_csv(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/assignments/w4/data/bodyfat.csv\"\n)\n\n\ndf_bodyfat.head()\n\n\n\n\n\n\n\n\n\nDensity\nAge\nWeight\nHeight\nADI\nFFW\nNeck\nChest\nADC\nHip\nThigh\nKnee\nAnkle\nEBC\nFC\nWC\nBody Fat\n\n\n\n\n0\n1.0708\n23\n154.25\n67.75\n23.7\n134.9\n36.2\n93.1\n85.2\n94.5\n59.0\n37.3\n21.9\n32.0\n27.4\n17.1\n12.6\n\n\n1\n1.0853\n22\n173.25\n72.25\n23.4\n161.3\n38.5\n93.6\n83.0\n98.7\n58.7\n37.3\n23.4\n30.5\n28.9\n18.2\n6.9\n\n\n2\n1.0414\n22\n154.00\n66.25\n24.7\n116.0\n34.0\n95.8\n87.9\n99.2\n59.6\n38.9\n24.0\n28.8\n25.2\n16.6\n24.6\n\n\n3\n1.0751\n26\n184.75\n72.25\n24.9\n164.7\n37.4\n101.8\n86.4\n101.2\n60.1\n37.3\n22.8\n32.4\n29.4\n18.2\n10.9\n\n\n4\n1.0340\n24\n184.25\n71.25\n25.6\n133.1\n34.4\n97.3\n100.0\n101.9\n63.2\n42.2\n24.0\n32.2\n27.7\n17.7\n27.8\n\n\n\n\n\n\n\n\n\nsns.heatmap(df_bodyfat.corr(), annot=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRegression Model\n\n\nExperiment 1:\n\nbody_fat_SL = custom_statsmodel_OLS(\n    df_bodyfat,\n    \"Body Fat\",\n    \"Density\",\n    \"Age\",\n    \"Weight\",\n    \"Height\",\n    \"ADI\",\n    \"FFW\",\n    \"Neck\",\n    \"Chest\",\n    \"ADC\",\n    \"Hip\",\n    \"Thigh\",\n    \"Knee\",\n    \"Ankle\",\n    \"EBC\",\n    \"FC\",\n    \"WC\",\n)\n\n\nbody_fat_SL_fit = body_fat_SL.fit()\n\n\nprint(body_fat_SL_fit.summary2())\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.986    \nDependent Variable: Body Fat         AIC:                680.7642 \nDate:               2024-03-29 11:15 BIC:                740.7645 \nNo. Observations:   252              Log-Likelihood:     -323.38  \nDf Model:           16               F-statistic:        1138.    \nDf Residuals:       235              Prob (F-statistic): 1.62e-212\nR-squared:          0.987            Scale:              0.81750  \n------------------------------------------------------------------\n              Coef.   Std.Err.    t     P&gt;|t|    [0.025    0.975] \n------------------------------------------------------------------\nconst        253.2587  15.2445  16.6132 0.0000  223.2255  283.2920\nDensity     -234.0972  13.1352 -17.8221 0.0000 -259.9750 -208.2193\nAge            0.0057   0.0068   0.8377 0.4030   -0.0077    0.0192\nWeight         0.1594   0.0164   9.7451 0.0000    0.1272    0.1916\nHeight         0.0125   0.0235   0.5314 0.5956   -0.0339    0.0589\nADI           -0.2339   0.0645  -3.6260 0.0004   -0.3610   -0.1068\nFFW           -0.2301   0.0183 -12.5450 0.0000   -0.2662   -0.1939\nNeck           0.0199   0.0499   0.3990 0.6903   -0.0785    0.1183\nChest          0.0688   0.0224   3.0749 0.0024    0.0247    0.1128\nADC            0.0238   0.0234   1.0183 0.3096   -0.0223    0.0699\nHip            0.0191   0.0312   0.6125 0.5408   -0.0424    0.0806\nThigh          0.0691   0.0313   2.2055 0.0284    0.0074    0.1309\nKnee           0.0116   0.0522   0.2230 0.8237   -0.0911    0.1144\nAnkle          0.0033   0.0475   0.0705 0.9438   -0.0902    0.0969\nEBC           -0.0030   0.0367  -0.0805 0.9359   -0.0752    0.0693\nFC             0.0987   0.0425   2.3232 0.0210    0.0150    0.1824\nWC             0.1632   0.1152   1.4165 0.1580   -0.0638    0.3902\n------------------------------------------------------------------\nOmnibus:             249.344      Durbin-Watson:         1.872    \nProb(Omnibus):       0.000        Jarque-Bera (JB):      25144.989\nSkew:                3.415        Prob(JB):              0.000    \nKurtosis:            51.457       Condition No.:         110523   \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n[2] The condition number is large, 1.11e+05. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\nInferences 1. P-value caluclated - For Density is 0.00, we can reject the null in favor of alternate i.e there is a truly relationship between Density and Body Fat - For Weight is 0.00, we can reject the null in favor of alternate i.e there is a truly relationship between Weight and Body Fat - For ADI is 0.00, we can reject the null in favor of alternate i.e there is a truly relationship between ADI and Body Fat - For Chest is 0.00, we can reject the null in favor of alternate i.e there is a truly relationship between Chest and Body Fat - For Density is 0.00, we can reject the null in favor of alternate i.e there is a truly relationship between Density and Body Fat - For Thigh is 0.02, we can reject the null in favor of alternate i.e there is a truly relationship between Thigh and Body Fat - For the independent variables: Age,Height,Neck,ADC,Hip,Knee,Ankle,EBC,and WC are having more p-value i.e &gt;0.05, hence we failed to raject the null i.e there is NO true relationship between these variabels and body fat\n\nTaking into account all the other explanatory variables in our model,\n\nfor every increase of one unit in Weight, there is an associated increase of on average 0.15 in Body Fat\n\nAn average of Body Fat 253 when all the exploratory variables are zero\nThe proportion of variability in the outcome variable i.e bike count explained by this model is about 0.98\n\n\ncustom_VIF(body_fat_SL)\n\n\n\n\n\n\n\n\n\ndf_bodyfat_sel = df_bodyfat[\n    [\"Knee\", \"Neck\", \"EBC\", \"WC\", \"Age\", \"Height\", \"FC\", \"Ankle\", \"Body Fat\"]\n]\n\n\nbody_fat_SL_2 = custom_statsmodel_OLS(\n    df_bodyfat_sel,\n    \"Body Fat\",\n    \"Knee\",\n    \"Neck\",\n    \"EBC\",\n    \"WC\",\n    \"Age\",\n    \"Height\",\n    \"FC\",\n    \"Ankle\",\n)\n\n\nbody_fat_SL_2_fit = body_fat_SL_2.fit()\n\n\nprint(body_fat_SL_2_fit.summary2())\n\n                 Results: Ordinary least squares\n==================================================================\nModel:              OLS              Adj. R-squared:     0.442    \nDependent Variable: Body Fat         AIC:                1609.1443\nDate:               2024-03-29 11:15 BIC:                1640.9092\nNo. Observations:   252              Log-Likelihood:     -795.57  \nDf Model:           8                F-statistic:        25.83    \nDf Residuals:       243              Prob (F-statistic): 1.05e-28 \nR-squared:          0.460            Scale:              33.534   \n--------------------------------------------------------------------\n          Coef.     Std.Err.      t      P&gt;|t|     [0.025     0.975]\n--------------------------------------------------------------------\nconst    -23.6281     9.1181   -2.5913   0.0101   -41.5887   -5.6674\nKnee       1.1799     0.2454    4.8071   0.0000     0.6964    1.6634\nNeck       0.7565     0.2681    2.8217   0.0052     0.2284    1.2846\nEBC        0.6198     0.2074    2.9889   0.0031     0.2113    1.0282\nWC        -2.4084     0.7041   -3.4207   0.0007    -3.7953   -1.0215\nAge        0.1878     0.0331    5.6709   0.0000     0.1226    0.2531\nHeight    -0.3859     0.1099   -3.5119   0.0005    -0.6023   -0.1694\nFC         0.2852     0.2632    1.0835   0.2797    -0.2333    0.8037\nAnkle      0.1151     0.2918    0.3944   0.6936    -0.4596    0.6898\n------------------------------------------------------------------\nOmnibus:               0.516        Durbin-Watson:           1.798\nProb(Omnibus):         0.773        Jarque-Bera (JB):        0.553\nSkew:                  0.108        Prob(JB):                0.758\nKurtosis:              2.921        Condition No.:           2811 \n==================================================================\nNotes:\n[1] Standard Errors assume that the covariance matrix of the\nerrors is correctly specified.\n[2] The condition number is large, 2.81e+03. This might indicate\nthat there are strong multicollinearity or other numerical\nproblems.\n\n\n\ncustom_VIF(body_fat_SL_2)"
  },
  {
    "objectID": "SSBBA_W2.html",
    "href": "SSBBA_W2.html",
    "title": "Process Stability and Capability Analysis",
    "section": "",
    "text": "0. Libraris and SetUp\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, poisson, ttest_1samp\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")\n\n\n\n1. Custom Functions - Charts and SixSigma Calucator\n\ndef calculate_center_line(data, column_name):\n    \"\"\"Calculates the center line for the control chart.\"\"\"\n    return data[column_name].mean()\n\n\ndef calculate_control_limits(center, average_defects_per_unit, multiplier):\n    \"\"\"Calculates upper and lower control limits.\"\"\"\n    std_dev = np.sqrt(average_defects_per_unit)\n    upper_limit = center + multiplier * std_dev\n    lower_limit = center - multiplier * std_dev\n    return upper_limit, lower_limit\n\n\ndef create_c_chart(data):\n    \"\"\"Creates a C chart for the given data.\"\"\"\n    center_line = calculate_center_line(data, \"no_of_resources\")\n    control_limits = calculate_control_limits(\n        center_line, center_line, 3\n    )  # 3 sigma limits\n    plt.figure(figsize=(10, 6))\n    plt.plot(data[\"row_nr\"], data[\"no_of_resources\"], \"o-\", label=\"no_of_resources\")\n    plt.axhline(y=center_line, color=\"r\", linestyle=\"--\", label=\"Center Line\")\n    plt.axhline(y=control_limits[0], color=\"b\", linestyle=\"--\", label=\"UCL\")\n    plt.axhline(y=control_limits[1], color=\"b\", linestyle=\"--\", label=\"LCL\")\n    plt.xlabel(\"Project\")\n    plt.ylabel(\"Number of Resources\")\n    plt.title(\"C Chart\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\ndef create_run_chart(data, column_name):\n    \"\"\"Creates a run chart for the given data.\"\"\"\n    plt.figure(figsize=(25, 6))\n    plt.plot(data.index, data[column_name], marker=\"o\", linestyle=\"-\")\n    plt.xlabel(\n        \"Projects-From Jan to Dec-2023\"\n    )  # Assuming the index represents time periods\n    plt.ylabel(column_name)  # Replace with the actual column name\n    plt.title(\"Run Chart for \" + column_name)\n    plt.grid(True)\n    plt.show()\n\n\ndef get_six_sigma_caluclator():\n    \"\"\"ISI Custom SixSigma Calucator\"\"\"\n    while True:\n        ### Inputs\n        print(f\"-------------------------------------------------\")\n        print(f\"############ Sigma Caluclator Inputs ############\")\n        print(f\"-------------------------------------------------\")\n        _mean = float(input(\"Enter the mean:\"))\n        _sd = float(input(\"Enter Standard Deviation:\"))\n        _LSL = float(input(\"Enter LSL:\"))\n        _USL = float(input(\"Enter USL:\"))\n        # Formulas and caluclations\n        ZLSL = (_LSL - _mean) / _sd\n        ZUSL = (_USL - _mean) / _sd\n        Area_ZLSL = norm.cdf(ZLSL)\n        Area_ZUSL = 1 - norm.cdf(ZUSL)\n        TOTAL_NC = Area_ZLSL + Area_ZUSL\n        YIELD = 1 - TOTAL_NC\n        CP_ = (_USL - _LSL) / (6 * _sd)\n        _A = (_USL - _mean) / (3 * _sd)\n        _B = (_mean - _LSL) / (3 * _sd)\n        CPK_ = min(_A, _B)\n        SIGMA_LEVEL = round(1.5 + norm.ppf(YIELD), 5)\n        DPMO = TOTAL_NC * 1000000\n        # Output\n        print(f\"-------------------------------------------------\")\n        print(f\"#### Summary Report ####\")\n        print(f\"-------------------------------------------------\")\n        print(f\"Total NonConfirmances:{round(TOTAL_NC,5)}\")\n        print(f\"Yield:{round(YIELD,5)}\")\n        print(f\"CP:{round(CP_,5)}\")\n        print(f\"CPK:{round(CPK_,5)}\")\n        print(f\"SIGMA_LEVEL:{round(SIGMA_LEVEL,5)}\")\n        print(f\"DPMO:{round(DPMO,5)}\")\n        print(f\"-------------------------------------------------\")\n        _next = input(\n            \"Would you like to continue to use sigma caluclator type 'yes' if so :\"\n        )\n        if _next.lower() == \"yes\":\n            continue\n        else:\n            print(f\"Thanks for using Sigma Caluclator..\")\n            print(f\"#### END ####\")\n            break\n\n\n\n2. Data Imports and Preparations\n\ndf_track = pl.read_excel(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/data_training_1/data/DBAS_SDS_Projects_2023_V2.xlsx\"\n)\n\ndf_track_tidy = (\n    df_track.with_columns(\n        pl.col(\"Team Member Name\").str.split(\",\").list.len().alias(\"no_of_resources\")\n    )\n    .sort(\"Started Date\")\n    .with_row_count()\n)\n\ndf_track_tidy_numericals = df_track_tidy.select(\n    [\n        \"row_nr\",\n        \"NO_OF_DAYS_NORM\",\n        \"NO_OF_SERVICE_WORKING_HRS\",\n        \"Total Files\",\n        \"no_of_resources\",\n    ]\n)\n\nOur Data Breach Discovery Analytics process leverages data mining techniques to automatically extract PII/PHI information from various file systems. While automation streamlines the process, a team of analysts still reviews a subset of 10-20% of files manually for added assurance.\nTo ensure transparency and tracking, we do record details for each project undergoing manual review. This record includes project name, number of files reviewed, start and end dates in a clear format, and the number of analysts involved.\nNote: As part of the assignment i have considered this real data to perform the stated analysis. Course Project that i’m planning to carry out would deal with another targer variable i.e DOCUMENT CLASSIFICATION/DEDUPLICATION,This future analysis could potentially incorporate the findings from the current assignment as well.\n\ndf_track_tidy.head()\n\n\n\nshape: (5, 10)\n\n\n\nrow_nr\nProject Name\nTotal Files\nStarted Date\nComplete Date\nTeam Member Name\nNO_OF_DAYS\nNO_OF_DAYS_NORM\nNO_OF_SERVICE_WORKING_HRS\nno_of_resources\n\n\nu32\nstr\ni64\nstr\nstr\nstr\ni64\ni64\ni64\nu32\n\n\n\n\n0\n\"PROJECT_1\"\n15\n\"01-03-23\"\n\"01-04-23\"\n\"Karthik\"\n1\n2\n16\n1\n\n\n1\n\"PROJECT_2\"\n2437\n\"01-09-23\"\n\"01-12-23\"\n\"Anitha,Saikris…\n3\n4\n32\n11\n\n\n2\n\"PROJECT_4\"\n9\n\"01-09-23\"\n\"01-09-23\"\n\"Naresh K,Sneh,…\n0\n1\n8\n4\n\n\n3\n\"PROJECT_5\"\n14\n\"01-10-23\"\n\"01-10-23\"\n\"Anitha\"\n0\n1\n8\n1\n\n\n4\n\"PROJECT_6\"\n26\n\"01-12-23\"\n\"01-13-23\"\n\"Anusha,Saikris…\n1\n2\n16\n6\n\n\n\n\n\n\n\n\n\n3. Process Stability and Capability Analysis\nThe manual document review process makes it difficult to determine the optimal number of analysts needed for each project, leading to delays in project delivery. If we can streamline the process of assigning analysts to projects, we can optimize resource allocation and ensure timely data notification delivery.\nWe have collected the last one year’s 260 projects data as showed about and out target variable is no_of_resources are used for each of the project\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.boxplot(df_track_tidy_numericals.select(\"no_of_resources\").to_pandas(), ax=ax1)\np2 = sns.histplot(\n    df_track_tidy_numericals.select(\"no_of_resources\").to_pandas(), ax=ax2, kde=True\n)\np1.set_title(\"BoxPlot: No of resources-Assignable + Random\")\np2.set_title(\"Histogram: No of resources-Assignable + Random\")\nplt.suptitle(\"Stability Analysis - No of Resources Required for manual document review\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNote: 1. The number of resources required per project exhibits a Poisson distribution, with an average of 3 analysts assigned per project.\n\ncreate_run_chart(\n    df_track_tidy_numericals.select(\"no_of_resources\").to_pandas(), \"no_of_resources\"\n)\n\n\n\n\n\n\n\n\n\ncreate_c_chart(\n    df_track_tidy_numericals.select(\"no_of_resources\").with_row_count().to_pandas()\n)\n\n\n\n\n\n\n\n\nNote:\n\nWe identified a significant number (approximately 218) of assignable causes in the process. These warrant further investigation to understand their root causes and implement corrective actions.\n\n\n3.1 Investigation on Assignable causes\n\ndf_track_random = (\n    df_track_tidy.select(pl.col([\"row_nr\", \"no_of_resources\"]))\n    .filter((pl.col(\"no_of_resources\") &lt; 7) & (pl.col(\"no_of_resources\") &gt; 3))\n    .with_columns(pl.lit(\"RANDOM\").alias(\"Type\"))\n)\n\n\ndf_track_tidy_v2 = df_track_tidy.join(\n    df_track_random.select([\"row_nr\", \"Type\"]), on=\"row_nr\", how=\"left\"\n).with_columns(\n    pl.when(pl.col(\"Type\").is_null())\n    .then(pl.lit(\"Assignable\").alias(\"Type\"))\n    .otherwise(pl.col(\"Type\"))\n)\n\n\ndf_assignable_counts = (\n    df_track_tidy_v2.select([\"Type\", \"no_of_resources\"])\n    .filter(pl.col(\"Type\") == \"Assignable\")\n    .select(\"no_of_resources\")\n    .sort(\"no_of_resources\")\n    .with_columns(pl.col(\"no_of_resources\").cast(pl.Utf8))\n    .group_by(\"no_of_resources\")\n    .count()\n    .to_pandas()\n    .sort_values(\"no_of_resources\")\n)\n\n\npl.from_pandas(df_assignable_counts).with_columns(\n    pl.sum(\"count\").alias(\"sum_of_asg\")\n).with_columns((pl.col(\"count\") / pl.col(\"sum_of_asg\")).alias(\"perc\"))\n\n\n\nshape: (13, 4)\n\n\n\nno_of_resources\ncount\nsum_of_asg\nperc\n\n\nstr\nu32\nu32\nf64\n\n\n\n\n\"1\"\n116\n218\n0.53211\n\n\n\"10\"\n2\n218\n0.009174\n\n\n\"11\"\n4\n218\n0.018349\n\n\n\"12\"\n1\n218\n0.004587\n\n\n\"13\"\n3\n218\n0.013761\n\n\n…\n…\n…\n…\n\n\n\"2\"\n42\n218\n0.192661\n\n\n\"3\"\n28\n218\n0.12844\n\n\n\"7\"\n8\n218\n0.036697\n\n\n\"8\"\n7\n218\n0.03211\n\n\n\"9\"\n2\n218\n0.009174\n\n\n\n\n\n\n\n\nsns.barplot(df_assignable_counts, y=\"no_of_resources\", x=\"count\")\nplt.show()\n\n\n\n\n\n\n\n\nNotes: 1. No of Data Points considered as Assignable causes ~ 218\n\nOut of 218 total assignable causes, approximately 116 are assignable to projects requiring only one person,This suggests that 85% of assignable causes come from projects with a small resource requirement of 1, 2, or 3 people.\nHigh-priority projects typically necessitate a larger team of analysts for successful delivery. This is reflected by the increased number of resources required, often ranging from 11 to 19 analysts.\nWe have done an investigation on these 218 data points and tag them as ASSINGNABLE and the remaining are as RANDOM once.\nWe would now conduct a stability analysis on thes RANDOM chance data points\n\n\ndf_random_resources = df_track_tidy_v2.filter(pl.col(\"Type\") == \"RANDOM\").select(\n    \"no_of_resources\"\n)\n\n\n\n3.2 Hypothesis One Mean\n\nHo: Average number of resource persons for manual review 5\n\n\nHA: Average number of resource persons for manual review not equal to 5\n\ninfer_v1 = ttest_1samp(\n    df_random_resources.select(\"no_of_resources\").to_numpy(), popmean=5\n)\n_PVAL = infer_v1.pvalue[0]\n_L, _R = (\n    infer_v1.confidence_interval(0.95)[0][0],\n    infer_v1.confidence_interval(0.95)[1][0],\n)\nprint(\n    f\"P-Value Caluclated is:{round(_PVAL,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis.\"\n)\nprint(\n    f\"We are 95% confident the true mean no of resources require for manual document review: is between {round(_L,3)} and {round(_R,3)}\"\n)\n\nP-Value Caluclated is:0.095 which is greater than to 0.05, hence we failed to reject the null hypothesis.\nWe are 95% confident the true mean no of resources require for manual document review: is between 4.532 and 5.039\n\n\n\n\n\n3.3 Control Charts- Run Chart and C Chart\n\ncreate_run_chart(df_random_resources.to_pandas(), \"no_of_resources\")\n\n\n\n\n\n\n\n\n\ncreate_c_chart(df_random_resources.with_row_count().to_pandas())\n\n\n\n\n\n\n\n\n\n\n3.4 Box Plot and Histogram - Random Variability\n\n# Create a figure and subplots\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\np1 = sns.boxplot(\n    df_random_resources.select(\"no_of_resources\").to_pandas(), ax=axes[0, 0]\n)\np2 = sns.histplot(\n    df_random_resources.select(\"no_of_resources\").to_pandas(),\n    ax=axes[0, 1],\n    kde=True,\n    bins=3,\n)\np5 = sns.histplot(norm.rvs(4.78, round(np.sqrt(0.8), 3), 1000), ax=axes[1, 1])\np1.set_title(\"BoxPlot: No of resources-Random\")\np2.set_title(\"Histogram: No of resources-Random\")\np5.set_title(\"Poisson to Normal: Mean: (Lambda) and SD: squar root of Lambda\")\nplt.suptitle(\"Stability Analysis - No of Resources Required for manual document review\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotes: 1. After identifying and removing assignable causes, we re-analyzed the process to assess the stability of remaining random variations in the number of resources required for statistically stable manual review. This analysis will help us determine the optimal resource allocation for future projects.\n\n\n\n4. SIGMA CALUCLATOR\n1. Mean: Number of Resource Persons Utilized for Manual Review ~ 5\n2. SD:   Standard Deviation ~ 0.8\n3. Business Specifications:\n     - LSL: ~ 2\n     - USL: ~ 8\n\nget_six_sigma_caluclator()\n\n-------------------------------------------------\n############ Sigma Caluclator Inputs ############\n-------------------------------------------------\nEnter the mean: 5\nEnter Standard Deviation: 0.8\nEnter LSL: 2\nEnter USL: 8\n-------------------------------------------------\n#### Summary Report ####\n-------------------------------------------------\nTotal NonConfirmances:0.00018\nYield:0.99982\nCP:1.25\nCPK:1.25\nSIGMA_LEVEL:5.07244\nDPMO:176.83457\n-------------------------------------------------\nWould you like to continue to use sigma caluclator type 'yes' if so : no\nThanks for using Sigma Caluclator..\n#### END ####\n\n\nNote 1. The process capability index (Cp) of 1.25 indicates low variability. While this means all current business requirements are likely being met. 2. The process achieved index(Cpk) of 1.25 is also same as Cp,it suggests the process mean is likely centered between the upper and lower specification limits.\n\nsns.histplot(norm.rvs(5, 0.8, 20000),kde=True)\nplt.axvline(5,color='black')\nplt.axvline(2,color='red')\nplt.axvline(8,color='red')\nplt.show()"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "SSBBA_W3.html",
    "href": "SSBBA_W3.html",
    "title": "Statistical Inferences",
    "section": "",
    "text": "import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nfrom scipy.stats import (\n    mannwhitneyu,\n    norm,\n    poisson,\n    ttest_1samp,\n    ttest_ind,\n    ttest_rel,\n    wilcoxon,\n)\nfrom statsmodels.stats.proportion import proportions_ztest\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nTo protect the privacy, we take steps to ensure the accuracy of our data breach notification lists. Before creating these lists, we identify and cluster duplicate records using advanced algorithms, including rule-based and machine learning techniques. This process significantly reduces the amount of data we handle while preserving all relevant information.\nWe track data processed for each project, including:\n  1. Number of records extracted and consolidated\n  2. Number of duplicate records identified\n  3. Number of duplicates manually reviewed by analysts for additional quality assurance\n  4. % of duplication\nThese information can be seen in the below pandas dataframe\n\ndf_delivery = pd.read_excel(r\"C:\\datos\\SSBB\\DATA\\dbas_sds_project_delivery_v1.xlsx\")\n\n\ndf_delivery.head()\n\n\n\n\n\n\n\n\n\nproject_name\nbefore_rollup\nafter_rollup\nmanual_checks\ndiff\nduplication_perc\n\n\n\n\n0\nPROJECT_1\n1211\n184\n250\n66.000000\n84.805945\n\n\n1\nPROJECT_2\n170210\n20808\n29568\n8760.000000\n87.775101\n\n\n2\nPROJECT_3\n40713\n27629\n16856\n61.008361\n32.137155\n\n\n3\nPROJECT_4\n53038\n6396\n9157\n2761.000000\n87.940722\n\n\n4\nPROJECT_5\n1318\n169\n389\n220.000000\n87.177542\n\n\n\n\n\n\n\n\nWe want to assess data deduplication efficiency for our algorithms. We’ll analyze 90 projects from the past 2 months to determine:\n    1. Duplication rate per project\n    2. Algorithm performance\n    3. Post-processing manual effort required\n\n\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.boxplot(df_delivery[\"duplication_perc\"], ax=ax1)\np2 = sns.histplot(df_delivery[\"duplication_perc\"], ax=ax2, kde=True, bins=16)\np1.set_title(\"BoxPlot: What percent of Data Duplication existed in each project?\")\np2.set_title(\"Histogram:  What percent of Data Duplication existed in each project?\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nThe distribution of data duplication rates exhibits a left skew.\nAs our data exhibits a non-normal distribution, a non-parametric test is more appropriate for this scenario to avoid potential biases from assuming normality.\n\nHypothesis\n        Ho: Half of the project data has a deduplication value of 75% \n        Ha: Half of the project data does not have a deduplication value of 75%\n\ndedup_perc = df_delivery[\"duplication_perc\"]\n\n_stat, _p = wilcoxon(dedup_perc - 75)\n\n\nprint(\n    f\"P-Value Caluclated is:{round(_p,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.607 which is greater than to 0.05, hence we failed to reject the null hypothesis.\n\n\n\n\n\nWe want to investigate whether the percentage of duplicate data is correlated with project size. Specifically, we aim to determine if:\n\nSmaller projects tend to have a higher duplication rate.\nLarger projects tend to have a lower duplication rate. (or vice versa)\n\nWe have introduced a new variable named “record group” to facilitate analysis. This variable groups projects according to the size of their consolidated data set (number of data points).\n\n# Creatinng record group column with different conditions\ndf_delivery.loc[\n    ((df_delivery[\"before_rollup\"] &gt;= 1) & (df_delivery[\"before_rollup\"] &lt; 1500)),\n    \"record_group\",\n] = \"1-1499\"\n\ndf_delivery.loc[\n    ((df_delivery[\"before_rollup\"] &gt;= 1500) & (df_delivery[\"before_rollup\"] &lt; 18000)),\n    \"record_group\",\n] = \"1500-17999\"\n\ndf_delivery.loc[\n    ((df_delivery[\"before_rollup\"] &gt;= 18000) & (df_delivery[\"before_rollup\"] &lt; 120000)),\n    \"record_group\",\n] = \"18000-119999\"\n\ndf_delivery.loc[\n    (\n        (df_delivery[\"before_rollup\"] &gt;= 120000)\n        & (df_delivery[\"before_rollup\"] &lt; 1000000)\n    ),\n    \"record_group\",\n] = \"120000-999999\"\n\ndf_delivery.loc[(df_delivery[\"before_rollup\"] &gt;= 1000000), \"record_group\"] = \"1M\"\n\n\ndf_record_group = df_delivery.value_counts(\"record_group\").reset_index()\n\ndf_record_group[\"total\"] = sum(df_record_group[\"count\"])\n\ndf_record_group[\"perc_of\"] = df_record_group[\"count\"] / df_record_group[\"total\"]\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.barplot(df_record_group, y=\"record_group\", x=\"count\", ax=ax1)\np2 = sns.barplot(df_record_group, x=\"record_group\", y=\"perc_of\", ax=ax2)\np1.set_title(\"BarChart: Number of projects per each record group?\")\np2.set_title(\"BarChart: (%) of projects per each record group?\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_record_A = df_delivery.loc[\n    df_delivery[\"record_group\"].isin([\"1-1499\", \"1500-17999\"]), [\"duplication_perc\"]\n]\n\ndf_record_B = df_delivery.loc[\n    df_delivery[\"record_group\"].isin([\"18000-119999\", \"120000-999999\"]),\n    [\"duplication_perc\"],\n]\n\nWe previously defined two record groups: Group A includes data observations ranging from 1 to 17.9 thousand, and Group B includes observations from 18 thousand to 100 thousand.\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_record_A[\"duplication_perc\"], ax=ax1, kde=True)\np2 = sns.histplot(df_record_B[\"duplication_perc\"], ax=ax2, kde=True)\np1.set_title(\"Histogram:Record Group-A\")\np2.set_title(\"Histogram:Record Group-B\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nBoth record groups show a left-skewed distribution in the data.\nSince the data in both record groups exhibits a non-normal distribution, a non-parametric test like the Mann-Whitney U test is more appropriate.\n\nHypothesis\n        Ho: On average, the amount of data duplication is the same for Record Group A and Record Group B.\n        Ha: On average, the amount of data duplication is different for Record Group A and Record Group B.\n\n_stat, _pval = mannwhitneyu(df_record_A, df_record_B)\n\nprint(\n    f\"P-Value Caluclated is:{round(_pval[0],3)} which is lesser than to 0.05, hence we can reject the null hypothesis in favour of alternative hypothesis\"\n)\n\nP-Value Caluclated is:0.001 which is lesser than to 0.05, hence we can reject the null hypothesis in favour of alternative hypothesis\n\n\n\n\n\nHalf of the project data has a deduplication value of 75%\nOn average, the amount of data duplication is different for Record Group A and Record Group B.\n\nAnalysis of data deduplication revealed that approximately half the dataset exhibits a 75% deduplication rate. However, the level of duplication appears to vary significantly between Record Group A and Record Group B, suggesting potential structural differences in the data within these groups."
  },
  {
    "objectID": "SSBBA_W3.html#a.-identifying-data-duplication-rates-through-hypothesis-testing",
    "href": "SSBBA_W3.html#a.-identifying-data-duplication-rates-through-hypothesis-testing",
    "title": "Statistical Inferences",
    "section": "",
    "text": "import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nfrom scipy.stats import (\n    mannwhitneyu,\n    norm,\n    poisson,\n    ttest_1samp,\n    ttest_ind,\n    ttest_rel,\n    wilcoxon,\n)\nfrom statsmodels.stats.proportion import proportions_ztest\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nTo protect the privacy, we take steps to ensure the accuracy of our data breach notification lists. Before creating these lists, we identify and cluster duplicate records using advanced algorithms, including rule-based and machine learning techniques. This process significantly reduces the amount of data we handle while preserving all relevant information.\nWe track data processed for each project, including:\n  1. Number of records extracted and consolidated\n  2. Number of duplicate records identified\n  3. Number of duplicates manually reviewed by analysts for additional quality assurance\n  4. % of duplication\nThese information can be seen in the below pandas dataframe\n\ndf_delivery = pd.read_excel(r\"C:\\datos\\SSBB\\DATA\\dbas_sds_project_delivery_v1.xlsx\")\n\n\ndf_delivery.head()\n\n\n\n\n\n\n\n\n\nproject_name\nbefore_rollup\nafter_rollup\nmanual_checks\ndiff\nduplication_perc\n\n\n\n\n0\nPROJECT_1\n1211\n184\n250\n66.000000\n84.805945\n\n\n1\nPROJECT_2\n170210\n20808\n29568\n8760.000000\n87.775101\n\n\n2\nPROJECT_3\n40713\n27629\n16856\n61.008361\n32.137155\n\n\n3\nPROJECT_4\n53038\n6396\n9157\n2761.000000\n87.940722\n\n\n4\nPROJECT_5\n1318\n169\n389\n220.000000\n87.177542\n\n\n\n\n\n\n\n\nWe want to assess data deduplication efficiency for our algorithms. We’ll analyze 90 projects from the past 2 months to determine:\n    1. Duplication rate per project\n    2. Algorithm performance\n    3. Post-processing manual effort required\n\n\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.boxplot(df_delivery[\"duplication_perc\"], ax=ax1)\np2 = sns.histplot(df_delivery[\"duplication_perc\"], ax=ax2, kde=True, bins=16)\np1.set_title(\"BoxPlot: What percent of Data Duplication existed in each project?\")\np2.set_title(\"Histogram:  What percent of Data Duplication existed in each project?\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nThe distribution of data duplication rates exhibits a left skew.\nAs our data exhibits a non-normal distribution, a non-parametric test is more appropriate for this scenario to avoid potential biases from assuming normality.\n\nHypothesis\n        Ho: Half of the project data has a deduplication value of 75% \n        Ha: Half of the project data does not have a deduplication value of 75%\n\ndedup_perc = df_delivery[\"duplication_perc\"]\n\n_stat, _p = wilcoxon(dedup_perc - 75)\n\n\nprint(\n    f\"P-Value Caluclated is:{round(_p,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.607 which is greater than to 0.05, hence we failed to reject the null hypothesis.\n\n\n\n\n\nWe want to investigate whether the percentage of duplicate data is correlated with project size. Specifically, we aim to determine if:\n\nSmaller projects tend to have a higher duplication rate.\nLarger projects tend to have a lower duplication rate. (or vice versa)\n\nWe have introduced a new variable named “record group” to facilitate analysis. This variable groups projects according to the size of their consolidated data set (number of data points).\n\n# Creatinng record group column with different conditions\ndf_delivery.loc[\n    ((df_delivery[\"before_rollup\"] &gt;= 1) & (df_delivery[\"before_rollup\"] &lt; 1500)),\n    \"record_group\",\n] = \"1-1499\"\n\ndf_delivery.loc[\n    ((df_delivery[\"before_rollup\"] &gt;= 1500) & (df_delivery[\"before_rollup\"] &lt; 18000)),\n    \"record_group\",\n] = \"1500-17999\"\n\ndf_delivery.loc[\n    ((df_delivery[\"before_rollup\"] &gt;= 18000) & (df_delivery[\"before_rollup\"] &lt; 120000)),\n    \"record_group\",\n] = \"18000-119999\"\n\ndf_delivery.loc[\n    (\n        (df_delivery[\"before_rollup\"] &gt;= 120000)\n        & (df_delivery[\"before_rollup\"] &lt; 1000000)\n    ),\n    \"record_group\",\n] = \"120000-999999\"\n\ndf_delivery.loc[(df_delivery[\"before_rollup\"] &gt;= 1000000), \"record_group\"] = \"1M\"\n\n\ndf_record_group = df_delivery.value_counts(\"record_group\").reset_index()\n\ndf_record_group[\"total\"] = sum(df_record_group[\"count\"])\n\ndf_record_group[\"perc_of\"] = df_record_group[\"count\"] / df_record_group[\"total\"]\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.barplot(df_record_group, y=\"record_group\", x=\"count\", ax=ax1)\np2 = sns.barplot(df_record_group, x=\"record_group\", y=\"perc_of\", ax=ax2)\np1.set_title(\"BarChart: Number of projects per each record group?\")\np2.set_title(\"BarChart: (%) of projects per each record group?\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_record_A = df_delivery.loc[\n    df_delivery[\"record_group\"].isin([\"1-1499\", \"1500-17999\"]), [\"duplication_perc\"]\n]\n\ndf_record_B = df_delivery.loc[\n    df_delivery[\"record_group\"].isin([\"18000-119999\", \"120000-999999\"]),\n    [\"duplication_perc\"],\n]\n\nWe previously defined two record groups: Group A includes data observations ranging from 1 to 17.9 thousand, and Group B includes observations from 18 thousand to 100 thousand.\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_record_A[\"duplication_perc\"], ax=ax1, kde=True)\np2 = sns.histplot(df_record_B[\"duplication_perc\"], ax=ax2, kde=True)\np1.set_title(\"Histogram:Record Group-A\")\np2.set_title(\"Histogram:Record Group-B\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotes\n\nBoth record groups show a left-skewed distribution in the data.\nSince the data in both record groups exhibits a non-normal distribution, a non-parametric test like the Mann-Whitney U test is more appropriate.\n\nHypothesis\n        Ho: On average, the amount of data duplication is the same for Record Group A and Record Group B.\n        Ha: On average, the amount of data duplication is different for Record Group A and Record Group B.\n\n_stat, _pval = mannwhitneyu(df_record_A, df_record_B)\n\nprint(\n    f\"P-Value Caluclated is:{round(_pval[0],3)} which is lesser than to 0.05, hence we can reject the null hypothesis in favour of alternative hypothesis\"\n)\n\nP-Value Caluclated is:0.001 which is lesser than to 0.05, hence we can reject the null hypothesis in favour of alternative hypothesis\n\n\n\n\n\nHalf of the project data has a deduplication value of 75%\nOn average, the amount of data duplication is different for Record Group A and Record Group B.\n\nAnalysis of data deduplication revealed that approximately half the dataset exhibits a 75% deduplication rate. However, the level of duplication appears to vary significantly between Record Group A and Record Group B, suggesting potential structural differences in the data within these groups."
  },
  {
    "objectID": "SSBBA_W3.html#b.-hypothesis-testing-approach-to-e-commerce-and-food-delivery-customer-data",
    "href": "SSBBA_W3.html#b.-hypothesis-testing-approach-to-e-commerce-and-food-delivery-customer-data",
    "title": "Statistical Inferences",
    "section": "B. Hypothesis Testing Approach to E-Commerce and Food Delivery Customer Data",
    "text": "B. Hypothesis Testing Approach to E-Commerce and Food Delivery Customer Data\n\nData Importing and Preparations\n\n# fill in the file path from where data to be imported\ndf_cs_ecom = pd.read_excel(r\"C:\\datos\\SSBB\\DATA\\customer_survey_ecom.xlsx\")\n\n\n# no fo rows and columns\ndf_cs_ecom.shape\n\n(28, 10)\n\n\n\n# first five observations\ndf_cs_ecom.head()\n\n\n\n\n\n\n\n\n\nCustomer\nGender\nAge\nHow satisfied were you with the overall quality of online services available in India.\nTo what extent you use the e-commerce/online delivery services\nHow satisfied were you with the following e-commerce/delivery companies ? [Amazon]\nHow satisfied were you with the following e-commerce/delivery companies ? [Flipkart]\nHow satisfied were you with the following e-commerce/delivery companies ? [Swiggy]\nHow satisfied were you with the following e-commerce/delivery companies ? [Zomato]\nHow satisfied were you with the following e-commerce/delivery companies ? [Others]\n\n\n\n\n0\nCUST_1\nMale\n34\n7\n6\n9\n7\n7\n7\n5\n\n\n1\nCUST_2\nMale\n33\n9\n9\n10\n3\n7\n7\n7\n\n\n2\nCUST_3\nFemale\n34\n8\n10\n10\n3\n4\n4\n8\n\n\n3\nCUST_4\nMale\n32\n8\n4\n8\n8\n5\n8\n5\n\n\n4\nCUST_5\nFemale\n40\n7\n4\n7\n7\n5\n5\n4\n\n\n\n\n\n\n\n\n\n# columns\ndf_cs_ecom.columns\n\nIndex(['Customer', 'Gender', 'Age',\n       'How satisfied were you with the overall quality of online services available in India.',\n       'To what extent you use the e-commerce/online delivery services ',\n       'How satisfied were you with the following e-commerce/delivery companies ? [Amazon]',\n       'How satisfied were you with the following e-commerce/delivery companies ? [Flipkart]',\n       'How satisfied were you with the following e-commerce/delivery companies ? [Swiggy]',\n       'How satisfied were you with the following e-commerce/delivery companies ? [Zomato]',\n       'How satisfied were you with the following e-commerce/delivery companies ? [Others]'],\n      dtype='object')\n\n\n\n# renaming column names from longer text to short\ndf_cs_ecom.columns = [\n    \"customer\",\n    \"gender\",\n    \"age\",\n    \"satiesfied_score\",\n    \"usage\",\n    \"amazon_sc\",\n    \"flipkart_sc\",\n    \"swiggy_sc\",\n    \"zomato_sc\",\n    \"others_sc\",\n]\n\n\n# first 5 observations afrer renaming the columns\ndf_cs_ecom.head()\n\n\n\n\n\n\n\n\n\ncustomer\ngender\nage\nsatiesfied_score\nusage\namazon_sc\nflipkart_sc\nswiggy_sc\nzomato_sc\nothers_sc\n\n\n\n\n0\nCUST_1\nMale\n34\n7\n6\n9\n7\n7\n7\n5\n\n\n1\nCUST_2\nMale\n33\n9\n9\n10\n3\n7\n7\n7\n\n\n2\nCUST_3\nFemale\n34\n8\n10\n10\n3\n4\n4\n8\n\n\n3\nCUST_4\nMale\n32\n8\n4\n8\n8\n5\n8\n5\n\n\n4\nCUST_5\nFemale\n40\n7\n4\n7\n7\n5\n5\n4\n\n\n\n\n\n\n\n\nThis study aims to analyze an online survey conducted in India to understand user demographics and preferences for e-commerce and food delivery services. The survey collected data on:\n\nDemographics: Age and gender\nOverall Satisfaction: Satisfaction with the general quality of online services in India\nService Usage: Frequency of using e-commerce/online delivery services\nCompany-Specific Satisfaction: Satisfaction levels with specific companies like Amazon, Flipkart, Swiggy, Zomato, and others\n\nBy analyzing this data, we hope to answer key questions such as:\n\nWhat are the demographic characteristics of our typical customer (age, gender)?\nAre there any gender differences in online service usage or satisfaction?\nHow satisfied are users with the overall quality of online services in India?\nWhich e-commerce/delivery services do users prefer the most?\nIs there a correlation between overall satisfaction and usage of specific companies?\n\n\n\nB(1):T-Test-1Sample\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.boxplot(df_cs_ecom[\"age\"], ax=ax1)\np2 = sns.histplot(df_cs_ecom[\"age\"], ax=ax2, kde=True, bins=16)\np1.set_title(\"BoxPlot: Customer Age\")\np2.set_title(\"Histogram:  Customer Age\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotes 1. Based on a sample of approximately 30 observations, the data suggests that the age is normally distributed. and the ages are independent\nHypothesis\n       Ho: The average age of customers using both e-commerce and food delivery services is 33 years old.\n       Ha: The average age of customers using both e-commerce and food delivery services is not 33 years old.\n\n_stat, _pval = ttest_1samp(df_cs_ecom[\"age\"], popmean=33)\n\nprint(\n    f\"P-Value Caluclated is:{round(_pval,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.204 which is greater than to 0.05, hence we failed to reject the null hypothesis.\n\n\n\n\nB(2):T-test-2-Sample\n\n# Sub selection gender, age and usage\ndf_gender_age_usage = df_cs_ecom[[\"gender\", \"age\", \"usage\"]]\n# Male- Age and Usage Data\ndf_cs_ecom_male = df_gender_age_usage[df_gender_age_usage[\"gender\"] == \"Male\"]\n# Female - Age and Usage data\ndf_cs_ecom_female = df_gender_age_usage[df_gender_age_usage[\"gender\"] == \"Female\"]\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_cs_ecom_male[\"age\"], ax=ax1, kde=True, bins=12)\np2 = sns.histplot(df_cs_ecom_female[\"age\"], ax=ax2, kde=True, bins=8)\np1.set_title(\"Histogram: Customer Age- Male\")\np2.set_title(\"Histogram:  Customer Age-Female\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotes 1. It suggests that the ages of both male and female customers are likely normally distributed.\nHypothesis\n        Ho: There is no difference between mean age of Female and Male Customers\n        Ha: There is a difference between mean age of Female and Male Customers\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 6))\np1 = sns.histplot(df_cs_ecom_male[\"usage\"], ax=ax1, kde=True, bins=8)\np2 = sns.histplot(df_cs_ecom_female[\"usage\"], ax=ax2, kde=True, bins=6)\np1.set_title(\"Histogram: Online platform usage- Male\")\np2.set_title(\"Histogram: Online platform usage-Female\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Perform 2-sample t-test on male and female ages\n_stat, _pval = ttest_ind(df_cs_ecom_male[\"age\"], df_cs_ecom_female[\"age\"])\n\n\nprint(\n    f\"P-Value Caluclated is:{round(_pval,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.264 which is greater than to 0.05, hence we failed to reject the null hypothesis.\n\n\nHypothesis\n        Ho: The average usage of the e-commerce platform is the same for female and male customers.\n        Ha: The average usage of the e-commerce platform is different for female and male customers.\n\n# Perform 2-sample t-test on male and female usages\n_stat, _pval = ttest_ind(df_cs_ecom_male[\"usage\"], df_cs_ecom_female[\"usage\"])\n\nprint(\n    f\"P-Value Caluclated is:{round(_pval,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.922 which is greater than to 0.05, hence we failed to reject the null hypothesis.\n\n\n\n\nB(3): 1-Proportion Test\n\ndf_ecom_plat_score = df_cs_ecom[\n    [\"amazon_sc\", \"flipkart_sc\", \"swiggy_sc\", \"zomato_sc\"]\n].melt()\n\ndf_ecom_plat_score.columns = [\"service_name\", \"satiesfied_score\"]\n\ndf_ecom_plat_score.loc[\n    df_ecom_plat_score[\"satiesfied_score\"] &gt;= 8, \"is_customer_satiesfied\"\n] = \"Yes\"\n\ndf_ecom_plat_score.loc[\n    df_ecom_plat_score[\"satiesfied_score\"] &lt; 8, \"is_customer_satiesfied\"\n] = \"No\"\n\nWe transformed the customer satisfaction scores for Amazon, Flipkart, Swiggy, and Zomato. A new binary column was added to indicate customer satisfaction. Scores 8 and above are categorized as “Yes” (satisfied), while scores below 8 are categorized as “No” (not satisfied).\n\ndf_sc = df_ecom_plat_score.value_counts(\"is_customer_satiesfied\").reset_index()\nsns.barplot(df_sc, x=\"is_customer_satiesfied\", y=\"count\")\nplt.show()\n\n\n\n\n\n\n\n\nHypothesis\n        Ho: The true satisfaction rate is 80%.\n        Ha: The true satisfaction rate is different from 80%.\n\n# Number of satisfied customers\nsuccesses = 66\n# sample size\ntrials = 112\n# Expected proportion of satisfied customers\nhypothesized_proportion = 0.8\n\n# Perform the test\nz_statistic, p_value = proportions_ztest(\n    count=successes, nobs=trials, value=hypothesized_proportion\n)\n\nprint(\n    f\"P-Value Caluclated is:{round(p_value,3)} which is less than to 0.05, hence we can reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.0 which is less than to 0.05, hence we can reject the null hypothesis.\n\n\n\n\nB(4): 2-Proportion Test\n\ndf_ser_amz_flpkart = df_ecom_plat_score[\n    df_ecom_plat_score[\"service_name\"].isin([\"amazon_sc\", \"flipkart_sc\"])\n]\n\n\ndf_sc_amzflp = (\n    df_ser_amz_flpkart.groupby([\"service_name\", \"is_customer_satiesfied\"])\n    .count()\n    .reset_index()\n)\n\n\nsns.barplot(\n    df_sc_amzflp, x=\"service_name\", y=\"satiesfied_score\", hue=\"is_customer_satiesfied\"\n)\nplt.show()\n\n\n\n\n\n\n\n\nHypothesis\n        Ho: The average level of customer satisfaction with Amazon is equal to the average level of customer satisfaction with Flipkart.\n        Ha: The average level of customer satisfaction with Amazon is not equal to the average level of customer satisfaction with Flipkart.\n\n# Number of successes in group 1 and group2\nsuccess = np.array([22, 14])\n# Total trials in group 1 and group 2 (sample size)\ntrials = np.array([28, 28])\n\n# Perform the test (assuming equal variances)\nz_statistic, p_value = proportions_ztest(\n    count=success, nobs=trials, alternative=\"two-sided\"\n)\n\nprint(\n    f\"P-Value Caluclated is:{round(p_value,3)} which is less than to 0.05, hence we can reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.026 which is less than to 0.05, hence we can reject the null hypothesis.\n\n\n\ndf_ecom_plat_score.loc[\n    df_ecom_plat_score[\"service_name\"].isin([\"amazon_sc\", \"flipkart_sc\"]),\n    \"service_type\",\n] = \"E-Commerce\"\ndf_ecom_plat_score.loc[\n    df_ecom_plat_score[\"service_name\"].isin([\"swiggy_sc\", \"zomato_sc\"]), \"service_type\"\n] = \"OnlineFood\"\n\n\ndf_sc_ef = (\n    df_ecom_plat_score.groupby([\"service_type\", \"is_customer_satiesfied\"])\n    .count()\n    .reset_index()\n)\n\n\nsns.barplot(\n    df_sc_ef, x=\"service_type\", y=\"satiesfied_score\", hue=\"is_customer_satiesfied\"\n)\nplt.show()\n\n\n\n\n\n\n\n\nHypothesis\n        Ho: The average level of customer satisfaction with E-commerce services is equal to the average level of customer satisfaction with Food Delivery services.\n        Ha: The average level of customer satisfaction with E-commerce services is not equal to the average level of customer satisfaction with Food Delivery services.\n\n# Number of successes in group 1 and group2\nsuccess = np.array([36, 30])\n# Total trials in group 1 and group 2 (sample size)\ntrials = np.array([56, 56])\n# Perform the test (assuming equal variances)\nz_statistic, p_value = proportions_ztest(\n    count=success, nobs=trials, alternative=\"two-sided\"\n)\nprint(\n    f\"P-Value Caluclated is:{round(p_value,3)} which is greater than to 0.05, hence we failed to reject the null hypothesis.\"\n)\n\nP-Value Caluclated is:0.249 which is greater than to 0.05, hence we failed to reject the null hypothesis.\n\n\n\nConclusions and Inferences\n\nThe average age of customers using both e-commerce and food delivery services is 33 years old.\nThere is no difference between mean age of Female and Male Customers\nThe true satisfaction rate is different from 80%.\nThe average level of customer satisfaction with Amazon is not equal to the average level of customer satisfaction with Flipkart.\nThe average level of customer satisfaction with E-commerce services is equal to the average level of customer satisfaction with Food Delivery services.\n\nOur analysis revealed that the average customer using both e-commerce and food delivery services is 33 years old. Interestingly, gender does not seem to be a factor, as the mean age is similar for both female and male customers. Additionally, the true customer satisfaction rate deviates from the assumed 80%. While satisfaction levels differ between Amazon and Flipkart, surprisingly, the average satisfaction for e-commerce and food delivery services is statistically the same."
  },
  {
    "objectID": "SSBBA_W3.html#c.pareto-analysis",
    "href": "SSBBA_W3.html#c.pareto-analysis",
    "title": "Statistical Inferences",
    "section": "C.Pareto Analysis",
    "text": "C.Pareto Analysis\n\n# Data Importing\ndf_causes = pd.read_excel(r'C:\\data_analytics_projects\\exploratory_data_analysis\\notebooks\\SSBB\\W3\\dbas_potential_cause.xlsx')\n\n\n# Grouping on potential causes and sum of RPN\ndf_causes=df_causes.groupby('Potential Causes').sum('RPN').reset_index()\nsorted_data = df_causes.sort_values(by=['RPN'], ascending=False)\nsorted_data['cumulative_freq'] = sorted_data['RPN'].cumsum()\nsorted_data['cumulative_pct'] = sorted_data['cumulative_freq'] / sorted_data['RPN'].sum() * 100\n\n\n# Visualizations\nfig, ax1 = plt.subplots(figsize=(22, 8))\nax2 = ax1.twinx()\nax1.bar(sorted_data['Potential Causes'], sorted_data['RPN'], color='skyblue')\nax2.plot(sorted_data['Potential Causes'], sorted_data['cumulative_pct'], color='red', marker='o', linestyle='-')\nax1.set_xlabel('Category')\nax1.set_ylabel('Frequency', color='skyblue')\nax2.set_ylabel('Cumulative Percentage', color='red')\nplt.title('Pareto Analysis- Document Review Services - Clasifying PII/PHI Records.')\nplt.show()"
  },
  {
    "objectID": "ISI_SSBBA_Book.html",
    "href": "ISI_SSBBA_Book.html",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "",
    "text": "import math\nimport re\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.graphics.gofplots as gof\nimport xgboost as xgb\nfrom feature_engine.encoding import OrdinalEncoder\nfrom scipy import stats\nfrom scipy.stats import (\n    chi2_contingency,\n    chisquare,\n    mannwhitneyu,\n    norm,\n    poisson,\n    ttest_1samp,\n    ttest_ind,\n    wilcoxon,\n)\nfrom sklearn.ensemble import (\n    GradientBoostingClassifier,\n    GradientBoostingRegressor,\n    RandomForestClassifier,\n    RandomForestRegressor,\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import (\n    ConfusionMatrixDisplay,\n    RocCurveDisplay,\n    accuracy_score,\n    confusion_matrix,\n    f1_score,\n    mean_squared_error,\n    precision_score,\n    r2_score,\n    recall_score,\n    roc_auc_score,\n    root_mean_squared_error,\n)\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats import outliers_influence as sm_oi\nfrom statsmodels.stats.anova import anova_lm\nfrom statsmodels.stats.descriptivestats import sign_test\nfrom statsmodels.stats.proportion import proportions_ztest\n\nimport spacy\nfrom wordcloud import WordCloud\n\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import AllKNN, NearMiss, RandomUnderSampler, TomekLinks\n\nsns.set_theme(\"notebook\", style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")\n\n\ndef calculate_center_line(data, column_name):\n    \"\"\"Calculates the center line for the control chart.\"\"\"\n    return data[column_name].mean()\n\n\ndef calculate_control_limits(center, average_defects_per_unit, multiplier):\n    \"\"\"Calculates upper and lower control limits.\"\"\"\n    std_dev = np.sqrt(average_defects_per_unit)\n    upper_limit = center + multiplier * std_dev\n    lower_limit = center - multiplier * std_dev\n    return upper_limit, lower_limit\n\n\ndef create_c_chart(data):\n    \"\"\"Creates a C chart for the given data.\"\"\"\n    center_line = calculate_center_line(data, \"no_of_resources\")\n    control_limits = calculate_control_limits(\n        center_line, center_line, 3\n    )  # 3 sigma limits\n    plt.figure(figsize=(10, 6))\n    plt.plot(data[\"row_nr\"], data[\"no_of_resources\"], \"o-\", label=\"no_of_resources\")\n    plt.axhline(y=center_line, color=\"r\", linestyle=\"--\", label=\"Center Line\")\n    plt.axhline(y=control_limits[0], color=\"b\", linestyle=\"--\", label=\"UCL\")\n    plt.axhline(y=control_limits[1], color=\"b\", linestyle=\"--\", label=\"LCL\")\n    plt.xlabel(\"Project\")\n    plt.ylabel(\"Number of Resources\")\n    plt.title(\"C Chart\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n\ndef create_run_chart(data, column_name):\n    \"\"\"Creates a run chart for the given data.\"\"\"\n    plt.figure(figsize=(18, 6))\n    plt.plot(data.index, data[column_name], marker=\"o\", linestyle=\"-\")\n    plt.xlabel(\n        \"Projects-From Jan to Dec-2023\"\n    )  # Assuming the index represents time periods\n    plt.ylabel(column_name)  # Replace with the actual column name\n    plt.title(\"Run Chart for \" + column_name)\n    plt.grid(True)\n    plt.show()\n\n\ndef custom_ols_qqplot(_resid):\n    \"\"\"Q-Q Plot of residuals\"\"\"\n    gof.qqplot(_resid, line=\"s\")\n    plt.xlabel(\"Theoritical Quantiles.\")\n    plt.ylabel(\"Sample Quantiles.\")\n    plt.title(\"Normal Q-Q plot\")\n    plt.show()\n\ndef get_six_sigma_caluclator():\n    \"\"\"ISI Custom SixSigma Calucator\"\"\"\n    while True:\n        ### Inputs\n        print(f\"---------\")\n        print(f\"############ Sigma Caluclator Inputs ############\")\n        print(f\"---------\")\n        _mean = float(input(\"Enter the mean:\"))\n        _sd = float(input(\"Enter Standard Deviation:\"))\n        _LSL = float(input(\"Enter LSL:\"))\n        _USL = float(input(\"Enter USL:\"))\n        # Formulas and caluclations\n        ZLSL = (_LSL - _mean) / _sd\n        ZUSL = (_USL - _mean) / _sd\n        Area_ZLSL = norm.cdf(ZLSL)\n        Area_ZUSL = 1 - norm.cdf(ZUSL)\n        TOTAL_NC = Area_ZLSL + Area_ZUSL\n        YIELD = 1 - TOTAL_NC\n        CP_ = (_USL - _LSL) / (6 * _sd)\n        _A = (_USL - _mean) / (3 * _sd)\n        _B = (_mean - _LSL) / (3 * _sd)\n        CPK_ = min(_A, _B)\n        SIGMA_LEVEL = round(1.5 + norm.ppf(YIELD), 5)\n        DPMO = TOTAL_NC * 1000000\n        # Output\n        print(f\"---------\")\n        print(f\"#### Summary Report ####\")\n        print(f\"---------\")\n        print(f\"Total NonConfirmances:{round(TOTAL_NC,5)}\")\n        print(f\"Yield:{round(YIELD,5)}\")\n        print(f\"CP:{round(CP_,5)}\")\n        print(f\"CPK:{round(CPK_,5)}\")\n        print(f\"SIGMA_LEVEL:{round(SIGMA_LEVEL,5)}\")\n        print(f\"DPMO:{round(DPMO,5)}\")\n        print(f\"---------\")\n        _next = input(\n            \"Would you like to continue to use sigma caluclator type 'yes' if so :\"\n        )\n        if _next.lower() == \"yes\":\n            continue\n        else:\n            print(f\"Thanks for using Sigma Caluclator..\")\n            print(f\"#### END ####\")\n            break\n\n\n# TEXT MINING FUNCTIONS\ndef get_tidy_text(_txt):\n    _doc = DBAS_NLP(_txt)\n    _tidy_word = [token.text for token in _doc if not token.is_stop]\n    return \" \".join(_tidy_word)\n\ndef get_lemma_pos(_txt):\n    _doc = DBAS_NLP(_txt)\n    _d = {\"_POS\": [], \"_LEMMA\": []}\n    for _token in _doc:\n        _d[\"_POS\"].append(_token.pos_)\n        _d[\"_LEMMA\"].append(_token.lemma_)\n    return _d\n\ndef _generate_word_cloud(_txt):\n    x, y = np.ogrid[:300, :300]\n    mask = (x - 150) ** 2 + (y - 150) ** 2 &gt; 130**2\n    mask = 255 * mask.astype(int)\n    wc = WordCloud(background_color=\"white\", repeat=True, mask=mask)\n    wc.generate(_txt)\n    plt.axis(\"off\")\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.show()\n\ndef get_word_rep(_word, _rep):\n    repeated_word = f\"{_word} \" * _rep\n    return repeated_word\n\ndf_challenges = pl.read_csv(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/dbas_sds_challenges.csv\"\n).with_row_count()\n\nlos_samples = []\nfor _ in range(10):\n    _df = df_challenges.sample(15, with_replacement=False,seed=100)\n    los_samples.append(_df)\n\ndf_conso = pl.concat(los_samples).to_pandas()\n\nDBAS_NLP = spacy.load(\n    \"en_core_web_sm\"\n)\n\ndf_conso[\"_tidy_text_1\"] = df_conso[\"#CHALLENGE\"].apply(lambda x: get_tidy_text(x))\n\nlos_doc_dicts = []\nfor _sen in df_conso[\"_tidy_text_1\"]:\n    _d = get_lemma_pos(_sen)\n    los_doc_dicts.append(_d)\n\nlos_dfs = []\nfor _d in los_doc_dicts:\n    _DF = pd.DataFrame(_d)\n    los_dfs.append(_DF)\n\nDF_POS = pd.concat(los_dfs)\n\nDF_POS_TIDY = DF_POS[DF_POS[\"_POS\"] != \"PUNCT\"]\n\n_LOS_NOUNS = (\n    DF_POS_TIDY[DF_POS_TIDY[\"_POS\"] == \"NOUN\"]\n    .groupby(\"_LEMMA\")\n    .count()\n    .reset_index()\n    .sort_values(\"_POS\", ascending=False)\n)\n\n_ADJCT = (\n    DF_POS_TIDY[DF_POS_TIDY[\"_POS\"] == \"ADJ\"]\n    .groupby(\"_LEMMA\")\n    .count()\n    .reset_index()\n    .sort_values(\"_POS\", ascending=False)\n)\n\n_VERBS = (\n    DF_POS_TIDY[DF_POS_TIDY[\"_POS\"] == \"VERB\"]\n    .groupby(\"_LEMMA\")\n    .count()\n    .reset_index()\n    .sort_values(\"_POS\", ascending=False)\n)\n\nLOS_VERBS = _VERBS[[\"_LEMMA\", \"_POS\"]].apply(lambda x: get_word_rep(x[0], x[1]), axis=1)\nLOS_NOUNS = _LOS_NOUNS[[\"_LEMMA\", \"_POS\"]].apply(lambda x: get_word_rep(x[0], x[1]), axis=1)\nLOS_ADJ = _ADJCT[[\"_LEMMA\", \"_POS\"]].apply(lambda x: get_word_rep(x[0], x[1]), axis=1)\n\ntodos_verb_list = []\nfor _verb in LOS_VERBS:\n    _vs = _verb.split(\" \")\n    todos_verb_list.extend(_vs)\n\ntodos_noun_list = []\nfor _verb in LOS_NOUNS:\n    _vs = _verb.split(\" \")\n    todos_noun_list.extend(_vs)\n\ntodos_adj_list = []\nfor _verb in LOS_ADJ:\n    _vs = _verb.split(\" \")\n    todos_adj_list.extend(_vs)\n\ntext_verb = \" \".join([_word for _word in todos_verb_list if len(_word) != 0])\ntext_noun = \" \".join([_word for _word in todos_noun_list if len(_word) != 0])\ntext_adj = \" \".join([_word for _word in todos_adj_list if len(_word) != 0])\n\n\n## ANALYZE PHASE CODE\ndf_org = (\n    pl.read_csv(\n        r\"/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/DBAS_Projects_Data_Tidy_V3.csv\"\n    )\n    .drop([\"Started Date\", \"Complete Date\", \"NO_OF_DAYS\", \"NO_OF_SERVICE_WORKING_HRS\"])\n    .with_columns(pl.col(\"Project Name\").str.replace_all(\"PROJECT\", \"P\"))\n).filter(pl.col(\"accuracy_\").is_not_null())\n\ndf_org.columns = [\n    \"Pname\",\n    \"#Files\",\n    \"#Days\",\n    \"#Analyst\",\n    \"#PII_PHI\",\n    \"#FPS\",\n    \"#FNS\",\n    \"FPS_FNS\",\n    \"TPS_TNS\",\n    \"Accuracy\",\n]\n\nDF_SSBB = df_org.filter((pl.col(\"#Files\") &gt; 10) & (pl.col(\"#Files\") &lt; 10000))\n\n\nDF_SSBB_V2 = (\n    DF_SSBB.with_columns(\n        (pl.col(\"#PII_PHI\") / pl.col(\"#Files\")).alias(\"prop_of_pii_phi\")\n    )\n    .with_columns(\n        pl.when((pl.col(\"#Days\") &gt;= 1) & (pl.col(\"#Days\") &lt; 8))\n        .then(pl.lit(\"1W\").alias(\"weeks_bucket\"))\n        .otherwise(None)\n    )\n    .with_columns(\n        pl.when((pl.col(\"#Days\") &gt;= 8) & (pl.col(\"#Days\") &lt; 15))\n        .then(pl.lit(\"2W\").alias(\"weeks_bucket\"))\n        .otherwise(pl.col(\"weeks_bucket\"))\n    )\n    .with_columns(\n        pl.when((pl.col(\"#Days\") &gt;= 15) & (pl.col(\"#Days\") &lt; 22))\n        .then(pl.lit(\"3W\").alias(\"weeks_bucket\"))\n        .otherwise(pl.col(\"weeks_bucket\"))\n    )\n    .with_columns(\n        pl.when((pl.col(\"#Days\") &gt;= 22) & (pl.col(\"#Days\") &lt; 29))\n        .then(pl.lit(\"4W\").alias(\"weeks_bucket\"))\n        .otherwise(pl.col(\"weeks_bucket\"))\n    )\n    .with_columns(\n        pl.when((pl.col(\"#Days\") &gt;= 29))\n        .then(pl.lit(\"more than 4 weeks\").alias(\"weeks_bucket\"))\n        .otherwise(pl.col(\"weeks_bucket\"))\n    )\n    .with_columns(\n        pl.when((pl.col(\"#PII_PHI\") &gt;= 1) & (pl.col(\"#PII_PHI\") &lt; 101))\n        .then(pl.lit(\"1-100\").alias(\"pii_phi_bucket\"))\n        .otherwise(None)\n    )\n    .with_columns(\n        [\n            pl.when((pl.col(\"#PII_PHI\") &gt;= 101) & (pl.col(\"#PII_PHI\") &lt; 501))\n            .then(pl.lit(\"101-500\").alias(\"pii_phi_bucket\"))\n            .otherwise(pl.col(\"pii_phi_bucket\")),\n        ]\n    )\n    .with_columns(\n        [\n            pl.when((pl.col(\"#PII_PHI\") &gt;= 501) & (pl.col(\"#PII_PHI\") &lt; 1001))\n            .then(pl.lit(\"500-1000\").alias(\"pii_phi_bucket\"))\n            .otherwise(pl.col(\"pii_phi_bucket\")),\n        ]\n    )\n    .with_columns(\n        [\n            pl.when((pl.col(\"#PII_PHI\") &gt;= 1001) & (pl.col(\"#PII_PHI\") &lt; 5001))\n            .then(pl.lit(\"1000-5000\").alias(\"pii_phi_bucket\"))\n            .otherwise(pl.col(\"pii_phi_bucket\")),\n        ]\n    )\n    .with_columns(\n        [\n            pl.when((pl.col(\"#PII_PHI\") &gt;= 5001))\n            .then(pl.lit(\"more than 5K\").alias(\"pii_phi_bucket\"))\n            .otherwise(pl.col(\"pii_phi_bucket\")),\n        ]\n    )\n    .with_columns(\n        pl.when(pl.col(\"#FPS\") &gt; pl.col(\"#FNS\"))\n        .then(pl.lit(\"FPS&gt;FNS\"))\n        .otherwise(pl.lit(\"FNS&gt;FPS\"))\n        .alias(\"FPS_vs_FNS\")\n    )\n    .with_columns(\n        pl.when(pl.col(\"FPS_FNS\") &gt; pl.col(\"TPS_TNS\"))\n        .then(pl.lit(\"FPS_FNS&gt;TPS_TNS\"))\n        .otherwise(pl.lit(\"TPS_TNS&gt;FPS_FNS\"))\n        .alias(\"FPFN_vs_TPTN\")\n    )\n)\n\nDF_SSBB_TIDY = DF_SSBB_V2.to_pandas()\n\n\n##IMPLEMENTATION PHASE CODE BLOCKS\ndef custom_string_tidy(texto):\n    \"\"\"Clean up all the punctuations from a message\"\"\"\n    ## STEP-1\n    rgx_ltrs = re.compile(\n        \"[\\!|\\\"|\\#|\\$|\\%|\\&|'|\\(|\\)|\\*|\\+|\\,|\\-|\\/|\\:|\\;|\\&lt;|\\=|\\&gt;|\\?|\\[|\\\\|\\]|\\^|\\_|\\`|\\{|\\||\\}|\\~|\\.]+|FRM\\:[\\w\\W]+(SUBJ\\:)?MSG\\:|\\@|http(s)?|HTTP(S)?|\\n|\\r\"\n    )\n    tidy_1_txt = re.sub(rgx_ltrs, \" \", str(texto))\n    tidy_2_txt = tidy_1_txt.strip()\n    return tidy_2_txt.lower()\n\n\ndef get_text_len(_txt):\n    return len(_txt)\n\n\ndef get_list_count(_txt):\n    _text_list = _txt.split(\" \")\n    return len(_text_list)\n\n\ndef custom_classification_metrics_report(_model_type, _actual, _predicted):\n    _metric_dict = {\n        \"Accuracy\": [],\n        \"ROC-AUC\": [],\n        \"F1-Score\": [],\n        \"Precision\": [],\n        \"Recall\": [],\n    }\n    _metric_dict[\"Accuracy\"].append(round(accuracy_score(_actual, _predicted), 5))\n    _metric_dict[\"ROC-AUC\"].append(round(roc_auc_score(_actual, _predicted), 5))\n    _metric_dict[\"F1-Score\"].append(round(f1_score(_actual, _predicted), 5))\n    _metric_dict[\"Precision\"].append(round(precision_score(_actual, _predicted), 5))\n    _metric_dict[\"Recall\"].append(round(recall_score(_actual, _predicted), 5))\n    print(\"-------------------------------------------------------------------\")\n    print(f\"{_model_type}#:ConfusionMatrix and ROC-AUC Curve\")\n    print(\"-------------------------------------------------------------------\")\n    _plt = ConfusionMatrixDisplay(confusion_matrix(_actual, _predicted))\n    _plt.plot()\n    RocCurveDisplay.from_predictions(_actual, _predicted)\n    plt.show()\n    _df = pd.DataFrame(_metric_dict)\n    _df.index = [f\"{_model_type}\"]\n    return _df\n\n# custom function\ndef tokenize_words(doc, lower_case=True):\n    \"\"\"Tokenizations Unigram\"\"\"\n    if lower_case is not True:\n        doc_prep = doc\n    doc_prep = str.lower(doc)\n    tokens = doc_prep.split()\n    return tokens\n\n# custom function\ndef my_counter(doc):\n    \"\"\"Custom counter function\"\"\"\n    x_dict = {}\n    for item in doc:\n        if item in x_dict.keys():\n            x_dict[item] += 1\n        else:\n            x_dict[item] = 1\n    return x_dict\n\ndef computeTFIDF(corpus):\n    \"\"\"Given a list of sentences as \"corpus\", return the TF-IDF vectors for all the\n    sentences in the corpus as a numpy 2D matrix.\n\n    Each row of the 2D matrix must correspond to one sentence\n    and each column corresponds to a word in the text corpus.\n\n    Please order the rows in the same order as the\n    sentences in the input \"corpus\".\n\n    Ignore puncutation symbols like comma, fullstop,\n    exclamation, question-mark etc from the input corpus.\n\n    For e.g, If the corpus contains sentences with these\n    9 distinct words, ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'],\n    then the first column of the 2D matrix will correpsond to word \"and\", the second column will\n    correspond to column \"document\" and so on.\n\n    Write this function using only basic Python code, inbuilt Data Structures and  NumPy ONLY.\n\n    Implement the code as optimally as possible using the inbuilt data structures of Python.\n    \"\"\"\n\n    ##############################################################\n    ####   YOUR CODE BELOW  as per the above instructions #######\n    ##############################################################\n\n    # Create a dictionary to hold doc and their tokens\n    docs = {}\n    doc_tokens = {}\n\n    for idx, doc in enumerate(corpus):\n        docs[\"doc\" + str(idx)] = tokenize_words(doc, lower_case=True)\n        doc_tokens[\"doc\" + str(idx)] = tokenize_words(doc, lower_case=True)\n\n    # Count the number of tokens in each document\n    for key, val in docs.items():\n        docs[key] = my_counter(val)\n\n    # Caluclate term frequency in each document\n    for key, val in docs.items():\n        tot = 0\n        for valor in val.values():\n            tot += valor\n        for k, v in val.items():\n            val[k] = v / tot\n\n    # Extracting the keys from each document\n    x_keys = []\n    for val in docs.values():\n        x_keys.extend(list(val.keys()))\n\n    # Count total number of tokens across corpus\n    doc_counts = my_counter(x_keys)\n\n    # Caluclate IDF\n    for k, v in doc_counts.items():\n        doc_counts[k] = math.log(len(docs) / v)\n\n    # A dictionary to hold the IDF values of each word\n    doc_idf = {}\n    for k, v in docs.items():\n        lista = {}\n        for key in v.keys():\n            if key in doc_counts.keys():\n                for val in v.values():\n                    lista[key] = doc_counts[key]\n                    break\n        doc_idf[k] = lista\n\n    # Mapping IDF values to the corresponding words\n    docs_tf_idf = {}\n    for k, v in docs.items():\n        docs_tf_idf[k] = [docs[k], doc_idf[k]]\n    # Caluclate the TF-IDF and keep it in a dictionary\n    docs_tf_idf_cal = {}\n    for k, v in docs_tf_idf.items():\n        docs_tf_idf_cal[k] = {\n            x: np.round(v[0].get(x, 0) * v[1].get(x, 0), 2) for x in v[0].keys()\n        }\n\n    # An array to store all the TF-IDF's\n    x_tf_arr = []\n    for kt, vt in doc_tokens.items():\n        for v in vt:\n            if v in list(docs_tf_idf_cal.get(kt).keys()):\n                x_tf_arr.append(docs_tf_idf_cal.get(kt)[v])\n\n    # Create a matrix with 4*6\n    # _grader_std = np.array(x_tf_arr,dtype='float64').reshape(4,6)\n\n    return x_tf_arr\n\n\ndf_pii_phi_raw = pd.read_excel(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/final/SDS_CLASSIFICATION_TIDY_FINAL_V2.xlsx\"\n)\ndf_pii_phi_raw[\"_dbas_field_name_tidy\"] = df_pii_phi_raw[\"_dbas_field_name\"].apply(\n    lambda x: custom_string_tidy(x)\n)\ndf_pii_phi_raw[\"class_label\"] = df_pii_phi_raw[\"class_label\"].replace(\n    {\"Y\": 1, \"N\": 0, \"y\": 1}\n)\ndf_pii_phi_raw[\"tidy_field_len\"] = df_pii_phi_raw[\"_dbas_field_name_tidy\"].apply(\n    lambda x: get_text_len(x)\n)\ndf_pii_phi_raw[\"no_of_words\"] = df_pii_phi_raw[\"_dbas_field_name_tidy\"].apply(\n    lambda x: get_list_count(x)\n)"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#introduction.",
    "href": "ISI_SSBBA_Book.html#introduction.",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "4.1 Introduction.",
    "text": "4.1 Introduction.\n\n4.1.1 Introduction to document review and classifications – how to capture PII/PHI.\nIn the digital era - Spam attacks are targeted on individual or organization levels to get some valuable information from them to do any kind of frauds activities. Here data breaches are happening knowingly or unknowingly.\nHere is a case:\nA customer has received a SMS or Email on his phone saying that his or her payment is pending and pay it off by logging in a website which is a phishing one and not aware to him/her, the required information (such as Names, Address, Email, Payment Card Number and PIN) is filled in and leaked to Spammers, and here they will make use this information to do financial frauds, and here a person’s data has been compromised unfortunately.\nHere is a list of industries or organizations which are attacked with different kinds of spam types such as ransomwares, malwares, spywares, smishing’s or phishing to take away the financial/personal/health information.\n\nBFSI Institutions\nE-commerce\nTelecom\nHealth\nEducational institutes\nSocial medias\nHotel and food services\nHospital\nPharma\nAgriculture\nGaming\nGovernment/Public, Military services\n\nWhen any spam attack is happened, the stolen information must be available in form of structured or unstructured data formats stored in different kind of file systems such as\n\nDatabase files\nText or pdf files\nXML or JSON\nSpreadsheets\n\nAs an organization or individuals, they would like to know what kind of NPPI (Nonpublic Personal Information) has been leaked out after a data breach happened. Our DBAS team of analysts come into picture to address their requests – looking for PII/PHI by banking upon the Intelligence systems (Data Mining/Information Retrievals) that we set up as part of services.\nAs we have seen above in an example case the users PII (Personal Identifiable Information) was leaked out as they contain an attribute such as Name, Address, Email and Payment card number/PIN.\nThere are loads of documents (it deals with all type of domains discussed in above) sent for reviewing/classifying whether any PII/PHI is existed in it or not, if any of attributes of PII/PHI found in any of these documents they need to be extracted and kept in a recommended format as per Protocol/Guidelines. Here as an analyst, we should be exploring, understanding, and deciding what makes an entry/document to be classified as PII/PHI.\n\n\n4.1.2 Introduction to DataBreach Discovery Analysis Process.\n\n4.1.2.1 Initial Review and Data Extraction.\n\nA team of trained analysts will embark on a meticulous review of a designated set of documents. These documents will be pre-tagged to identify specific types of Personally Identifiable Information (PII) and Protected Health Information (PHI) data points.\nDuring this review, each analyst will meticulously extract the required PII/PHI data from the documents assigned to them. This extracted information could include elements like names, addresses, Social Security numbers, or medical diagnoses, depending on the specific types of PII/PHI being targeted.\n\n\n\n4.1.2.2 Data Consolidation and Standardization.\n\nOnce all analysts have completed their initial review, a data consolidation phase will commence. This involves meticulously gathering the extracted PII/PHI data from each analyst’s files.\nThe consolidated data will then be uploaded or “dumped” into a pre-defined project template. This template will have a structured format with designated metadata fields. These fields ensure proper organization and searchability of the extracted information. Examples of metadata fields might include document source, date of extraction, and the analyst who reviewed the document.\n\n\n\n4.1.2.3 Data Deduplication: Unifying Multiple Instances.\n\nThe next critical process is data deduplication. This step addresses the inherent challenge of PII/PHI potentially appearing in multiple documents for the same individual.\nImagine a scenario where a person’s name, address, and phone number appear in both a customer registration form and a service call log. Data deduplication techniques will identify these duplicate entries and cluster them into a single “observation” within the project template.\nThis clustering process helps eliminate redundancy and ensures a more accurate and concise representation of each individual’s PII/PHI data within the final dataset.\n\n\n\n4.1.2.4 Data Quality Control (QC): Ensuring Accuracy and Completeness.\nAfter the data deduplication process, the consolidated dataset will undergo a rigorous Manual Quality Control (QC) check. This critical step involves a team of experts meticulously cross-checking the data quality using pre-defined criteria.\n\n4.1.2.4.1 Focus Areas of Manual QC.\n\nData Capture Accuracy: The QC team will verify that all required PII/PHI data points were correctly extracted from the documents during the initial review phase. This might involve checking for missing information, typos, or inconsistencies in data formatting.\nMinimizing Errors: The team will identify and address any potential false positives (data points mistakenly classified as PII/PHI) or false negatives (actual PII/PHI data points missed during extraction).\nOverall Data Integrity: The QC process ensures the overall accuracy, completeness, and consistency of the deduplicated dataset.\n\n\n\n4.1.2.4.2 Notification and Action.\n\nOnce the Manual QC process is complete, a notification list will be generated. This list will detail any identified errors or inconsistencies within the data.\nThe notification list will then be submitted to the legal counsel for review, typically aiming for a completion rate of at least 95% for the Quality Checks. This high threshold ensures a high level of confidence in the data quality before further action.\n\n\n\n\n\n4.1.3 DataBreach Analysis-Data Extraction Process\n\n4.1.3.1 Execute NER Analysis on Pool.\nTo identify potentially sensitive information, all incoming documents are analyzed by a Named Entity Recognition (NER) system. This AI-powered tool scans for Personally Identifiable Information (PII) and Protected Health Information (PHI) such as names, addresses, Social Security numbers, and medical records. Documents flagged by the NER system are then routed to the structured data services team for further categorization and data extraction.\n\n\n4.1.3.2 Collect files and Divide into Sets for review.\nGiven the diverse file formats received (e.g., spreadsheets, text documents, PDFs, images), an initial sorting step is crucial. Our system automatically classifies incoming documents based on their file type. This streamlines the review process by directing documents to reviewers with the appropriate expertise.\n\n\n4.1.3.3 Start reviewing the documents.\nOnce classified and assigned, reviewers begin analyzing the documents. This initial assessment involves techniques like examining column headers in spreadsheets, counting rows and columns, and visually evaluating data structure. This initial analysis helps reviewers understand the document’s content and determine the most effective approach for further review.\n\n\n4.1.3.4 Figure out and Classify the PII/PHI columns.\nDue to the complexities of identifying PII/PHI data, a manual review process is necessary after the initial NER analysis. Reviewers with expertise in data privacy regulations and compliance protocols meticulously examine each assigned document. This in-depth review involves:\n\nOpening the file: Reviewers access the document for analysis.\nAnalyzing column headers: Particular focus is placed on column headers in spreadsheets and similar data structures. Reviewers compare these headers against predefined lists of PII/PHI elements as outlined in the established counsel protocol.\nMatching data elements: Reviewers meticulously check if any column headers correspond to known PII/PHI categories. This may include names, addresses, Social Security numbers, phone numbers, email addresses, and health information.\nIterative classification: This process is repeated for all assigned documents, allowing reviewers to systematically classify columns containing PII/PHI data.\n\n\n\n4.1.3.5 Extract the information from the PII/PHI columns.\nOnce PII/PHI columns are identified, reviewers extract the specific information from those designated fields. This critical step involves carefully collecting the relevant data points, ensuring accuracy and completeness. The extracted information is then forwarded to the subsequent stage in the processing workflow.\n\n\n4.1.3.6 Consolidate the gathered PII/PHI information from the files.\nTo gain a comprehensive view of all PII/PHI data within a project, a consolidation step is essential. Extracted data from each file is meticulously merged into a central repository. This consolidated dataset provides a holistic perspective on all PII/PHI information associated with the project.\n\n\n4.1.3.7 Data Cleaning and Tidying.\nReal-world data often contains inconsistencies and errors. To ensure the accuracy and usability of the extracted PII/PHI information, a data cleaning and tidying process is implemented. This stage may involve:\n\nIdentifying and correcting errors: Reviewers address any inconsistencies or inaccuracies present in the extracted data.\nFormatting data: Data is formatted according to predefined standards to ensure consistency and facilitate further analysis.\nSplitting data: If necessary, data elements may be split into more granular components to enhance organization and usability.\n\n\n\n4.1.3.8 Enter the tidy data in the project protocol template sheet.\nFollowing the data cleaning and tidying process, the resulting high-quality information needs to be integrated into the project workflow. This step involves:\n\nData transfer: The meticulously cleaned and formatted data is carefully transferred to the designated project protocol template sheet. This template likely resides within a project management tool or a centralized repository.\nData integration: By incorporating the clean data into the project protocol template, it becomes part of the project’s overall record. This ensures all relevant information is readily available for subsequent stages, such as data deduplication."
  },
  {
    "objectID": "ISI_SSBBA_Book.html#business-case.",
    "href": "ISI_SSBBA_Book.html#business-case.",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "4.2 Business Case.",
    "text": "4.2 Business Case.\nThe current manual document review process suffers from significant inaccuracies, leading to rework and delays in generating data breach notification lists. This inefficiency stems from two primary issues:\n\nFalse Positives and False Negatives: During data element extraction, analysts encounter a high rate of errors. They may mistakenly identify irrelevant information as PII/PHI (false positives), or miss crucial PII/PHI data points (false negatives). These errors necessitate rework, requiring analysts to re-review documents and correct mistakes.\nExcessive Time Spent on Manual Extraction: The reliance on manual extraction significantly increases processing time. Analysts spend a considerable amount of effort manually extracting data elements from various documents, hindering their ability to complete projects within the designated timeframe.\n\nThese issues have a detrimental impact on the document review service in several ways:\n\nIncreased Costs: Rework due to errors necessitates additional analyst time, leading to increased operational costs.\nDelayed Notification: Inaccurate data extraction delays the generation of data breach notification lists, potentially putting individuals at risk for longer periods.\nReduced Customer Satisfaction: Delays and inaccuracies compromise the quality of service provided to clients.\nInefficient Resource Allocation: Analysts’ time spent on manual extraction could be better utilized for more complex tasks requiring human judgment."
  },
  {
    "objectID": "ISI_SSBBA_Book.html#voice-of-customersvoc",
    "href": "ISI_SSBBA_Book.html#voice-of-customersvoc",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "4.3 Voice Of Customers(VOC)",
    "text": "4.3 Voice Of Customers(VOC)\n\n\n\nTable 1: Voice of Customers\n\n\n\n\n\n\n\n\n\n\n\nS.NO\nVOC\nTYPE\nCTQ\n\n\n\n\n1\nSpreadsheets with massive amounts of data are cumbersome and slow to review.\nInternal-Customer\nDocument Review Time\n\n\n2\nLarge volumes of unstructured data are challenging to parse and extract accurately.\nInternal-Customer\nDocument Classification\n\n\n3\nDifficulty identifying and selecting PII/PHI fields.\nInternal-Customer\nDocument Classification\n\n\n4\nUnclear headers and diverse file formats make data interpretation difficult.\nInternal-Customer\nDocument Classification\n\n\n5\nManual data extraction is error-prone and time-consuming.\nInternal-Customer\nDocument Review Time\n\n\n6\nDelays and reworks occur due to false positives/negatives.\nExternal-Customer\nDocument Classification\n\n\n7\nSlow response times from the review team on queries hinder productivity.\nInternal-Customer\nDocument Review Time\n\n\n8\nNotification lists are not delivered on time.\nExternal-Customer\nDocument Review Time\n\n\n9\nLack of automation tools for data extraction leads to repetitive manual tasks.\nInternal-Customer\nDocument Classification\n\n\n10\nLack of file management system\nInternal-Customer\nDocument Review Time"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#process-map",
    "href": "ISI_SSBBA_Book.html#process-map",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "4.4 Process Map",
    "text": "4.4 Process Map\n\n\n\nDataBreach Analysis Processs Flow Diagram\n\n\n\n4.4.1 SIPOC\n\n\n\nDBAS-HighLeve Process Map\n\n\n\n\n\nTable 2: SIPOC Table\n\n\n\n\n\n\n\n\n\n\n\nProcess\nInput\nOutput\nCustomer (Who Benefits)\n\n\n\n\nInitial Review & Data Extraction\n- Documents requiring review (various formats)  - Predefined PII/PHI identification rules  - Review team\n- Extracted PII/PHI data  - Classified document  - Initial data points\nInternal Customer (Next stage in review process)\n\n\nData Consolidation & Standardization\n- Extracted PII/PHI data from various documents  - Data cleaning & formatting rules\n- Consolidated & Standardized PII/PHI dataset\nInternal Customer (Next stage in review process)\n\n\nData Deduplication: Unifying Multiple Instances\n- Consolidated & Standardized PII/PHI dataset\n- Deduplicated PII/PHI dataset (removing duplicates)\nInternal Customer (Next stage in review process)\n\n\nData Quality Control (QC): Ensuring Accuracy & Completeness\n- Deduplicated PII/PHI dataset\n- Reviewed & Validated PII/PHI dataset (ensured accuracy & completeness)\nInternal Customer (Using the data) & External Customer (Potentially impacted by data)\n\n\nNotification & Action\n- Reviewed & Validated PII/PHI dataset\n- Notification of completion/issues (to relevant parties)  - Documented actions taken\nInternal Customer (Project manager/stakeholders) & External Customer (Legal Counsels, Companies whose data got breached)"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#project-charter",
    "href": "ISI_SSBBA_Book.html#project-charter",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "4.5 Project Charter",
    "text": "4.5 Project Charter\n\n\n\nDBAS-Project Charter"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#data-collection-plan.",
    "href": "ISI_SSBBA_Book.html#data-collection-plan.",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "5.1 Data Collection Plan.",
    "text": "5.1 Data Collection Plan.\nWithin Data Breach Discovery Services, the following key metrics are captured in the project management tool to assess document review performance.\n\n5.1.1 Metrics:\n1.Project Initiation:\nStart Date: Capture the date the project was initiated in the project management tool.\n2.Document Volume:\nTotal Number of Documents:Track the total number of documents included in the review process.\n3.Personal Identifiable Information (PII) / Protected Health Information (PHI) Detection:\nNumber of PII/PHI Tagged Documents: Measure the number of documents containing identified PII/PHI data points.\n4.Resource Allocation:\nNumber of Assigned Analysts: Record the number of analysts assigned to the project.\n5.Process Cycle Time:\nThis metric captures the overall time taken for each stage of the review process. Ideally, categorize and track the time taken for each step:\n\nExtraction - Time taken to extract data from documents.\nCleaning & Tidying - Time spent cleaning and formatting extracted data.\nConsolidation - Time taken to combine and organize cleaned data.\nDe-duplication - Time spent removing duplicate data entries.\nQuality Control (QC) - Time taken for quality checks on the processed data.\nFinal Notification Delivery - Time taken to deliver the final report or notification.\n\n6.Error Rate:\nNumber of False Positives/False Negatives: Track the number of instances where PII/PHI was incorrectly identified (False Positive) or missed (False Negative).\n7.Challenge and Inquiry Tracking:\n\nNumber of Specific Challenges: Record the number of specific challenges encountered during the review process. Examples could be unclear document formats, data inconsistencies, etc.\nNumber of Specific Queries: Track the number of specific questions raised by analysts requiring clarification or further guidance.\n\n8.Project Completion:\nEnd Date: Capture the date the document review project was completed in the project management tool.\n\n\n5.1.2 Glance at Data\n\ndf_org = pl.read_csv(\n    r\"/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/DBAS_Projects_Data_Tidy_V3.csv\"\n).drop([\"Started Date\", \"Complete Date\", \"NO_OF_DAYS\", \"NO_OF_SERVICE_WORKING_HRS\"]).with_columns(pl.col(\"Project Name\").str.replace_all(\"PROJECT\", \"P\"))\n\ndf_org.columns = [\n    \"Pname\",\n    \"#Files\",\n    \"#Days\",\n    \"#Analyst\",\n    \"#PII_PHI\",\n    \"#FPS\",\n    \"#FNS\",\n    \"FPS_FNS\",\n    \"TPS_TNS\",\n    \"Accuracy\",\n]\nDF_DBAS = df_org.to_pandas()\n\nThe following two tables present databreach discovery project management information collected from roughly 250 projects handled over the last year.\n\ndf_1 = df_org.filter(pl.col('#Files')&gt;100).sample(10).to_pandas()\n\ndf_1.iloc[:,:5]\n\n\n\n\n\n\nDBAS-Document Reviews Data-1 {#f02b3b3e}\n\n\n\nPname\n#Files\n#Days\n#Analyst\n#PII_PHI\n\n\n\n\n0\nP_68\n141\n9\n5\n122\n\n\n1\nP_250\n5498\n34\n15\n4563\n\n\n2\nP_109\n367\n44\n2\n322\n\n\n3\nP_28\n1103\n15\n4\n948\n\n\n4\nP_120\n1769\n2\n3\n1645\n\n\n5\nP_140\n232\n5\n2\n222\n\n\n6\nP_124\n894\n7\n2\n840\n\n\n7\nP_41\n111\n1\n2\n104\n\n\n8\nP_119\n4766\n33\n9\n4527\n\n\n9\nP_228\n2076\n5\n7\n1764\n\n\n\n\n\n\n\n\n\npd.concat([df_1.iloc[:,0],df_1.iloc[:,4:]],axis=1)\n\n\n\n\n\n\nDBAS-Document Reviews Data-2 {#75c08506}\n\n\n\nPname\n#PII_PHI\n#FPS\n#FNS\nFPS_FNS\nTPS_TNS\nAccuracy\n\n\n\n\n0\nP_68\n122\n29\n31\n60\n62\n0.508197\n\n\n1\nP_250\n4563\n182\n1688\n1870\n2693\n0.590182\n\n\n2\nP_109\n322\n41\n22\n63\n259\n0.804348\n\n\n3\nP_28\n948\n37\n199\n236\n712\n0.751055\n\n\n4\nP_120\n1645\n82\n411\n493\n1152\n0.700304\n\n\n5\nP_140\n222\n44\n44\n88\n134\n0.603604\n\n\n6\nP_124\n840\n109\n285\n394\n446\n0.530952\n\n\n7\nP_41\n104\n7\n37\n44\n60\n0.576923\n\n\n8\nP_119\n4527\n1312\n543\n1855\n2672\n0.590236\n\n\n9\nP_228\n1764\n317\n158\n475\n1289\n0.730726\n\n\n\n\n\n\n\n\nThis table summarizes the key challenges identified by analysts during the Extraction Phase of the Document Review Service Process. Data was collected through an internal survey.\n\ndf_challenges.to_pandas()[['row_nr','#CHALLENGE']].sample(15,replace=False)\n\n\n\n\n\n\nReviewers Challenges/Problems {#155ef8a2}\n\n\n\nrow_nr\n#CHALLENGE\n\n\n\n\n13\n13\nworking on large volumes files effects\n\n\n5\n5\nlarge file size\n\n\n12\n12\ndiverse formats\n\n\n2\n2\nmultiple sheets\n\n\n0\n0\ndifferent file structures\n\n\n9\n9\nexcel documents having different sheet formats\n\n\n6\n6\njumbled data\n\n\n22\n22\ntaking time for the consolidation process.\n\n\n24\n24\nextracting data from large files.\n\n\n20\n20\nhandling large set data while extracting\n\n\n19\n19\nextracting data from multiple sheets is a bit ...\n\n\n21\n21\nthe files which contains large volumes of data\n\n\n1\n1\nunstructured data\n\n\n11\n11\nin multiple sheet with jumble data taking lot ...\n\n\n23\n23\ntime taking when we have too many sheets with ...\n\n\n\n\n\n\n\n\n\ndf_PII=pd.read_excel(r'/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/class_pii_phi_tiny_v2.xlsx',sheet_name='PIIPHI')\ndf_NOPII=pd.read_excel(r'/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/class_pii_phi_tiny_v2.xlsx',sheet_name='NOPIIPHI')\n\nThe following table showcases a collection of document labels gathered from past projects. We’ve meticulously classified them (PII/PHI OR NO-PII/PHI) for future use in process improvement and automation endeavors.\n\ndf_PII\n\n\n\n\n\n\nSamples of PII/PHI Classified Labels {#48db2008}\n\n\n\nField_Name\nClass_Label\n\n\n\n\n0\nFirst Name\nPII/PHI\n\n\n1\nLast Name\nPII/PHI\n\n\n2\nEmail\nPII/PHI\n\n\n3\nGender\nPII/PHI\n\n\n4\nBirthdate\nPII/PHI\n\n\n5\nSocial Security Number\nPII/PHI\n\n\n6\nPayment Plan Name\nPII/PHI\n\n\n7\nPayment Plan Details\nPII/PHI\n\n\n8\npatient_name_last\nPII/PHI\n\n\n9\npatient_name_first\nPII/PHI\n\n\n10\npatient_name_middle\nPII/PHI\n\n\n11\npatient_dob\nPII/PHI\n\n\n12\npatient_sex\nPII/PHI\n\n\n13\npatient_addr1\nPII/PHI\n\n\n14\npatient_addr2\nPII/PHI\n\n\n15\npatient_city\nPII/PHI\n\n\n16\npatient_state\nPII/PHI\n\n\n17\npatient_zip\nPII/PHI\n\n\n18\npatient_phone\nPII/PHI\n\n\n19\npolicyholder_name_last\nPII/PHI\n\n\n20\npolicyholder_name_first\nPII/PHI\n\n\n21\npolicyholder_name_middle\nPII/PHI\n\n\n22\npolicyholder_dob\nPII/PHI\n\n\n23\npolicyholder_addr1\nPII/PHI\n\n\n24\npolicyholder_addr2\nPII/PHI\n\n\n25\npolicyholder_city\nPII/PHI\n\n\n26\npolicyholder_state\nPII/PHI\n\n\n27\npolicyholder_zip\nPII/PHI\n\n\n28\nParent 1 First Name\nPII/PHI\n\n\n29\nParent 1 Last Name\nPII/PHI\n\n\n30\nParent 1 Email\nPII/PHI\n\n\n31\nParent 1 Mobile Number\nPII/PHI\n\n\n32\nParent 2 First Name\nPII/PHI\n\n\n33\nParent 2 Last Name\nPII/PHI\n\n\n34\nParent 2 Email\nPII/PHI\n\n\n35\nParent 2 Mobile Number\nPII/PHI\n\n\n36\nAddress\nPII/PHI\n\n\n37\nCity\nPII/PHI\n\n\n38\nState\nPII/PHI\n\n\n39\nZip\nPII/PHI\n\n\n40\nGuardian - Full Name\nPII/PHI\n\n\n41\nGuardian - Cell/Main Phone Number\nPII/PHI\n\n\n42\nGuardian - Email Address\nPII/PHI\n\n\n43\npolicynumber\nPII/PHI\n\n\n44\ngroupnumber\nPII/PHI\n\n\n45\nmedicaid_id\nPII/PHI\n\n\n46\npatient_ssn\nPII/PHI\n\n\n47\nclaimID\nPII/PHI\n\n\n48\nmrn\nPII/PHI\n\n\n49\nDOB\nPII/PHI\n\n\n50\nptName\nPII/PHI\n\n\n51\nptAddress\nPII/PHI\n\n\n52\nptAddress2\nPII/PHI\n\n\n53\ninsuranceName\nPII/PHI\n\n\n54\nSocial Security #\nPII/PHI\n\n\n\n\n\n\n\n\n\ndf_NOPII\n\n\n\n\n\n\nSamples of NO-PII/PHI Classified Labels {#69b53f60}\n\n\n\nField_Name\nClass_Label\n\n\n\n\n0\nAmount Paid\nNO-PII/PHI\n\n\n1\nchn_payorClass\nNO-PII/PHI\n\n\n2\nchnLocation\nNO-PII/PHI\n\n\n3\ncopay\nNO-PII/PHI\n\n\n4\nDiscount code\nNO-PII/PHI\n\n\n5\ndos\nNO-PII/PHI\n\n\n6\nGA member role\nNO-PII/PHI\n\n\n7\nGA name\nNO-PII/PHI\n\n\n8\ninsurance_VMID\nNO-PII/PHI\n\n\n9\nMembership Expiration Date\nNO-PII/PHI\n\n\n10\nMembership level\nNO-PII/PHI\n\n\n11\nnotes\nNO-PII/PHI\n\n\n12\nOpt-in to Newsletter\nNO-PII/PHI\n\n\n13\nOriginal Role\nNO-PII/PHI\n\n\n14\nOutstanding Balance\nNO-PII/PHI\n\n\n15\nParent 1 SMS Opt-In\nNO-PII/PHI\n\n\n16\nParent 2 SMS Opt-In\nNO-PII/PHI\n\n\n17\npat_relation_to_policyholder\nNO-PII/PHI\n\n\n18\npatient_PCP_VMID\nNO-PII/PHI\n\n\n19\nPayment\nNO-PII/PHI\n\n\n20\nPayment\nNO-PII/PHI\n\n\n21\nPhoto\nNO-PII/PHI\n\n\n22\nRegistration Date\nNO-PII/PHI\n\n\n23\nRole\nNO-PII/PHI\n\n\n24\nSmall Group\nNO-PII/PHI\n\n\n25\nStatus\nNO-PII/PHI\n\n\n26\nsubscriberNo\nNO-PII/PHI\n\n\n27\nTotal Amount\nNO-PII/PHI\n\n\n28\nWaiver Acceptance Date\nNO-PII/PHI"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#establishing-performance-variable.",
    "href": "ISI_SSBBA_Book.html#establishing-performance-variable.",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "5.2 Establishing Performance Variable.",
    "text": "5.2 Establishing Performance Variable.\nBuilding on the previous table, let’s explore the specific information recorded for each project:\n\nNumber of Files: This represents the total number of files included in a review pool.\nNumber of PII/PHI: This indicates how many files within the pool were identified as containing PII/PHI using Named Entity Recognition (NER) analysis.\nNumber of False Positives (FPS): This reflects the number of documents incorrectly classified as PII/PHI when they actually contained no PII/PHI data.\nNumber of False Negatives (FNS): This refers to the number of documents that should have been flagged as PII/PHI but were mistakenly classified as not containing such information.\nAccuracy Calculation: The accuracy metric is derived by dividing the sum of True Positives and True Negatives by the total number of documents categorized (True Positives, True Negatives, False Positives, and False Negatives).\n\nDocument classification accuracy hinges on managing False Positives (FPs) and False Negatives (FNs). By minimizing the rate of both FPs (incorrectly classifying non-PII documents as PII) and FNs (missing true PII documents), we can significantly improve overall accuracy.\nThis translates to the below key benefits:\n\nReduced Notification Delays: Fewer FPs mean fewer unnecessary notifications, streamlining the process and ensuring timely alerts for critical PII findings.\nEnhanced Analyst Efficiency: With fewer FNs, analysts spend less time investigating irrelevant documents and can focus their expertise on genuine PII cases.\nEnhanced Client Satisfaction: By minimizing errors, we deliver high-quality notification lists containing accurate PII/PHI data. This allows clients to confidently present this information to legal counsel, fostering trust and satisfaction.\n\n\ndf_ctq_tbl = pd.read_csv(r'/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/DBAS_CTQ.csv')\ndf_ctq_tbl\n\n\n\n\n\n\nPerformance Variables {#1f7e4f0f}\n\n\n\nNo#\nCTQ\nType\nObjective\nTarget\nLSL\nUSL\n\n\n\n\n0\n1\nAccuracy\nMeasurable\nImproved\n0.85\n-\n0.95\n\n\n1\n2\nNotification Delay Time\nMeasurable\nMinimized\n1Week\n-\n&lt;2 Wks."
  },
  {
    "objectID": "ISI_SSBBA_Book.html#performance-evaluation-stability-analysis.",
    "href": "ISI_SSBBA_Book.html#performance-evaluation-stability-analysis.",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "5.3 Performance Evaluation-(Stability Analysis).",
    "text": "5.3 Performance Evaluation-(Stability Analysis).\n\ndf_org = (\n    pl.read_csv(\n        r\"/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/DBAS_Projects_Data_Tidy_V3.csv\"\n    )\n    .drop([\"Started Date\", \"Complete Date\", \"NO_OF_SERVICE_WORKING_HRS\"])\n    .with_columns(pl.col(\"Project Name\").str.replace_all(\"PROJECT\", \"P\"))\n).filter(pl.col(\"accuracy_\").is_not_null())\n\ndf_org = df_org.to_pandas()\n\ndf_dbas_project = df_org.drop(\"NO_OF_DAYS\", axis=1)\n\ndf_dbas_project.columns = [\n    \"Pname\",\n    \"#Files\",\n    \"#Days\",\n    \"#Analyst\",\n    \"#PII_PHI\",\n    \"#FPS\",\n    \"#FNS\",\n    \"FPS_FNS\",\n    \"TPS_TNS\",\n    \"_Accuracy\",\n]\n\ndf_dbas_project = df_dbas_project[~df_dbas_project[\"_Accuracy\"].isna()]\n\n\n5.3.1 Checking Process Stability\n\n5.3.1.1 BoxPlot and Histogram on Performance Variable.(Accuracy)\n\n# VIS-1\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\np1 = sns.boxplot(df_dbas_project[\"_Accuracy\"], ax=ax1)\np2 = sns.histplot(df_dbas_project[\"_Accuracy\"], ax=ax2, kde=True)\np1.set_title(\"BoxPlot: Document Classification Accuracy-Assignable + Random\")\np2.set_title(\"Histogram: Document Classification Accuracy-Assignable + Random\")\nplt.suptitle(\"Stability Analysis - Document Classification Accuracy\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nInference:\nThe distribution of Accuracy values is approximately normal.\n\n\n5.3.1.2 Descriptive on Performance Variable.(Accuracy)\n\npd.DataFrame(df_dbas_project[\"_Accuracy\"].describe()).T.apply(lambda x:round(x,2))\n\n\n\n\n\n\nDescriptive Stats on Accuracy {#9105b569}\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n_Accuracy\n234.0\n0.72\n0.17\n0.34\n0.6\n0.72\n0.82\n1.0\n\n\n\n\n\n\n\n\n\n\n5.3.1.3 Run Chart on Performance Variable.(Accuracy)\n\ncreate_run_chart(df_dbas_project, \"_Accuracy\")\n\n\n\n\n\n\n\n\nInference:\nSome projects in the Run Chart have a document classification accuracy of 1.0. This might be due to the low volume of documents in these projects (less than 10 files). which will be studied further.\n\n\n\n5.3.2 Study on Assignable causes\n\ndf_dbas_project_tidy = df_dbas_project[\n    (df_dbas_project[\"#Files\"] &gt; 10) & (df_dbas_project[\"#Files\"] &lt; 10000)\n]\n\nNotes:\n\nThere are a total of 234 data points.\nOut of the 234 data points, 68 are identified as having assignable causes. These projects likely have high accuracy (1.0) due to low document volume (less than 10 files). With a smaller number of documents, it’s generally easier for analysts to achieve perfect accuracy in classification.\nWe have done an investigation on these 68 data points and tag them as ASSINGNABLE and the remaining are as RANDOM once.\nWe would now conduct a stability analysis on thes RANDOM chance data points.\n\n\n# VIS-1\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\np1 = sns.boxplot(df_dbas_project_tidy[\"_Accuracy\"], ax=ax1)\np2 = sns.histplot(df_dbas_project_tidy[\"_Accuracy\"], ax=ax2, kde=True)\np1.set_title(\"BoxPlot: Document Classification Accuracy- Random Chances Only\")\np2.set_title(\"Histogram: Document Classification Accuracy-Random Chances Only\")\nplt.suptitle(\"Stability Analysis - Document Classification Accuracy\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ncreate_run_chart(df_dbas_project_tidy, \"_Accuracy\")\n\n\n\n\n\n\n\n\n\ncustom_ols_qqplot(df_dbas_project_tidy['_Accuracy'])\n\n\n\n\n\n\n\n\n\npd.DataFrame(df_dbas_project_tidy[\"_Accuracy\"].describe()).T.apply(lambda x:round(x,2))\n\n\n\n\n\n\nDescriptive Stats on Accuracy {#e0f6f78e}\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\n_Accuracy\n166.0\n0.67\n0.14\n0.34\n0.58\n0.67\n0.79\n0.96\n\n\n\n\n\n\n\n\nInferences:\n\nCount: There are 166 data points (samples) included in the table.\nMean: The average accuracy across all data points is 0.67.\nStandard Deviation (SD): The data has a standard deviation of 0.14. This indicates the data points are somewhat clustered around the mean, but there’s also some variability.\nThe data likely follows a bell-shaped curve (normal distribution) because the mean is close to the median, and the values are spread out somewhat evenly around the center.\nMost of the accuracy values fall between 0.58 and 0.79 (between Q1 and Q3).\nThere are some outliers on both ends, with a few data points having accuracy as low as 0.34 and as high as 0.96.\n\nConclusion:\nTo assess the stability of our document review process, we removed assignable causes such as low document volume projects from the data and re-analyzed the remaining random variations in classification accuracy. This analysis of a statistically stable process will help us identify areas for improvement and ultimately enhance the accuracy of document classification and extraction for future projects."
  },
  {
    "objectID": "ISI_SSBBA_Book.html#performance-evaluation-capability-analysis.",
    "href": "ISI_SSBBA_Book.html#performance-evaluation-capability-analysis.",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "5.4 Performance Evaluation-(Capability Analysis).",
    "text": "5.4 Performance Evaluation-(Capability Analysis).\n\n5.4.1 Business Specifications.\n\nBusiness Specification\n\n\nSpec\nValue\n\n\n\n\nCTQ\nImprove Classification Accuracy\n\n\nMean\n0.67\n\n\nSD\n0.14\n\n\nLSL\n0\n\n\nUSL\n0.95\n\n\n\n\n\n5.4.2 Performance Metrics\n\nPerformance Metrics\n\n\nMetric\nValue\n\n\n\n\nTotal NonConfirmances\n0.023\n\n\nYield\n0.97725\n\n\nCP\n1.13095\n\n\nCPk\n0.66667\n\n\nSigmaLevel\n3.49998\n\n\nDPMO\n22750.98385\n\n\n\n\nYield (0.98): This indicates a very good performance with a yield.\nCapability: CP(Potential) and CPk(Achieved):\n\n\nThese CP(1.13) and CPk(0.67) values are moderate,the CP value (1.3) is greater than 1 indicates the process spread is less than the total specification allowance. This implies the process has the potential to produce accurate results within the given specifications.\nCPk is lower than CP which indicates that the process has good potential for accuracy but is not perfectly centered within the desired specification limits.\n\n\nSigma Level: The sigma level is significantly at 3.5, indicating a narrower spread of data points around the mean.\nDPMO: The Defects Per Million Opportunities (DPMO) is 22750, its reflecting a much lower rate of misclassified documents.\n\nConclusion\nThe Document Classification process can achieve the desired results,and there is a chance to improve the process by making it more centered within the specification limits, reducing the risk of producing out-of-specification items."
  },
  {
    "objectID": "ISI_SSBBA_Book.html#detailed-process-analysis",
    "href": "ISI_SSBBA_Book.html#detailed-process-analysis",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "6.1 Detailed Process Analysis",
    "text": "6.1 Detailed Process Analysis\n\n6.1.1 Research Questions.\nA. Distribution of PII/PHI Documents:\n1. What is the spread of PII/PHI documents across projects? (Are there outliers with very high or very low PII/PHI proportions?)\n2. Are there different proportions of PII/PHI documents based on project type or other categories?\n\nsns.histplot(DF_SSBB_TIDY[\"prop_of_pii_phi\"])\nplt.title('Proportion of PII/PHI Classified Document')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nThe distribution of PII/PHI documents appears to be relatively normal, centered around a proportion of 0.85. This indicates that most projects have a similar proportion of documents containing PII/PHI.\nHowever, there is also some variation, with proportions ranging from approximately 0.75 to 0.98. This suggests that a few projects may have a significantly higher or lower proportion of PII/PHI documents compared to the average.\nB. PII/PHI Files vs. Total Project Files:\n3. Is there a correlation between the number of PII/PHI files and the total number of project files?(Are larger projects more likely to have more PII/PHI documents?)\n\nsns.scatterplot(DF_SSBB_TIDY, x=\"#Files\", y=\"#PII_PHI\")\nplt.title('Number of Files Vs. Number of PII_PHI classified docs')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nYes, there appears to be a positive correlation between the number of PII/PHI files and the total number of project files based on the scatter plot. This suggests that projects with a larger number of documents tend to have a higher number of PII/PHI files. However, the scatter plot doesn’t necessarily confirm that projects with fewer documents will have fewer PII/PHI files. There could be outliers or projects with specific characteristics that deviate from this trend.\nC. PII/PHI Volume and Notification List Delivery Time:\n4. Does the number of PII/PHI documents in a project impact the time it takes to deliver a notification list?\n\nsns.scatterplot(DF_SSBB_TIDY, y=\"#Days\", x=\"#PII_PHI\")\nplt.title('Number of PII_PHI classified docs Vs No.of Days')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nThere appears to be a positive correlation between the number of PII/PHI documents in a project and the time it takes to deliver a notification list. This is likely because processing a larger volume of documents naturally takes more time. As seen in the above visualization, projects with 4000, 5000, or 8000 documents tend to have longer processing times.\nHowever, the number of documents isn’t the sole factor. Even smaller projects (below 1000 documents) can experience extended processing times. This could be due to the complexity of the data, such as:\nVaried formats: Having documents in different formats (e.g., PDFs, spreadsheets, text files) might require additional steps to convert or extract the data consistently.\nLarge volumes of observations: Even within a smaller number of documents, a high number of data points (e.g., many rows in a spreadsheet) can extend processing time.\nComplex extractions/validations: If the PII/PHI data extraction process is intricate or requires extensive validation to ensure accuracy, it can add to the overall time.\nIn conclusion, while the number of documents plays a role, the complexity of the data within those documents also significantly impacts notification list delivery time.\nD. Analyst Allocation and PII/PHI Volume:\n5. How are analysts assigned to projects? (Is there a system based on project complexity or PII/PHI volume?)\n6. Do projects with a higher volume of PII/PHI documents have more analysts assigned? (Compare analyst allocation across projects with varying PII/PHI volume)\n\nsns.scatterplot(DF_SSBB_TIDY, y=\"#Analyst\", x=\"#PII_PHI\")\nplt.title('Number of PII_PHI classified docs Vs No.of Analysts.')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nThe provided visualization suggests a possible correlation between the number of project documents and the number of analysts assigned. Projects with a larger volume of documents (likely exceeding 2000) tend to have more analysts allocated. This makes sense as it allows analysts to dedicate sufficient time for thorough processing.\nHowever, it’s important to consider that the number of documents might not be the sole factor influencing analyst allocation. Other factors like project complexity or specific PII/PHI content might also play a role.\nE. PII Volume and Document Classification Accuracy:\n7. Is there a relationship between the number of PII/PHI documents and the accuracy of document classification? (Does a higher volume lead to lower accuracy?)\n\nsns.scatterplot(DF_SSBB_TIDY, y=\"Accuracy\", x=\"#PII_PHI\")\nplt.title('Number of PII_PHI classified docs Vs Accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nThe number of PII/PHI documents may not directly impact document classification accuracy. While a smaller volume of documents can lead to lower accuracy due to limited training data, larger projects can also experience accuracy issues for other reasons. In most cases, large projects haven’t shown a significant decrease in accuracy.\nF. Number of Analysts and Classification Accuracy:\n8. Does involving more analysts in PII/PHI reviews improve classification accuracy? (Visualize the relationship between analyst count and accuracy)\n\nsns.scatterplot(DF_SSBB_TIDY, y=\"Accuracy\", x=\"#Analyst\")\nplt.title('Number of Analysts Vs Accuracy')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nInvolving more analysts in PII/PHI reviews may not directly correlate to improved classification accuracy. While a limited number of analysts might lead to lower accuracy due to workload or lack of diverse perspectives, simply adding more analysts isn’t a guaranteed solution. In most cases, the number of analysts on a project hasn’t significantly impacted overall accuracy.\nG. False Positives vs. False Negatives:\n9. What is the distribution of false positives and false negatives in PII/PHI classification? (Are there more of one type of error?)\n\nsns.countplot(DF_SSBB_TIDY,x=\"FPS_vs_FNS\")\nplt.title('Are FalsePostive higher than False Negatives?')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nI analyzed the results and categorized them based on whether there were more missed PII/PHI instances (false negatives) or mistakenly identified ones (false positives).\nFrom the visualization, it appears that there are more false negatives than false positives. This means the analysts might be missing more relevant PII/PHI data than incorrectly identifying non-PII/PHI data. However, further statistical analysis is recommended for a more conclusive picture.\nH. True vs. False Classifications:\n10. How do the volumes of correctly classified documents (true positives) compare to incorrectly classified documents (false positives and negatives)?\n\nsns.countplot(DF_SSBB_TIDY,x=\"FPFN_vs_TPTN\")\nplt.title('Are TrueClassifications higher than False Classification?')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nI have categorized the observations into two groups based on the sum:\nMore correct classifications (True Positives & True Negatives) than incorrect ones (False Positives & False Negatives): This would be labeled as “Correct(TPS_TNS) &gt; Incorrect(FPS_FNS)”.\nMore incorrect classifications than correct ones: This would be labeled as “Incorrect(FPS_FNS) &gt; Correct(TPS_TNS)”.\nBased on this analysis, it appears that there are significantly more correctly classified documents (identified as PII/PHI when they are, and not identified as PII/PHI when they aren’t) compared to documents that are classified incorrectly (mistakenly identified or missed).\nI. Project Completion Time:\n11. What is the distribution of project completion times (number of weeks)? (Are there outliers with very long or short completion times?)\n\nsns.countplot(\n    DF_SSBB_TIDY,\n    x=\"weeks_bucket\",\n    order=DF_SSBB_TIDY[\"weeks_bucket\"].value_counts().index,\n)\nplt.title('No of Projects Completed for in each categorized week bucket.')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nOur analysis shows that the majority of projects (around 80%) are completed within a short timeframe of 2 weeks. This suggests a well-defined and efficient process for these projects.\nJ. Volume of Classified PII/PHI Documents:\n12. What is the total volume of PII/PHI documents that have been classified? (This provides a general sense of workload)\n\nsns.countplot(\n    DF_SSBB_TIDY,\n    x=\"pii_phi_bucket\",\n    order=DF_SSBB_TIDY[\"pii_phi_bucket\"].value_counts().index,\n)\nplt.title('No of PII/PHI Classified Buckets')\nplt.show()\n\n\n\n\n\n\n\n\nInferences:\nWe’ve classified a high volume of PII/PHI documents - approximately 95% fall within a range of up to 5,000 documents. This gives us a good idea of the overall workload involved in the PII/PHI classification process.\n\n\n6.1.2 Statistical Test/Inferences on Research Questions.\nA.Hypothesis #1.\nNull Hypothesis(H0):The average Accuracy of PII/PHI document classification is 0.67.\nAlternate Hypothesis(HA):The average Accuracy of PII/PHI document classification is not equal to 0.67\n\n\n\nTable 3: OneSample T-test(Two Sided)\n\n\n\n\n\nParameters\nValue\n\n\n\n\nt-statistic\n0.09\n\n\np-value\n0.92\n\n\nCI at 95%\n(0.64,0.69)\n\n\n\n\n\n\nInferences:\nOne sample T-test is carried out to test this hypothesis, here p-value(0.92) suggest that we failed to reject the null hypothesis. The data doesn’t provide strong enough evidence to say the average accuracy is definitively different from 0.67.\nAnd the lower t-statistic(0.09) also indicates that the observed average accuracy is very close to the hypothesized value (0.67).\nB.Hypothesis #2.\nNull Hypothesis(H0):The average Proportion of PII/PHI documents is 0.8.\nAlternate Hypothesis(HA):The average Proportion of PII/PHI documents is more than 0.8\n\n\n\nTable 4: OneSample T-test(Right tailed)\n\n\n\n\n\nParameters\nValue\n\n\n\n\nt-statistic\n21.01\n\n\np-value\n0.0\n\n\nCI at 95%\n(0.88,-)\n\n\n\n\n\n\nInferences:\nOne Sample t-test (right tailed) test is carried out to see if the average proportion of PII/PHI documents is more than 0.8 or not, the p-value(0.0) suggest that we can reject the null hypothesis in favor of alternate, and the data provides strong evidence to suggest that the average proportion of PII/PHI documents is significantly higher than the initially assumed value of 0.8.\nC.Hypothesis #3.\nNull Hypothesis(H0):The average accuracy for FNS&gt;FPS is equal to the average accuracy for FPS&gt;FNS (there’s no difference).\nAlternate Hypothesis(HA):The average accuracy for FNS&gt;FPS is different from the average accuracy for FPS&gt;FNS.\n\n\n\nTable 5: TwoSample T-test\n\n\n\n\n\nParameters\nValue\n\n\n\n\nt-statistic\n-4.64\n\n\np-value\n0.0\n\n\n\n\n\n\nInferences:\nI have conducted a two-sample t-test to compare the accuracy between these two scenarios. The resulting p-value of 0.0 indicates a statistically significant difference. The data strongly suggests that the average accuracy for FNS&gt;FPS scenarios is not the same as the average accuracy for FPS&gt;FNS scenarios. There’s a difference in how well the classification performs depending on whether there are more missed PII/PHI instances or more incorrectly identified ones.\nD.Hypothesis #4.\nNull Hypothesis(H0):There is NO difference between the average accuracy of TPS_TNS&gt;FPS_FNS and FPS_FNS&gt;TPS_TNS in classfication\nAlternate Hypothesis(HA):There is a difference between the accuracy of TPS_TNS&gt;FPS_FNS and FPS_FNS&gt;TPS_TNS in classfication\n\n\n\nTable 6: TwoSample T-test\n\n\n\n\n\nParameters\nValue\n\n\n\n\nt-statistic\n10.22\n\n\np-value\n0.0\n\n\n\n\n\n\nInferences:\nI have conducted a two-sample t-test to compare the accuracy between these two scenarios. The resulting p-value of 0.0 indicates a statistically significant difference. The data strongly suggests that the average accuracy for TPS_TNS&gt;FPS_FNS scenarios is not the same as the average accuracy for FPS_FNS&gt;TPS_TNS scenarios. There’s a difference in how well the classification performs depending on whether there are more missed PII/PHI instances or more incorrectly identified ones.\nE.Hypothesis #5.\nNull Hypothesis(H0):The true proportion of classifications with FNS&gt;FPS is 0.6 \nAlternate Hypothesis(HA):The true proportion of classifications with FNS&gt;FPS is not equal to 0.6\n\n\n\nTable 7: OneProportion Z-test\n\n\n\n\n\nParameters\nValue\n\n\n\n\nZ-statistic\n0.06\n\n\np-value\n0.94\n\n\n\n\n\n\nInferences:\nI have a one-proportion z-test to analyze the data. The resulting p-value of 0.94 indicates that we fail to reject the null hypothesis, bbased on the data, we don’t have strong evidence to say the true proportion of FNS&gt;FPS classifications is definitively different from 0.6.\nF. Hypothesis #6.\nNull Hypothesis(H0):The proportion of False Positives (FPS) compared to False Negatives (FNS) is the same across both categories: documents with more False Positives & False Negatives (FPFN) and documents with more True Positives & True Negatives (TPTN).\nAlternate Hypothesis(HA):The proportion of FPS compared to FNS is not the same across the two categories (FPFN vs. TPTN).\n\nsns.countplot(DF_SSBB_TIDY,x=\"FPFN_vs_TPTN\",hue=\"FPS_vs_FNS\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTable 8: TwoProportion Z-test\n\n\n\n\n\nParameters\nValue\n\n\n\n\nZ-statistic\n8.62\n\n\np-value\n0.00\n\n\n\n\n\n\nInferences:\nThe Two-Sample Z-test was conducted to compare the proportion of False Positives (FPS) vs. False Negatives (FNS) across two categories: documents with more False Positives & False Negatives (FPFN) and documents with more True Positives & True Negatives (TPTN).\nThe resulting p-value of 0.0 suggests a statistically significant difference between the two categories. In other words, the proportion of FPS compared to FNS is not the same for documents with high FPFN vs. documents with high TPTN.\nG.Hypothesis #7.\nNull Hypothesis(H0): There is no association between the number of PII/PHI category buckets a project belongs to and its finishing time in weeks. In other words, project finishing time is the same across all PII/PHI category buckets. Alternate Hypothesis(HA): There is an association between the number of PII/PHI category buckets and project finishing time. This means projects with different numbers of PII/PHI categories might have different average finishing times in weeks.\n\nsns.countplot(\n    DF_SSBB_TIDY[\n        (DF_SSBB_TIDY[\"pii_phi_bucket\"] != \"more than 5K\")\n        & (DF_SSBB_TIDY[\"weeks_bucket\"] != \"more than 4 weeks\")\n        & (DF_SSBB_TIDY[\"weeks_bucket\"] != \"4W\")\n    ],\n    x=\"pii_phi_bucket\",\n    hue=\"weeks_bucket\",\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTable 9: Chi-Square Test Association\n\n\n\n\n\nParameters\nValue\n\n\n\n\nChiSquare\n83.62\n\n\np-value\n0.00\n\n\n\n\n\n\nInferences:\nThe Chi-Square test was conducted to investigate a possible association between the number of PII/PHI category buckets in a project and its finishing time (weeks).\nThe results show a statistically significant p-value (0.0), which allows us to reject the null hypothesis.it means there’s evidence of a relationship between the number of PII/PHI categories and project finishing time. Projects with different numbers of PII/PHI categories might, on average, take different amounts of time to complete in weeks.\nH.Hypothesis #8.\nNull Hypothesis(H0):On average, the Classification accuracy of a project isn’t affected by its the size of PII/PHI volumes\nAlternate Hypothesis(HA):On average, the Classification accuracy of a project is affected by its the size of PII/PHI volumes\n\nsns.boxplot(x='pii_phi_bucket',y='Accuracy',data=DF_SSBB_TIDY)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define the model formula\ndf_3g_model = ols(\n    \"Accuracy ~ pii_phi_bucket\", data=DF_SSBB_TIDY\n).fit()\n\n# Perform ANOVA\nanova_table_3g = anova_lm(df_3g_model)\n\n\nanova_table_3g.apply(lambda x:round(x,5))\n\n\n\n\n\n\nANOVA TABLE {#bc6e9960}\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\npii_phi_bucket\n4.0\n0.21050\n0.05262\n2.75543\n0.02982\n\n\nResidual\n161.0\n3.07482\n0.01910\nNaN\nNaN\n\n\n\n\n\n\n\n\nInferences:\nI have conducted a one-way ANOVA test to investigate the relationship between the volume of PII/PHI documents in a project and its average classification accuracy.\nThe resulting p-value of 0.02 suggests a statistically significant difference. This means we can reject the null hypothesis. In simpler terms, the data indicates that the size of PII/PHI volumes does, on average, affect the classification accuracy of projects.\n\n\n\nConclusions\n\n\n\nFrom the above statistical tests result we can conclude that these results provide valuable insights for improving PII/PHI classification.\n\nThe average accuracy might be hovering around 0.67, but there’s room for improvement.\nThere’s a higher proportion of PII/PHI documents than initially expected.\nThe classification performs differently depending on the type of error (missed vs. incorrect PII/PHI).\nThe proportion of errors (False Positives vs. False Negatives) also varies based on document characteristics.\nProject size (PII/PHI volume) can impact classification accuracy.\n\n\n\n6.1.3 Study on Root Causes using text mining techniques.\nBased on an internal survey, we analyzed feedback from our team of analysts regarding the challenges they face during manual PII/PHI extraction from various files.\nHere’s a breakdown of the key themes showed in the wordcloud maps.\n\n6.1.3.1 Actions.\n\n_generate_word_cloud(text_verb) \n\n\n\n\n\n\n\n\n1.Extract: This is the most prominent action analysts struggle with. They likely encounter difficulties in efficiently extracting the desired PII/PHI data from the files.\n2.Manage: This suggests analysts may have trouble managing large volumes of files or complex data structures while performing extractions.\n3.Handle: This indicates potential challenges in handling diverse file formats or unstructured data that requires additional processing.\n\n\n6.1.3.2 Nouns.\n\n_generate_word_cloud(text_noun) \n\n\n\n\n\n\n\n\n1.Data: This is the core focus of their work, with analysts primarily dealing with PII/PHI data.\n2.Files: The feedback indicates that the analysts process a significant number of files containing the PII/PHI.\n3.Sheets: This could refer to spreadsheets within the files or separate sheets containing the PII/PHI data. Analysts might be struggling with extracting data from multiple sheets or inconsistent sheet formats.\n\n\n6.1.3.3 Adjectives.\n\n_generate_word_cloud(text_adj) \n\n\n\n\n\n\n\n\n1.Large: The volume of data or number of files may be overwhelming, making manual extraction time-consuming.\n2.Unstructured: The data format might not be organized in a way that’s easily digestible for automated extraction, requiring manual intervention.\n3.Different/Diverse: This suggests analysts encounter various file formats or data structures, making consistent extraction challenging.\n4.Accurate: Accuracy is likely a major concern, as ensuring they capture the correct PII/PHI is crucial.\nOverall, the feedback highlights the analyst’s struggle with the volume, format, and complexity of data when manually extracting PII/PHI. This information can help us identify solutions for streamlining the extraction process and improving their efficiency.\n\n\n\n6.1.4 FMEA\n\n\n\nDBAS-FMEA\n\n\n\n\n6.1.5 Pareto Analysis\nLet’s identify the biggest contributors to low document classification accuracy.\nWe can use Pareto analysis, a technique that helps prioritize tasks based on their impact. It follows the 80/20 rule, where a small percentage (often 20%) of causes contribute to a large percentage (often 80%) of the effect.\nHere are the potential causes for lower accuracy:\n\nManual Extraction of data\nManual Classification of documents\nInadequate training and process documentation\nInefficient Data De-duplication Algorithm\nInefficient Data Quality Control Algorithm\n\nBy plotting these causes on a Pareto chart, we can see which ones have the biggest impact on accuracy.\n\n\n\nDBAS-PARETO\n\n\nLooking at the Pareto chart, we can see that manual tasks (including Classification and Extraction) contribute the most (60%) to the low accuracy. Lack of proper training materials and process documentation (20%) is another significant factor.\n\n\n6.1.6 Summary: Potential Variation Sources(Root Causes)\n\n\n\nTable 10: Root Causes\n\n\n\n\n\n\n\n\n\n\nS.NO\nRootCause\n\n\n\n\n\n1\nManual tasks (including Classification and Extraction)\n\n\n\n2\nStruggle with volume, format, and complexity of data when manually extracting PII/PHI.\n\n\n\n3\nLack of proper training materials and process documentation"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#discover-variable-relationships",
    "href": "ISI_SSBBA_Book.html#discover-variable-relationships",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "7.1 Discover Variable Relationships",
    "text": "7.1 Discover Variable Relationships\nIn the improvement phase of Six Sigma, we’re investigating automation opportunities for document classification and extraction tasks, which are currently handled manually.\nWe have data from past projects that holds valuable insights. This data includes the fields extracted from each document and the assigned classification labels. Notably, a team of analysts meticulously identified the header information and its corresponding labels.\nBy leveraging this labeled data, we can build a powerful classification model. This model will automate the process of classifying headers within future project files, significantly reducing manual effort and improving efficiency.\nTo gain insights into the data we’ll use for building the predictive model, we’ll perform some initial explorations:\n\nAnalyze header field text lengths to understand their distribution.\nInvestigate the number of words within each header.\nExplore word frequencies and usage patterns within the headers.\nExamine the class proportions for PII/PHI and Non-PII/PHI labels.\n\nThe data in this table is a representative sample that will be used to build the predictive model.\n\ndf_pii_phi_raw[ (df_pii_phi_raw[\"tidy_field_len\"]&lt;10) ].sample(10)\n\n\n\n\n\n\nDBAS DATA- PII/PHI and NO/PHI {#d81af647}\n\n\n\n_dbas_field_name\nclass_label\n_dbas_field_name_tidy\ntidy_field_len\nno_of_words\n\n\n\n\n5088\nNew_Pt\n1\nnew pt\n6\n2\n\n\n4317\nCIN\n0\ncin\n3\n1\n\n\n8921\nEarnTot03\n0\nearntot03\n9\n1\n\n\n5480\nPt_name\n1\npt name\n7\n2\n\n\n1935\nMD(2)\n0\nmd 2\n4\n2\n\n\n11279\nPayeeSup\n0\npayeesup\n8\n1\n\n\n6001\navg_hours_\n0\navg hours\n9\n2\n\n\n9488\nField176\n0\nfield176\n8\n1\n\n\n1124\nECCRMANum\n0\neccrmanum\n9\n1\n\n\n8294\nCR_2010\n0\ncr 2010\n7\n2\n\n\n\n\n\n\n\n\nHere is the data dictionary.\ndbas_field_name (Original Header): This field stores the raw header text extracted from the project file. It might contain inconsistencies or extraneous characters.\nclass_label (PII/PHI Classification): This field indicates whether the header likely contains Personally Identifiable Information (PII) or Protected Health Information (PHI). The value is binary:\n- 1: Represents a header containing PII/PHI (sensitive data).\n- 0: Represents a header containing Non-PII/PHI (non-sensitive data).\ndbas_field_name_tidy (Cleaned Header): This field is a cleaned version of the original header (dbas_field_name). It has undergone processing to remove unwanted characters, formatting inconsistencies, or extra spaces. This cleaned header is used for further analysis and model training.\ntidy_field_len (Clean Header Length): This field represents the number of characters in the cleaned header (dbas_field_name_tidy). It reflects the length of the header after any unnecessary characters or spaces are removed during the cleaning process.\nno_of_words (Number of Words): This field indicates the number of words present in the cleaned header (dbas_field_name_tidy). It provides insights into the overall complexity of the header and can be useful for feature engineering in the model building process.\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\np1 = sns.histplot(df_pii_phi_raw[\"tidy_field_len\"],ax=ax1)\np2 = sns.boxplot(df_pii_phi_raw[\"tidy_field_len\"],ax=ax2)\np2.set_title(\"BoxPlot\")\np1.set_title(\"Histogram\")\nplt.suptitle(\"Header Field Text Length Distribution\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nOur initial exploration focused on the distribution of text lengths within the header fields. Visualizations like box plots and histograms above revealed that:\n\n80% of the headers fall within a length range of 2 to 20 characters. This suggests a prevalence of relatively concise headers.\nThe remaining 20% of headers exhibit lengths exceeding 20 characters. These longer headers warrant further investigation.\n\nTo understand the potential sensitivity of information within these longer headers, we’ll delve deeper. We’ll examine whether there’s a correlation between header length and the presence of PII/PHI and this analysis will help us determine if longer headers are more likely to contain sensitive data.\n\npd.DataFrame(df_pii_phi_raw[df_pii_phi_raw[\"tidy_field_len\"] &gt; 20][\"class_label\"].value_counts()).reset_index()\n\n\n\n\n\n\nAssignable Causes Per Each Classification {#88df9149}\n\n\n\nclass_label\ncount\n\n\n\n\n0\n1\n309\n\n\n1\n0\n175\n\n\n\n\n\n\n\n\n\nsns.boxplot(x='class_label',y='tidy_field_len',data=df_pii_phi_raw)\nplt.title(\"Text length per each of classification label PII/PHI and NO-PII/PHI\")\nplt.show()\n\n\n\n\n\n\n\n\nOur analysis of the data revealed that 484 observations have a text length exceeding 20 characters. Among these longer headers:\n\n65% contain PII/PHI (Personally Identifiable Information/Protected Health Information). This indicates a significant presence of sensitive data within these headers.\nThe remaining 35% do not contain any PII/PHI content.\n\nWe would proceed with next steps as below:\nRetain the 65% of headers containing PII/PHI: These headers are valuable for training our model to identify sensitive information.\nFurther investigate the non-PII/PHI headers (35%): We can explore these headers to understand why they were classified as non-sensitive despite their length. This might involve manually reviewing a sample or applying additional criteria to refine the classification.\nDecision on Non-PII/PHI headers: Based on the investigation, we can determine whether to keep these headers in the training data or remove them. Factors to consider might include their informativeness for the model and potential redundancy.\n\ndf_pii_phi_tidy = pd.concat(\n    [\n        df_pii_phi_raw[df_pii_phi_raw[\"tidy_field_len\"] &lt;= 20],\n        df_pii_phi_raw[\n            (\n                (df_pii_phi_raw[\"tidy_field_len\"] &gt; 20)\n                & (df_pii_phi_raw[\"tidy_field_len\"] &lt; 50)\n            )\n            & (df_pii_phi_raw[\"class_label\"] == 1)\n        ],\n    ]\n)\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\np1 = sns.histplot(df_pii_phi_tidy[\"tidy_field_len\"],ax=ax1)\np2 = sns.boxplot(df_pii_phi_tidy[\"tidy_field_len\"],ax=ax2)\np2.set_title(\"BoxPlot:After removing the assignable causes\")\np1.set_title(\"Histogram:After removing the assignable causes\")\nplt.suptitle(\"Header Field Text Length Distribution-Tidy Format\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nRemoving assignable causes from the non-PII/PHI text has resulted in a desirable and stable text length distribution. This suggests that the original length variations were likely due to these assignable causes. The cleaned text lengths are now more consistent and suitable for further analysis or model training.\n\nplt.style.use('classic')\ndf_no_of_words = df_pii_phi_tidy[\"no_of_words\"].value_counts().reset_index()\n# sns.barplot(df_no_of_words, x=\"no_of_words\", y=\"count\")\n# plt.title(\"Number of Words Distribution\")\n# plt.show()\n# Pareto Diagram\nsorted_data = df_no_of_words.sort_values(by=[\"count\"], ascending=False)\nsorted_data[\"cumulative_freq\"] = sorted_data[\"count\"].cumsum()\nsorted_data[\"cumulative_pct\"] = (\n    sorted_data[\"cumulative_freq\"] / sorted_data[\"count\"].sum() * 100\n)\n\n# Visualizations\nfig, ax1 = plt.subplots(figsize=(22, 8))\nax2 = ax1.twinx()\nax1.bar(sorted_data[\"no_of_words\"], sorted_data[\"count\"], color=\"blue\")\nax2.plot(\n    sorted_data[\"no_of_words\"],\n    sorted_data[\"cumulative_pct\"],\n    color=\"red\",\n    marker=\"o\",\n    linestyle=\"--\",\n)\nax1.set_xlabel(\"Category\")\nax1.set_ylabel(\"Frequency\", color=\"green\")\nax2.set_ylabel(\"Cumulative Percentage\", color=\"red\")\nplt.title(\"Pareto Analysis-No of Words\")\nplt.show()\nsns.set_theme(\"notebook\", style=\"whitegrid\")\n\n\n\n\n\n\n\n\nInferences\nOur Pareto analysis revealed as,\n\n80% of observations have headers containing only one or two words.\n97% of observations have headers with one to four words.\nImplications for Text-to-Numerical Conversion: These findings suggest that unigrams (single words) and bigrams (two-word phrases) might be sufficient for capturing most of the information within the headers when converting text to numerical features for our model. While trigrams (three-word phrases) could be explored, the high prevalence of shorter phrases suggests that unigrams and bigrams might be a good starting point to balance model complexity and performance.\n\nThe below two tables shows the header text of PII and NO-PII/PHI classes separately.\n\ndf_pii_phi_tidy[(df_pii_phi_tidy[\"class_label\"] == 0) & (df_pii_phi_tidy[\"tidy_field_len\"]&lt;10)].sample(10)\n\n\n\n\n\n\nSample Data:NO/PII-PHI {#00609d06}\n\n\n\n_dbas_field_name\nclass_label\n_dbas_field_name_tidy\ntidy_field_len\nno_of_words\n\n\n\n\n8350\nCUR SAL\n0\ncur sal\n7\n2\n\n\n14095\nYOB_1984\n0\nyob 1984\n8\n2\n\n\n4274\nTier\n0\ntier\n4\n1\n\n\n10321\nJandSPerc\n0\njandsperc\n9\n1\n\n\n7423\nasofdate\n0\nasofdate\n8\n1\n\n\n13772\nvac hrs\n0\nvac hrs\n7\n2\n\n\n13473\nTotalHrs??\n0\ntotalhrs\n8\n1\n\n\n4142\nWILLIAM\n0\nwilliam\n7\n1\n\n\n4271\ntDob\n0\ntdob\n4\n1\n\n\n1457\nhicn\n0\nhicn\n4\n1\n\n\n\n\n\n\n\n\n\ndf_pii_phi_tidy[(df_pii_phi_tidy[\"class_label\"] == 1) & (df_pii_phi_tidy[\"tidy_field_len\"]&lt;10)].sample(10)\n\n\n\n\n\n\nSample Data:PII-PHI {#ca33fa10}\n\n\n\n_dbas_field_name\nclass_label\n_dbas_field_name_tidy\ntidy_field_len\nno_of_words\n\n\n\n\n1272\nFFZip\n1\nffzip\n5\n1\n\n\n6790\nss\n1\nss\n2\n1\n\n\n4538\nAddress\n1\naddress\n7\n1\n\n\n8742\nDOBa17\n1\ndoba17\n6\n1\n\n\n5849\naddr2\n1\naddr2\n5\n1\n\n\n12983\nSS#\n1\nss\n2\n1\n\n\n5534\nzipcode\n1\nzipcode\n7\n1\n\n\n2373\nPayBTCity\n1\npaybtcity\n9\n1\n\n\n11140\nP_ID#\n1\np id\n4\n2\n\n\n6229\ncity_\n1\ncity\n4\n1\n\n\n\n\n\n\n\n\nIn order to understand the significance of words within each document and their distinctiveness across all documents, we’ll utilize TF-IDF.\nTF-IDF stands for Term Frequency-Inverse Document Frequency. It’s a way to evaluate how important a word is to a document in a collection (corpus). It considers two factors:\nTerm Frequency (TF): How often a word appears in a specific document.\nInverse Document Frequency (IDF): How common the word is across all documents in the corpus. Words that appear frequently everywhere are considered less informative (low IDF).\nBy combining these, TF-IDF gives more weight to words that are specific and relevant to a particular document.\nAs as example we will consider the below table to understand the importance of TF-IDF.\n\npd.DataFrame(\n    {\n        \"field\": [\n            \"firstname\",\n            \"lastname\",\n            \"lastname\",\n            \"socialsecuritynumber\",\n            \"ssn\",\n            \"socialsecuritynumber\",\n            \"address1\",\n            \"dateofbirth\",\n            \"dateofbirth\",\n            \"dateofbirth\",\n        ],\n        \"tf-idf\":[2.3, 1.61, 1.61, 1.61, 2.3, 1.61, 2.3, 1.2, 1.2, 1.2]\n    }\n).sort_values('tf-idf',ascending=False)\n\n\n\n\n\n\nTF-IDF DEMO. {#4896aefa}\n\n\n\nfield\ntf-idf\n\n\n\n\n0\nfirstname\n2.30\n\n\n4\nssn\n2.30\n\n\n6\naddress1\n2.30\n\n\n1\nlastname\n1.61\n\n\n2\nlastname\n1.61\n\n\n3\nsocialsecuritynumber\n1.61\n\n\n5\nsocialsecuritynumber\n1.61\n\n\n7\ndateofbirth\n1.20\n\n\n8\ndateofbirth\n1.20\n\n\n9\ndateofbirth\n1.20\n\n\n\n\n\n\n\n\nInferences\nOur exploration revealed interesting insights about Term Frequency-Inverse Document Frequency (TF-IDF) values considering the above example table:\nUnique Words and High TF-IDF: Words like “firstname,” “ssn,” and “address1” appeared only once in the corpus, resulting in high TF-IDF values (around 2.30). This is because: - Their TF (Term Frequency) is 1 (appearing once). - Their IDF (Inverse Document Frequency) is likely high because they are uncommon across the documents.\nRepeated Words and Lower TF-IDF: The word “dateofbirth” appeared three times, leading to a lower TF-IDF value (around 1.20). While it still occurs in each document, its higher TF is balanced by a potentially lower IDF due to its presence in multiple documents.\nIn essence, TF-IDF considers both how often a word appears within a document (TF) and how uncommon it is across the entire document collection (IDF). Words appearing only once tend to have higher TF-IDF because their rarity across documents (high IDF) outweighs their single occurrence within a specific document (TF=1).\n\npd.DataFrame({'tf_idf':computeTFIDF(df_pii_phi_tidy[\"_dbas_field_name_tidy\"])}).describe().reset_index().T\n\n\n\n\n\n\nDescriptive Stats on TI-IDF. {#cb1b441a}\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\nindex\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\ntf_idf\n25433.0\n4.27692\n3.132489\n0.37\n1.89\n2.88\n7.46\n9.54\n\n\n\n\n\n\n\n\n\nsns.histplot(computeTFIDF(df_pii_phi_tidy[\"_dbas_field_name_tidy\"]))\nplt.title(\"TF-IDF Distributions\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create a figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\np1 = sns.histplot(\n    computeTFIDF(\n        df_pii_phi_tidy[df_pii_phi_tidy[\"class_label\"] == 1][\"_dbas_field_name_tidy\"]\n    ),ax=ax1\n)\np2 = sns.histplot(\n    computeTFIDF(\n        df_pii_phi_tidy[df_pii_phi_tidy[\"class_label\"] == 0][\"_dbas_field_name_tidy\"]\n    ),ax=ax2\n)\np1.set_title(\"Histogram: TF-IDF for PII/PHI\")\np2.set_title(\"Histogram: TF-IDF for NO PII-PHI \")\n\nplt.suptitle(\"TF-IDF Distributions\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nInferences\nAnalysis of TF-IDF Distribution as follows,\nAverage TF-IDF: The average TF-IDF value within the corpus is 4.27. This indicates that, on average, words tend to be somewhat specific and informative in relation to the documents they appear in.\nStandard Deviation: The standard deviation of TF-IDF values is 3.13. This suggests a significant spread in TF-IDF values, implying a variety of word specificities within the corpus.\nWord Distribution: We observed that approximately 50% of the words have a TF-IDF value below 3. This suggests a notable presence of repetitive words across the corpus. These words likely contribute less to the overall informational content of individual documents.\nCorpus Composition: Based on the TF-IDF distribution, the corpus appears to be a mixture of unique and non-unique words. The high average TF-IDF and standard deviation indicate the presence of both specific terms and frequently occurring words."
  },
  {
    "objectID": "ISI_SSBBA_Book.html#develop-potential-solutions",
    "href": "ISI_SSBBA_Book.html#develop-potential-solutions",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "7.2 Develop Potential Solutions",
    "text": "7.2 Develop Potential Solutions\n\n7.2.1 MachineLearning Modeling-Classification Problem.\n\n7.2.1.1 Data Preparations.\nText Preprocessing and Feature Engineering for Machine Learning Model.\nIn our machine learning model development process, we’ll focus on the dbas_field_name_tidy field from the dataset. This field contains the header text, which serves as our input or predictor variable. However, raw text data isn’t directly usable by machine learning algorithms.\nFeature Engineering: Text to Numerical Conversion.\nTo address this challenge, we’ll perform feature engineering by converting the text data in dbas_field_name_tidy into numerical features. We’ll employ TF-IDF (Term Frequency-Inverse Document Frequency) Vectorizer specifically tailored for bigrams (two-word phrases). This technique considers both the frequency of words within a document (TF) and their rarity across the entire dataset (IDF). By focusing on bigrams, we leverage the insights from our earlier analysis indicating a predominance of short phrases within the header text.\nTarget Variable\nOur model aims to perform a binary classification task. The target variable, named classlabel, is a two-class variable containing values of 0 or 1. These values correspond to the presence or absence of PII and PHI within the data:\n\n0: Represents data that does NOT contain PII/PHI.\n1: Represents data that contains PII/PHI\n\n\ndbas_corpus = df_pii_phi_tidy[\"_dbas_field_name_tidy\"]\ndbas_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=\"english\")\ndbas_pii_phi_vectors = dbas_vectorizer.fit_transform(dbas_corpus)\n\n\ny_target = df_pii_phi_tidy[\"class_label\"]\n\n\n\n7.2.1.2 Model Validation:\nOur machine learning project utilizes a dataset containing approximately 13,000 observations. As described earlier, these observations possess clearly defined input and output variables.\nIn machine learning, we train models on input data. The model learns patterns and relationships within the data and uses these to make predictions on unseen examples. While a model might perform well on the data it’s trained on, its performance on unseen data is crucial.\nHere’s where the concepts of overfitting and underfitting come into play:\nOverfitting: This occurs when a model becomes too attuned to the specific training data and fails to generalize well to unseen examples. It essentially “memorizes” the training data instead of learning the underlying patterns.\nUnderfitting: This happens when a model is too simple and lacks the capacity to capture the essential relationships within the training data. It leads to poor performance on both the training data and unseen data.\nMitigating Overfitting and Underfitting:\nFortunately, we can employ various model validation techniques to assess a model’s generalizability and address overfitting or underfitting issues. These techniques allow us to evaluate a model’s performance on unseen data and ensure it can effectively learn and apply its knowledge to new scenarios.\n\n\n7.2.1.3 Train/Test Validation\nThis is the simplest and most common validation technique. Here’s how it works:\n1.Split the data into two sets: training data (usually 70-80%) and testing data (remaining 20-30%).\n2.Train the model on the training data.\n3.Evaluate the model’s performance on the unseen testing data.\nThis gives us an idea of how well the model generalizes to new data.\nFor the Current problem-To evaluate the generalizability of our machine learning model, I’ll be utilizing the train-test split validation approach with an 80/20 split. and its summary as follows.\n\n# Data splitting\nX_train_header_text, X_test_header_text, Y_train_PIIPHI_flag, Y_test_PIIPHI_flag = (\n    train_test_split(\n        dbas_pii_phi_vectors.toarray(), y_target, random_state=2021, test_size=0.20\n    )\n)\n\n\n\n\nTable 11: Train/Test Validation\n\n\n\n\n\nType\nSize\n\n\n\n\nTotal\n13963\n\n\nTrain\n11170\n\n\nTest\n2793\n\n\n\n\n\n\n\n\n\n7.2.2 Priliminary MachineLearning Modeling.\n\n7.2.2.1 LogisticRegression,Bagging and Boosting(RandomForest and XGBoost)\nLogistic Regression:\n\nSimple and interpretable: Easy to understand and diagnose potential issues.\nWorks well for binary classification: Suitable for predicting probabilities of belonging to one of two classes (0 or 1).\nMay struggle with complex relationships: Limited ability to handle highly non-linear data.\n\nRandom Forest(Bagging):\n\nEnsemble method: Combines multiple decision trees for improved accuracy and robustness.\nHandles complex data well: Can capture non-linear relationships effectively.\nLess interpretable: Can be difficult to understand the inner workings of the model.\n\nXGBoost(Boosting):\n\nPowerful ensemble method: Often achieves high accuracy on various machine learning tasks.\nFlexible and efficient: Handles different data types and scales well for large datasets.\nHyperparameter tuning can be complex: Requires careful selection of model parameters for optimal performance.\n\n\n# model_dbas_LRC = LogisticRegression()\n# model_dbas_LRC.fit(X_train_header_text, Y_train_PIIPHI_flag)\n\n# los_pred = model_dbas_LRC.predict(X_train_header_text)\n# los_pred_ = model_dbas_LRC.predict(X_test_header_text)\n\n\n# custom_classification_metrics_report(\"LogisticRegression-Traininig\", Y_train_PIIPHI_flag, los_pred)\n\n\n# custom_classification_metrics_report(\"LogisticRegression-Testing\", Y_test_PIIPHI_flag, los_pred_)\n\n\n# _model_RFC_tuned = RandomForestClassifier(\n#     n_estimators=200, max_depth=30, max_features=60, random_state=2024\n# )\n\n\n# _model_RFC_tuned.fit(X_train_header_text, Y_train_PIIPHI_flag)\n\n\n# los_tr_preds_rf = _model_RFC_tuned.predict(X_train_header_text)\n# los_tes_preds_rf = _model_RFC_tuned.predict(X_test_header_text)\n\n\n\n7.2.2.2 Priliminary MachineLearning Model Summary Report.\n\ndf_metric = pd.read_excel(r'/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/ssbba_model_metrics_v1.xlsx')\n\n\ndf_metric\n\n\n\n\n\n\nClassification Model Summary Table {#f3be14dd}\n\n\n\nModelType\nAccuracy\nROC-AUC\nF1\nPrecision\nRecall\n\n\n\n\n0\nLogisticRegression-Traininig\n0.88747\n0.73692\n0.63869\n0.95529\n0.47971\n\n\n1\nLogisticRegression-Testing\n0.87504\n0.70946\n0.58600\n0.93561\n0.42660\n\n\n2\nRandomForest-Training\n0.98827\n0.98766\n0.97213\n0.95807\n0.98661\n\n\n3\nRandomForest-Testing\n0.58145\n0.71304\n0.48160\n0.32399\n0.93782\n\n\n4\nXGBoost-Tranining\n0.88568\n0.73419\n0.63294\n0.94669\n0.47539\n\n\n5\nXGBoost-Testing\n0.87325\n0.71407\n0.59122\n0.89199\n0.44214\n\n\n\n\n\n\n\n\nHere are the few points about priliminary model performances.\nTraining vs. Testing Performance:\nAll models have a significant drop in performance between the training and testing data. This suggests potential overfitting, especially for Random Forest.\nModel-Specific Observations:\n1.Logistic Regression:\n1.Training accuracy (0.887) is reasonable, but ROC-AUC (0.737) suggests it might not be the best at distinguishing classes.\n2.Precision (0.955) is high, indicating the model rarely makes false positives when predicting the positive class.\n3.However, recall (0.480) is low, meaning it misses many actual positive cases\n2.Random Forest:\n1.Training performance is very high (accuracy and F1-score near 1), but testing performance is significantly lower (accuracy 0.581). This is a clear case of overfitting. The model memorized the training data and performs poorly on unseen data.\n3.XGBoost:\n1.Similar to Logistic Regression, training and testing performance show a gap, but it’s less severe than Random Forest. Performance metrics are comparable to Logistic Regression, with slightly lower precision and recall.\nConclusion:\nBased on this limited data, Logistic Regression or XGBoost might be better initial choices for this task. However, further investigation is needed.\n\n\n\n7.2.3 Experimentations:MachineLearning Models-Dealing with Class Imbalance Problem.\nFrom the above summary table- three of the model’s precision is greater than to recall indicate a strong bias towards precision at the cost of recall. Let’s break down what this means:\nHigh Precision(0.95):\nOut of all the positive predictions of our model made, 95.0% were actually correct. In other words, the model is very good at avoiding false positives (classifying something positive when it’s actually negative).\nLow Recall (0.42):\nThis is the concerning aspect. Out of all the actual positive cases in your data, the model only identified 42.0%. This means the model misses a significant portion of the true positives, resulting in false negatives (failing to classify something positive that actually is positive).\nInterpretation:\nThis suggests our model prioritizes precision over recall. It excels at identifying truly positive cases but misses many of them overall. This could be because:\n\nThe data itself might be imbalanced, with far fewer positive cases than negative cases. The model learns to be cautious and avoids classifying things as positive unless very certain, leading to many false negatives.\nThe model training might be biased towards precision. Techniques like adjusting class weights or choosing an appropriate cost function during training could influence this bias.\n\nImpact:\n\nThe impact of this bias depends on our specific application. In some cases, precision is more crucial. For instance, a Document Classifier with high precision might be desirable to avoid mistakenly marking important elements as PII/PHI (even if it misses NO-PII/PHI).\nHowever, in many scenarios, missing a significant portion of positive cases (low recall) could be detrimental. For example, The same Document Classifier with low recall might miss many actual PII/PHI elements, leading to missed Potential Sensitive Informations to be extracted.\n\nRecommendations:\n\nsns.countplot(df_pii_phi_tidy,x='class_label')\nplt.show()\n\n\n\n\n\n\n\n\n1.Our exploration of the data indicates a potential class imbalance issue for our machine learning classification model. As observed in the visualization, approximately 20% of the data points are classified as PII/PHI (positive class), while the remaining 80% belong to the NO-PII/PHI category (negative class).\n2.Class imbalance occurs when a dataset has a significant difference in the number of examples between different classes. In our case, the positive class (PII/PHI) has a considerably lower representation compared to the negative class (NO-PII/PHI).\n3.This class imbalance can negatively impact the performance of machine learning classification models. Many algorithms tend to prioritize the majority class during training, leading to poorer performance in classifying the minority class (PII/PHI in our case).\nFound thtat data has class imbalance, we can address it through techniques like oversampling or undersampling.\nClass Imbalance Solution Techniques:\n1.Oversampling:\n\nIncreases the number of data points in the minority class.\nTechniques like duplicate sampling or Synthetic Minority Oversampling Technique (SMOTE) can be used.\nBenefit: Helps the model learn the characteristics of the minority class more effectively.\nDrawback: Can lead to overfitting if not done carefully.\n\n2.Undersampling:\n\nReduces the number of data points in the majority class.\nTechniques like random undersampling or near-miss sampling can be used.\nBenefit: Creates a more balanced dataset for training.\nDrawback: Can lead to loss of information from the majority class.\n\n\n\n\nTable 12: ClassImbalance Technique Metrics\n\n\n\n\n\ntype\nsampling_tech\nmodel\nroc_auc\n\n\n\n\nUNDER\nrandom\nLogisticRegression\n0.84\n\n\nUNDER\nrandom\nRanddomForest\n0.72\n\n\nUNDER\nrandom\nXGBoost\n0.77\n\n\nOVER\nrandom\nLogisticRegression\n0.86\n\n\nOVER\nrandom\nRanddomForest\n0.77\n\n\nOVER\nrandom\nXGBoost\n0.80\n\n\nOVER\nSMOTE\nLogisticRegression\n0.87\n\n\nOVER\nSMOTE\nRanddomForest\n0.74\n\n\nOVER\nSMOTE\nXGBoost\n0.82\n\n\n\n\n\n\nFrom the above table OVER SAMPLE could be the choice for our classification model\n\nThe provided table summarizes the performance of various machine learning models trained on a potentially imbalanced dataset.\nThe table compares the impact of undersampling (UNDER) and oversampling (OVER) techniques, along with the baseline performance without sampling (type absent).\nWe’ve also explored the effect of using SMOTE (Synthetic Minority Oversampling Technique) within the oversampling approach.\n\nOversampling Potential: Based on the results, oversampling techniques (particularly OVER-SMOTE) generally yielded comparable or slightly better ROC-AUC scores for Logistic Regression and XGBoost compared to undersampling or no sampling. However, Random Forest performance seems less affected by the sampling technique.\nSMOTE creates new data points for the minority class, essentially adding synthetic neighbors to existing minority examples. This helps the machine learning model learn the minority class better. this has been considered for doing the over sampling within our data. the oversampled data will be trained and analyzed in the next steps.\n\n\n7.2.4 MachineLearning Model Tuning,Evaluating,Selection,Interpreting and Finalizing.\nThe below table shows that the data has been balanced, with each class in the target variable having the same number of data points.\n\n\n\nTable 13: ClassLabel Counts afer OverSampling\n\n\n\n\n\nclasslabel\ntotal\n\n\n\n\n0\n8854\n\n\n1\n8854\n\n\n\n\n\n\nTo train the machine learning model, we will follow these steps:\n1.Hyperparameter tuning:\nWe will use GridSearchCV to optimize the parameters of each classifier. This ensures we find the best settings for each model’s performance.\n2.Model evaluation:\nWe will evaluate the resulting models using metrics like accuracy and ROC_AUC. This helps us compare their performance and identify the most effective model.\n3.Model selection:\nBased on the evaluation results and specified requirements, we will select the best performing classifier for our needs.\nThrough hyperparameter tuning, we found these parameters to be most effective for training each classification model:\n\n\n\nTable 14: Parameter Tuning\n\n\n\n\n\n\n\n\n\nmodel\nrecommended_parameters\n\n\n\n\nLogistic Regression\npenalty:L2,C:1.0,fit_intercept:True,solver:liblinear\n\n\nBagging(RandomForest)\nmax_depth:30,max_features:60,n_estimators:200\n\n\nBoosting(XGBOOST )\ngamma:0,learning_rate:0.05,max_depth:5,reg_lambda:5\n\n\n\n\n\n\n\ncustom_SMOTE_spec = SMOTE(random_state=2024)\nX_header_SMOTE, Y_pii_phi_SMOTE = custom_SMOTE_spec.fit_resample(\n    X_train_header_text, Y_train_PIIPHI_flag\n)\n\n\nmodel_dbas_LRC = LogisticRegression()\nmodel_dbas_LRC.fit(X_header_SMOTE, Y_pii_phi_SMOTE)\n\nlos_pred = model_dbas_LRC.predict(X_header_SMOTE)\nlos_pred_ = model_dbas_LRC.predict(X_test_header_text)\n\n\n7.2.4.1 Logistic Regression.\n\ncustom_classification_metrics_report('LogisticRegression-Training',Y_pii_phi_SMOTE,los_pred)\n\n-------------------------------------------------------------------\nLogisticRegression-Training#:ConfusionMatrix and ROC-AUC Curve\n-------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nLogisticRegression-Training\n0.89863\n0.89863\n0.89037\n0.96941\n0.82324\n\n\n\n\n\n\n\n\n\ncustom_classification_metrics_report('LogisticRegression-Testing',Y_test_PIIPHI_flag,los_pred_)\n\n-------------------------------------------------------------------\nLogisticRegression-Testing#:ConfusionMatrix and ROC-AUC Curve\n-------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nLogisticRegression-Testing\n0.88507\n0.78466\n0.68865\n0.7854\n0.61313\n\n\n\n\n\n\n\n\n\n\n7.2.4.2 Bagging(RandomForest).\n\n_model_RFC_tuned = RandomForestClassifier(\n    n_estimators=200, max_depth=30, max_features=60,random_state=2024,\n)\n_model_RFC_tuned.fit(X_header_SMOTE, Y_pii_phi_SMOTE)\n\nlos_tr_preds_rf = _model_RFC_tuned.predict(X_header_SMOTE)\nlos_tes_preds_rf = _model_RFC_tuned.predict(X_test_header_text)\n\n\ncustom_classification_metrics_report(\n    \"Bagging-Training\", Y_pii_phi_SMOTE, los_tr_preds_rf\n)\n\n-------------------------------------------------------------------\nBagging-Training#:ConfusionMatrix and ROC-AUC Curve\n-------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nBagging-Training\n0.77987\n0.77987\n0.72193\n0.97986\n0.57149\n\n\n\n\n\n\n\n\n\ncustom_classification_metrics_report(\n    \"Bagging-Testing\", Y_test_PIIPHI_flag, los_tes_preds_rf\n)\n\n-------------------------------------------------------------------\nBagging-Testing#:ConfusionMatrix and ROC-AUC Curve\n-------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nBagging-Testing\n0.88006\n0.74897\n0.64475\n0.83516\n0.52504\n\n\n\n\n\n\n\n\n\n\n7.2.4.3 Boosting(XGBoost).\n\n_model_XGB_D_tuned = xgb.XGBClassifier(objective=\"binary:logistic\", seed=2024)\n_model_XGB_D_tuned.fit(X_header_SMOTE, Y_pii_phi_SMOTE)\nlos_tr_preds_XGB = _model_XGB_D_tuned.predict(X_header_SMOTE)\nlos_tes_preds_XGB = _model_XGB_D_tuned.predict(X_test_header_text)\n\n\ncustom_classification_metrics_report('Boosting-Training',Y_pii_phi_SMOTE,los_tr_preds_XGB)\n\n-------------------------------------------------------------------\nBoosting-Training#:ConfusionMatrix and ROC-AUC Curve\n-------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nBoosting-Training\n0.80602\n0.80602\n0.76474\n0.97146\n0.63056\n\n\n\n\n\n\n\n\n\ncustom_classification_metrics_report('Boosting-Testing',Y_test_PIIPHI_flag,los_tes_preds_XGB)\n\n-------------------------------------------------------------------\nBoosting-Testing#:ConfusionMatrix and ROC-AUC Curve\n-------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nROC-AUC\nF1-Score\nPrecision\nRecall\n\n\n\n\nBoosting-Testing\n0.88042\n0.75366\n0.65063\n0.82493\n0.53713\n\n\n\n\n\n\n\n\n\n\n7.2.4.4 Model Selection and Interpretations.\n\ndf_metric_final = pd.read_excel(r'/Users/malleshamyamulla/Desktop/SSBBA/MBB_PROJECT/data/sds_final_model_metrics.xlsx')\n\n\ndf_metric_final\n\n\n\n\n\n\nModel Selection {#e9986e31}\n\n\n\nModelType\nAccuracy\nROC-AUC\nF1\nPrecision\nRecall\n\n\n\n\n0\nLogisticRegression-Traininig\n0.89863\n0.89863\n0.89037\n0.96941\n0.82324\n\n\n1\nLogisticRegression-Testing\n0.88507\n0.78466\n0.68865\n0.78540\n0.61313\n\n\n2\nRandomForest-Training\n0.77987\n0.77987\n0.72193\n0.97986\n0.57149\n\n\n3\nRandomForest-Testing\n0.88006\n0.74897\n0.64475\n0.83516\n0.52504\n\n\n4\nXGBoost-Tranining\n0.80602\n0.80602\n0.76474\n0.97146\n0.63056\n\n\n5\nXGBoost-Testing\n0.88042\n0.75366\n0.65063\n0.82493\n0.53713\n\n\n\n\n\n\n\n\nAccuracy: This represents the overall percentage of correct predictions made by the model. Here, all models performed well in training (around 80-90% accuracy). However, in testing with unseen data, their accuracy dropped slightly (around 88%).\nROC-AUC: This metric measures how well the model distinguishes between positive and negative cases. All models have similar ROC-AUC scores in both training and testing, indicating a decent ability to differentiate classes.\n1.Logistic Regression:\n\nThis model’s performance dropped the most between training and testing (accuracy -1%, ROC-AUC -12%). This suggests some overfitting on the training data.\n\n2.Random Forest:\n\nThis model’s performance improved slightly in testing compared to training for accuracy (+10%) but dropped a bit for other metrics (F1, Precision, Recall). This could indicate some issues with model complexity or randomness in the forest.\n\n3.XGBoost:\n\nXGBoost achieved a slight improvement in testing accuracy (+7%) compared to Random Forest and Logistic Regression.\nOther Metrics such as F1, Precision, and Recall scores decreased slightly compared to its training performance, they remain better overall than those of Random Forest and Logistic Regression.\nWith larger datasets, XGBoost’s performance across all metrics may improve further. We would be investigating hyperparameter tuning to optimize XGBoost for even better results in future experiments.\n\nAfter evaluating several models, XGBoost emerged as the most effective option for classifying PII/PHI and NO-PII/PHI data within documents.\n\n\n\n7.2.5 MachineLearning Model Deployment in Streamlit Cloud App.\nHaving proven its effectiveness in predicting PII/PHI and NO-PII/PHI elements, the final XGBoost model is now deployed on Streamlit Cloud. This means we can easily access it through your web browser for convenient predictions.\nPlease click on the below blue color text or copy the URL to view it in browser.\nDeployed Demo App:\nURL Link: https://ssbbba2024-sxsvtbi6newjbkauulyan7.streamlit.app"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#propose-validate-new-process.",
    "href": "ISI_SSBBA_Book.html#propose-validate-new-process.",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "7.3 Propose & Validate New Process.",
    "text": "7.3 Propose & Validate New Process.\n\n7.3.1 New Process Flow Diagram.\n\n\n\nDBAS-Classification New Process Automation\n\n\n\n7.3.1.1 Streamlining Information Review and Extraction.\nThis new process automation is a game-changer! It will significantly improve our ability to review and extract information with both speed and accuracy. Here’s how:\nReduced Manual Effort: Manual tasks will be minimized, freeing up valuable human resources for other critical activities.\nEnhanced Accuracy: Automation helps to reduce human error, leading to more accurate information extraction.\nThe provided process flow diagram offers a clear picture of how automation integrates into the new approach:\n1.Data Gathering: Information is efficiently collected from various sources like databases and files.\n2.Automated Classification and Extraction: The data is then fed into a machine learning (ML) PII/PHI classifier, which acts as the automation tool. This powerful tool excels at identifying and extracting the desired information (PII/PHI and NO-PII/PHI elements) with impressive precision.\n3.Seamless Integration with Existing Automation: Once the ML classifier completes the extraction phase, the remaining data mining steps seamlessly integrate with existing rule-based and fuzzy-based techniques. These established automation methods further accelerate the overall process.\n4. Human Oversight Ensures Quality: While automation takes center stage, human expertise remains crucial. Analysts will review a representative sample of the extracted data by the classifier. This human oversight ensures that any missed or overly classified information is carefully addressed, guaranteeing the highest quality results.\nThis combined approach, leveraging the power of automation and human expertise, promises a significant leap forward in information review and extraction efficiency."
  },
  {
    "objectID": "ISI_SSBBA_Book.html#establish-improved-performance.",
    "href": "ISI_SSBBA_Book.html#establish-improved-performance.",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "8.1 Establish Improved Performance.",
    "text": "8.1 Establish Improved Performance.\nIn an improve phase we have developed an automation to address the root causes which hamper thes document classifications accuracies, extractions difficulties such as more number of sheets, big data, etc etc.\nWe haven’t applied it on the ongoing process to see the results how it is performing, as stated in the new proposed process we will implement the automation on good number of projects atleaset 50, will record all the required metrics such how much time it takes to complete the phase-1 tasks, classification accuracies, false positive, false negative ratios, notification delivery time, how are the complex files being handled, how much time it spent for processing big data etc. etc. So that we would be able to infer how much the new process has been improved caluclating the Cp,Cpk,SigmaLevel and DPMo.\n\n8.1.1 Addressing Challenges and Measuring Improvement\nWe’ve developed an innovative automation solution to tackle the root causes behind classification inaccuracies and extraction difficulties. These challenges often arise due to factors like a high number of document sheets and large data volumes.\n\n\n8.1.2 Pilot Testing and Metrics Collection\nBefore integrating automation into ongoing processes, we’ll conduct a pilot test on a significant number of projects (at least 50). During this pilot, we’ll meticulously record key metrics to gauge the effectiveness of the new approach. These metrics will include:\n1.Phase-1 Completion Time: We’ll track how much faster the new process completes the initial tasks compared to the current approach.\n2.Classification Accuracy: This will measure the ability of the automation to correctly identify PII/PHI and NO-PII/PHI elements.\n3.Error Rates: We’ll monitor both false positive (incorrectly identified PII/PHI) and false negative (missed PII/PHI) rates.\n4.Notification Delivery Time: We’ll assess how quickly the data breach notification lists are prepared.\n5.Complex File Handling: We’ll evaluate the automation’s efficiency in processing documents with a large number of sheets.\n6.Big Data Processing Time: We’ll measure the time taken to process large datasets.\n\n\n8.1.3 Evaluating Success with Statistical Methods\nBy collecting this comprehensive data, we can calculate metrics like Cp, Cpk, Sigma Level, and DPMO. These statistical tools will provide a clear picture of how significantly the new process has improved overall efficiency and accuracy.\nThis pilot testing approach allows us to assess the automation’s impact in a controlled environment before full-scale deployment. The gathered data and subsequent analysis will provide objective evidence of the new process’s effectiveness."
  },
  {
    "objectID": "ISI_SSBBA_Book.html#control-plan",
    "href": "ISI_SSBBA_Book.html#control-plan",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "8.2 Control Plan",
    "text": "8.2 Control Plan\n\n\n\nDBAS-Control Plan"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#project-sign-off",
    "href": "ISI_SSBBA_Book.html#project-sign-off",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "8.3 Project Sign-Off",
    "text": "8.3 Project Sign-Off\n\n\n\nDBAS-Project Sign-OFF"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#project-summary",
    "href": "ISI_SSBBA_Book.html#project-summary",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "8.4 Project Summary",
    "text": "8.4 Project Summary\n\n\n\nDBAS-Project Summary"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#executive-summary",
    "href": "ISI_SSBBA_Book.html#executive-summary",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "8.5 Executive Summary",
    "text": "8.5 Executive Summary\n\n\n\nDBAS-Executive Summary"
  },
  {
    "objectID": "ISI_SSBBA_Book.html#refereces-books-and-tools",
    "href": "ISI_SSBBA_Book.html#refereces-books-and-tools",
    "title": "SixSigma Study On Improving PII/PHI Classifications in DataBreach Analysis",
    "section": "9.1 Refereces-Books and Tools",
    "text": "9.1 Refereces-Books and Tools\n\nISLP\nPython for data analysis\nPolars for data analysis\nStories with data\nModern Statistics\nInferential Statistics\nFeatureEngineering\nApplied Machine Learning\nSix Sigma-Green Belt(Advanced) by GMR- ISI,Hyderabad.\nStatQuest Illustrated Guide to MachineLearning.\nPython-MachineLearning\nData Visualization\nCode Editor-1\nCode Editor-2\nApp Deployment\nNoteBook and Documentation"
  }
]